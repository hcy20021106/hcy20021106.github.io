<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hechenyi.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://hechenyi.github.io/blog/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hechenyi.github.io/blog/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2026/01/04/1-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/04/1-3/" class="post-title-link" itemprop="url">1-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-04 11:16:15 / Modified: 17:11:21" itemprop="dateCreated datePublished" datetime="2026-01-04T11:16:15+08:00">2026-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="Open-Sora"><a href="#Open-Sora" class="headerlink" title="Open-Sora"></a>Open-Sora</h3><p>We begin by leveraging the open-source HunyuanVideo VAE as the initial autoencoder for our model. To further reduce both training and interference costs, <strong>Video DC-AE</strong> is used as a video autoencoder to improve efficiency while maintaining high reconstruction fidelity.</p>
<h3 id="High-Compression-Autoencoder-Adaptation"><a href="#High-Compression-Autoencoder-Adaptation" class="headerlink" title="High Compression Autoencoder Adaptation"></a>High Compression Autoencoder Adaptation</h3><p>$D_{token} &#x3D; D_T × D_H × D_W × P_T × P_H × P_W$.  </p>
<p>$D_{info} &#x3D;  (D_T × D_H × D_W × C_{in}) &#x2F; C_{out}$<br>Additionally, we find that maintaining similar reconstruction performance requires a 4×increase in channels when doubling the height and width compression ratio, whereas adding a 4×temporal compression on top of the original image DC-AE (no temporal compression) incurs no additional channels possibly due to redundancy in temporal information. Based on these insights, we train a Video DC-AE with a 2x Dinfo and 16 x Dtoken as compared to the Hunyuan Video VAE while maintaining comparable performances.</p>
<h3 id="Auto-encoder-reconstruction-performance-comparison"><a href="#Auto-encoder-reconstruction-performance-comparison" class="headerlink" title="Auto-encoder reconstruction performance comparison"></a>Auto-encoder reconstruction performance comparison</h3><ul>
<li>Hunyuan video vae<br>TxHxW: 4 x 8 x 8,   channel: 16</li>
<li>Open Sora 1.2<br>TxHxW: 4 x 8 x 8,   channel: 16</li>
<li>StepVideo VAE<br>TxHxW: 8 x 16 x 16, channel: 64</li>
<li>Video DC-AE<br>TxHxW: 4 x 32 x 32, channel: 128</li>
</ul>
<h3 id="EfficientViT-Multi-Scale-Linear-Attention-for-High-Resolution-Dense-Prediction"><a href="#EfficientViT-Multi-Scale-Linear-Attention-for-High-Resolution-Dense-Prediction" class="headerlink" title="EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction"></a>EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction</h3><p>High-resolution dense prediction enables many applealing real-world applications, such as computational photography, autonomous driving.Unlike prior high-resolution dense prediction models<br>that rely on heavy softmax attention, hardware-inefficient<br>large-kernel convolution, or complicated topology structure to obtain good performances, the efficientViT uses multi-scale linear<br>attention to achieve the global receptive field and multi-scale learning. </p>
<h3 id="DCVC"><a href="#DCVC" class="headerlink" title="DCVC"></a>DCVC</h3><h3 id="UniTok-A-Unified-Tokenizer-for-Visual-Generation-and-Understanding"><a href="#UniTok-A-Unified-Tokenizer-for-Visual-Generation-and-Understanding" class="headerlink" title="UniTok: A Unified Tokenizer for Visual Generation and Understanding"></a>UniTok: A Unified Tokenizer for Visual Generation and Understanding</h3><p>Leading VQVAE tokenizers predominantly adopt the CNN architecture, while ViT is preferred in CLIP training for its scalability. To take advantage of both, we choose a hybrid architecture, ViTamin-L&#x2F;16, to instanitate UniTok.<br><strong>ViTamin-L&#x2F;16</strong> is referred as<br>“Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition, pages 12954–12966, 2024.”<br>Generally, ViTamin &#x3D; ViT + CNN inductive bias</p>
<h4 id="Image-Generation-Models-Comparision"><a href="#Image-Generation-Models-Comparision" class="headerlink" title="Image Generation Models Comparision"></a>Image Generation Models Comparision</h4><ul>
<li>diffusion model。</li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>Res</th>
<th>FID</th>
</tr>
</thead>
<tbody><tr>
<td>SDXL</td>
<td>1024</td>
<td>9.55</td>
</tr>
<tr>
<td>PixArt</td>
<td>1024</td>
<td>6.14</td>
</tr>
<tr>
<td>Playground</td>
<td>1024</td>
<td><strong>4.48</strong></td>
</tr>
</tbody></table>
<ul>
<li>自回归模型。</li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>Res</th>
<th>FID</th>
</tr>
</thead>
<tbody><tr>
<td>Liquid</td>
<td>512</td>
<td>5.47</td>
</tr>
<tr>
<td>Janus</td>
<td>384</td>
<td>10.10</td>
</tr>
<tr>
<td>LWM</td>
<td>256</td>
<td>17.77</td>
</tr>
<tr>
<td>VILA-U</td>
<td>256</td>
<td>12.81</td>
</tr>
<tr>
<td><strong>UniTok</strong></td>
<td>256</td>
<td><strong>7.46</strong></td>
</tr>
</tbody></table>
<ul>
<li>Discrete Diffusion.</li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>Res</th>
<th>FID</th>
</tr>
</thead>
<tbody><tr>
<td>Show-o</td>
<td>256</td>
<td>15.18</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2026/01/03/1-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2026/01/03/1-2/" class="post-title-link" itemprop="url">1-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-03 15:12:02 / Modified: 16:25:53" itemprop="dateCreated datePublished" datetime="2026-01-03T15:12:02+08:00">2026-01-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="Wireless-Deep-Video-Semantic-Transmission"><a href="#Wireless-Deep-Video-Semantic-Transmission" class="headerlink" title="Wireless Deep Video Semantic Transmission"></a>Wireless Deep Video Semantic Transmission</h3><h4 id="DVST-框架：非线性变换-深度-JSCC-时序上下文的统一视频-JSCC"><a href="#DVST-框架：非线性变换-深度-JSCC-时序上下文的统一视频-JSCC" class="headerlink" title="DVST 框架：非线性变换 + 深度 JSCC + 时序上下文的统一视频 JSCC"></a>DVST 框架：非线性变换 + 深度 JSCC + 时序上下文的统一视频 JSCC</h4><p>提出一个专为无线视频设计的 Deep Video Semantic Transmission (DVST) 框架：</p>
<p>上游用 NVTC 类非线性变换 做语义特征提取与概率建模；<br>下游是 可变长的 deep JSCC 编码器&#x2F;解码器 直接面对信道；<br>整体端到端训练，直接优化 rate–distortion。</p>
<p>相比只在图像、小分辨率上做的 deep JSCC，他们解决了 高维视频源上编码增益早饱和 的问题。</p>
<h4 id="上下文驱动的语义特征建模（Context-Driven-Semantic-Feature-Modeling）"><a href="#上下文驱动的语义特征建模（Context-Driven-Semantic-Feature-Modeling）" class="headerlink" title="上下文驱动的语义特征建模（Context-Driven Semantic Feature Modeling）"></a>上下文驱动的语义特征建模（Context-Driven Semantic Feature Modeling）</h4><p>在 语义特征域（latent y） 和 码字域（s） 两个层面都引入“上下文”：</p>
<p>通过光流在特征&#x2F;码字空间 warp 参考帧，生成 Tx&#x2F;Rx context $\check{c}_t, \hat{c}_t, \check{d}_t, \hat{d}_t$。<br>分别条件化分析变换 $g_a$、合成变换 $g_s$、编码器 $f_e$、解码器 $f_d$。</p>
<p>其中$\check{c}<em>t &#x3D; φa(\check{x}</em>{t−1})&#x3D;φ_{ref} (warp(φ_{fe}(\check{x}_{t−1}), \check{m}_t)),$.<br>其中运动向量$m_t$是通过flow estimation network 得来的，然后$\check{m}_t is attained by the whole pipeline including each function except for the channel transfer function W.</p>
<p>这样一来，模型可以：</p>
<p>对“老内容”更多依赖参考帧（少发）；<br>对“新内容”多发信息；<br>本质上是端到端学出来的、非显式残差编码。</p>
<h4 id="熵驱动的速率自适应传输机制（Rate-Adaptive-Semantic-Feature-Transmission）"><a href="#熵驱动的速率自适应传输机制（Rate-Adaptive-Semantic-Feature-Transmission）" class="headerlink" title="熵驱动的速率自适应传输机制（Rate-Adaptive Semantic Feature Transmission）"></a>熵驱动的速率自适应传输机制（Rate-Adaptive Semantic Feature Transmission）</h4><p>利用 BA（自回归）+ hyperprior + temporal context 构建 带时间先验的熵模型，估计每个 embedding 的“重要性”（熵）。<br>引入一个 可学习的比例因子 $\eta_t$，把熵值映射到实际信道维度 $k_{t,i}$，再量化到离散集合。<br>在网络实现上：</p>
<p>用 Swin Transformer + rate tokens + 动态 FC 做 variable-length deep JSCC，而不是为每个码率单独训练一个网络。</p>
<p>这部分就是他们实现“同一模型支持不同 CBR、在空间和时间上动态分配码率”的关键创新。</p>
<p>运动链路 + 语义特征 MEMC：在特征&#x2F;码字域做运动补偿</p>
<p>不是在像素域做传统 MEMC，而是在：</p>
<p>语义特征域上，<br>和 JSCC 码字域上，<br>结合光流 + warp + refine，构造上下文。</p>
<p>再加上 MV 自己也是经 NTC + JSCC 压缩传输的 motion link。<br>这使得 DVST 可以：</p>
<p>更好地利用时域冗余；<br>在有限带宽下把更多资源给“真正变动的语义”。</p>
<p>GOP 级联合训练 + 带宽分配学习</p>
<p>不是单帧训练，而是 在一个 GOP 内联合优化：</p>
<p>让模型学会 在帧与帧之间分配带宽：哪些帧&#x2F;哪些位置应该多给、哪些可以少给。</p>
<p>训练上使用渐进式策略（先单链路预训练，再整个系统 joint finetune）来防止不稳定。</p>
<p>性能结果上的“亮点”：可当作卖点写</p>
<p>在 AWGN &#x2F; Rayleigh 渠道上，相比：</p>
<p>H.264&#x2F;H.265 + LDPC（现实配置）<br>H.264&#x2F;H.265 + capacity-achieving code（理论上界）</p>
<p>DVST 在很多序列上：</p>
<p>PSNR&#x2F;MS-SSIM 更高，尤其高分辨率、高码率区域；<br>达到同样画质时，可节省约 20–60% 的平均信道带宽（最高约 50%）。</p>
<p>同时：</p>
<p>对信道 SNR mismatch 不会出现 cliff effect，性能曲线比较平滑；<br>在联合 segmentation 训练下，对机器视觉任务（mIoU）和重建 PSNR 都有优势。</p>
<h3 id="Nonlinear-Transforms-for-Lossy-Image-Compression"><a href="#Nonlinear-Transforms-for-Lossy-Image-Compression" class="headerlink" title="Nonlinear Transforms for Lossy Image Compression"></a>Nonlinear Transforms for Lossy Image Compression</h3><h4 id="GDN"><a href="#GDN" class="headerlink" title="GDN"></a>GDN</h4><p>GDN（Generalized Divisive Normalization，广义除法归一化）是一种用于神经网络中 非线性变换 的操作，特别是在图像和信号处理任务中，常用于神经网络的激活层。它的主要作用是通过对输入进行除法归一化（divisive normalization），来实现非线性变换，从而有助于网络的学习和特征提取。<br>GDN 的基本原理<br>GDN 是通过对输入信号应用 归一化 和 非线性 操作来增强模型的表达能力。具体来说，它是对输入信号进行除法归一化，并通过一个参数化的方式使得输出具有非线性性质。这一过程的核心是：<br>$$y_i &#x3D; \frac{x_i}{\sqrt{c_i + \sum_{j} W_{ij} x_j^2}}$$</p>
<p>$x_i$ 是输入信号的第 $i$ 个元素。<br>$y_i$ 是输出信号的第 $i$ 个元素。<br>$c_i$ 是一个常数，用于控制归一化的尺度。<br>$W_{ij}$ 是一个权重矩阵，控制了输入元素之间的相互影响。<br>归一化操作会根据输入信号的局部上下文信息进行计算。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/31/12-30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/31/12-30/" class="post-title-link" itemprop="url">12-30</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-31 00:05:29" itemprop="dateCreated datePublished" datetime="2025-12-31T00:05:29+08:00">2025-12-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-01-02 17:15:36" itemprop="dateModified" datetime="2026-01-02T17:15:36+08:00">2026-01-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="Promptus-Can-Prompts-Streaming-Replace-Video-Streaming-with-Stable-Diffusion"><a href="#Promptus-Can-Prompts-Streaming-Replace-Video-Streaming-with-Stable-Diffusion" class="headerlink" title="Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion"></a>Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion</h3><p>Since we adopt a single-step denoising Stable Diffusion, if the input noise is far from the target image in the latent space, this movement cannot be completed in a singel step, making it impossible to fit the inverse prompt. Using random noise as input, after the loss value converges, the generated frame still has noticeable differences from the tar- get frame, including blurring, noise artifacts, and inconsistent details. Therefore, we need to reduce the distance between the input noise and the target frame in the latent space.</p>
<p><strong>Prompt regularization</strong>: We found that even with real video as supervision, fitting occasionally fails. This is because as gradient descent progresses, the distribution of prompts gradually shifts away from the distribution of CLIP embeddings, resulting in an inability to generate meaningful images.</p>
<p><strong>插值公式</strong><br>prompt(1) &#x3D; 0.9 * prompt(0) + 0.1 * prompt(10)<br>prompt(2) &#x3D; 0.8 * prompt(0) + 0.2 * prompt(10)<br>…<br>prompt(9) &#x3D; 0.1 * prompt(0) + 0.9 * prompt(10)</p>
<p>第 1–10 帧这一段里，真正“训练 &#x2F; 保存”的 prompt 只对应第 0 帧和第 10 帧；<br>第 1–9 帧的 prompt 是在这两个 prompt 之间“插值出来的”，并没有单独训练。</p>
<p><strong>steamingdiffusion</strong><br>tensorRT</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.denoise_engine = Engine(engine_path)</span><br><span class="line">self.decoder_engine = Engine(engine_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.batch = batch  <span class="comment"># the number of frames generated at once</span></span><br></pre></td></tr></table></figure>

<h2 id="diffusion-model"><a href="#diffusion-model" class="headerlink" title="diffusion model"></a>diffusion model</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">taesd = AutoencoderTiny.from_pretrained(<span class="string">&quot;madebyollin/taesd&quot;</span>, torch_dtype=torch.float32).cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                   <span class="comment"># generating a frame</span></span><br><span class="line">                   samples_z = sampler(denoiser, randn, cond=c, uc=uc)</span><br><span class="line">                   samples_x = decoder(samples_z)</span><br></pre></td></tr></table></figure>


<h2 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h2><ul>
<li>inversion</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=7 python inversion.py -frame_path <span class="string">&quot;/Huang_group/zyyz/tianyi/promptus/data/test_hcy/a03001a8-a50f-4d28-8513-637c3ec627bb&quot;</span> -max_id 140 -rank 15 -interval 10</span><br></pre></td></tr></table></figure>


<ul>
<li>generation</li>
</ul>
<pre><code class="language-bash">python generation.py -frame_path &quot;/Huang_group/zyyz/tianyi/promptus/data/test_hcy/00fe7c5e-130c-42eb-8e1c-7576e6a647b8&quot; -rank 16 -interval 10

```1
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/29/12-29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/29/12-29/" class="post-title-link" itemprop="url">12-29</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-29 11:55:44" itemprop="dateCreated datePublished" datetime="2025-12-29T11:55:44+08:00">2025-12-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-30 01:54:45" itemprop="dateModified" datetime="2025-12-30T01:54:45+08:00">2025-12-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="Promptus-Can-Prompts-Streaming-Replace-Video-Streaming-with-Stable-Diffusion"><a href="#Promptus-Can-Prompts-Streaming-Replace-Video-Streaming-with-Stable-Diffusion" class="headerlink" title="Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion"></a>Promptus: Can Prompts Streaming Replace Video Streaming with Stable Diffusion</h3><p>fine-tune the post-processing nerual networks for each video segmennt and send the fune-tuned nerual networks along with the video.<br>Neural network-based post-processing methods such as super-resolution<br>StreamDiffusion can generate images at a speed of 100FPS<br><strong>Video2seq</strong>模型和“Scaling up vision-language pre-training for image captioning.” 分别是video&#x2F;image captioning。<br>CLIP model converting discrete natural language p into a continuous text embedding c (an m ∗ n matrix)</p>
<h3 id="StreamDiffusion-A-Pipeline-level-Solution-for-Real-time-Interactive-Generation"><a href="#StreamDiffusion-A-Pipeline-level-Solution-for-Real-time-Interactive-Generation" class="headerlink" title="StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation"></a>StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input Image Stream</span><br><span class="line">   ↓</span><br><span class="line">IO Queue  →  VAE Encoder</span><br><span class="line">                ↓</span><br><span class="line">         Stream Batch UNet  +  R-CFG</span><br><span class="line">                ↓</span><br><span class="line">            VAE Decoder</span><br><span class="line">   ↓</span><br><span class="line">Output Queue  →  Renderer</span><br></pre></td></tr></table></figure>
<p>不等一帧完全denoise完，每来一帧就立刻进入pipeline，不同帧处在不同denoising step，把这些step拼成一个batch一次性送进unet</p>
<p>SD是在cross-attention条件分布上训练的，而 CLIP 只是一个生成该分布的工具。<br>K, V ∈ R^{77 × 1024}<br>Q &#x3D; Wq · h<br>K &#x3D; Wk · c<br>V &#x3D; Wv · c</p>
<p>Attention(Q, K, V)<br>计算过程如下：</p>
<h4 id="1-1-Latent-特征（图像侧）"><a href="#1-1-Latent-特征（图像侧）" class="headerlink" title="1.1 Latent 特征（图像侧）"></a>1.1 Latent 特征（图像侧）</h4><ul>
<li><p>扩散过程中某一层 UNet 的中间特征：</p>
<p>h ∈ R^{B × C × H × W}</p>
</li>
<li><p>其中：</p>
<ul>
<li>B：batch size</li>
<li>C：通道数（随 UNet 层变化，如 320 &#x2F; 640 &#x2F; 1280）</li>
<li>H, W：latent 空间分辨率</li>
</ul>
</li>
</ul>
<h4 id="1-2-条件特征（文本-连续条件）"><a href="#1-2-条件特征（文本-连续条件）" class="headerlink" title="1.2 条件特征（文本 &#x2F; 连续条件）"></a>1.2 条件特征（文本 &#x2F; 连续条件）</h4><ul>
<li><p>条件 embedding（来自 CLIP 或连续 prompt）：</p>
<p>c ∈ R^{B × T × C_c}</p>
</li>
<li><p>其中：</p>
<ul>
<li>T &#x3D; 77（token 数）</li>
<li>C_c &#x3D; 1024（SD2.x &#x2F; SD-Turbo）</li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-Latent-展平（Flatten）"><a href="#2-Latent-展平（Flatten）" class="headerlink" title="2. Latent 展平（Flatten）"></a>2. Latent 展平（Flatten）</h4><p>将空间维度展平：</p>
<ul>
<li>h ∈ R^{B × C × H × W}</li>
<li>reshape →</li>
<li>h ∈ R^{B × (H·W) × C}</li>
</ul>
<hr>
<h4 id="3-线性投影生成-Q-K-V"><a href="#3-线性投影生成-Q-K-V" class="headerlink" title="3. 线性投影生成 Q &#x2F; K &#x2F; V"></a>3. 线性投影生成 Q &#x2F; K &#x2F; V</h4><h4 id="3-1-投影权重"><a href="#3-1-投影权重" class="headerlink" title="3.1 投影权重"></a>3.1 投影权重</h4><ul>
<li>W_q ∈ R^{C × d}</li>
<li>W_k ∈ R^{C_c × d}</li>
<li>W_v ∈ R^{C_c × d}</li>
</ul>
<p>其中：</p>
<ul>
<li>d &#x3D; head_dim</li>
<li>多头注意力时：d &#x3D; embed_dim &#x2F; num_heads</li>
</ul>
<h4 id="3-2-线性变换"><a href="#3-2-线性变换" class="headerlink" title="3.2 线性变换"></a>3.2 线性变换</h4><ul>
<li><p>Query（来自 latent）：</p>
<p>Q &#x3D; h · W_q<br>Q ∈ R^{B × (H·W) × d}</p>
</li>
<li><p>Key（来自条件）：</p>
<p>K &#x3D; c · W_k<br>K ∈ R^{B × T × d}</p>
</li>
<li><p>Value（来自条件）：</p>
<p>V &#x3D; c · W_v<br>V ∈ R^{B × T × d}</p>
</li>
</ul>
<hr>
<h4 id="4-1-计算相似度矩阵"><a href="#4-1-计算相似度矩阵" class="headerlink" title="4.1 计算相似度矩阵"></a>4.1 计算相似度矩阵</h4><p>对每个 batch：</p>
<ul>
<li>Q ∈ R^{(H·W) × d}</li>
<li>K ∈ R^{T × d}</li>
</ul>
<p>计算：</p>
<ul>
<li>S &#x3D; Q · Kᵀ &#x2F; √d</li>
<li>S ∈ R^{(H·W) × T}</li>
</ul>
<p>语义含义：</p>
<ul>
<li>每一个空间位置（pixel &#x2F; latent token）</li>
<li>对所有条件 token 的相关性</li>
</ul>
<hr>
<p>在 token 维度上归一化：</p>
<ul>
<li>A &#x3D; softmax(S, dim &#x3D; -1)</li>
<li>A ∈ R^{B × (H·W) × T}</li>
</ul>
<p>含义：</p>
<ul>
<li>对每个空间位置，所有 token 的权重和为 1</li>
</ul>
<hr>
<h4 id="5-加权求和-Value"><a href="#5-加权求和-Value" class="headerlink" title="5. 加权求和 Value"></a>5. 加权求和 Value</h4><p>计算 attention 输出：</p>
<ul>
<li>out &#x3D; A · V</li>
<li>out ∈ R^{B × (H·W) × d}</li>
</ul>
<p>含义：</p>
<ul>
<li>对每个空间位置</li>
<li>按注意力权重，从条件中提取信息</li>
</ul>
<hr>
<ul>
<li>Q → [B × num_heads × (H·W) × d]</li>
<li>K → [B × num_heads × T × d]</li>
<li>V → [B × num_heads × T × d]</li>
</ul>
<p>每个 head 独立执行第 4–5 步。</p>
<hr>
<h4 id="6-2-合并-heads"><a href="#6-2-合并-heads" class="headerlink" title="6.2 合并 heads"></a>6.2 合并 heads</h4><ul>
<li>concat(out_heads) → [B × (H·W) × (num_heads · d)]</li>
<li>线性投影 →</li>
<li>out ∈ R^{B × (H·W) × C}</li>
</ul>
<hr>
<h4 id="7-1-恢复特征图形状"><a href="#7-1-恢复特征图形状" class="headerlink" title="7.1 恢复特征图形状"></a>7.1 恢复特征图形状</h4><ul>
<li>out ∈ R^{B × (H·W) × C}</li>
<li>reshape →</li>
<li>out ∈ R^{B × C × H × W}</li>
</ul>
<h4 id="7-2-残差连接"><a href="#7-2-残差连接" class="headerlink" title="7.2 残差连接"></a>7.2 残差连接</h4><ul>
<li>h_out &#x3D; h_in + out</li>
</ul>
<hr>
<h4 id="8-关键总结"><a href="#8-关键总结" class="headerlink" title="8. 关键总结"></a>8. 关键总结</h4><ul>
<li>Query 来自 <strong>图像 latent 特征</strong></li>
<li>Key &#x2F; Value 来自 <strong>条件 embedding</strong></li>
<li>UNet 对条件来源（CLIP &#x2F; 连续 prompt）不敏感</li>
<li>只要 embedding 分布可用，cross-attention 即可正常工作</li>
</ul>
<hr>
<p>这是传统SD的思想，promptus用只在训练低秩文本条件（U、V），模型权重全部冻结<br>c &#x3D; (U @ V &#x2F; sqrt(rank)).unsqueeze(0)<br>c &#x3D; {‘crossattn’: c}</p>
<table>
<thead>
<tr>
<th>张量</th>
<th>形状</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>U</td>
<td>(77, r)</td>
<td>token 维度</td>
</tr>
<tr>
<td>V</td>
<td>(r, 1024)</td>
<td>cross-attention channel</td>
</tr>
<tr>
<td>U @ V</td>
<td>(77, 1024)</td>
<td>等价于 CLIP text embedding</td>
</tr>
<tr>
<td>c[‘crossattn’]</td>
<td>(1, 77, 1024)</td>
<td>直接喂给 UNet</td>
</tr>
</tbody></table>
<h3 id="Akio-Kodaira-Chenfeng-Xu-Toshiki-Hazama-Takanori-Yoshimoto-Kohei-Ohno-Shogo-Mitsuhori-Soichi-Sug-ano-Hanying-Cho-Zhijian-Liu-and-Kurt-Keutzer-Streamdiffusion-A-pipeline-level-solution-for-real-time-interactive-generation-arXiv-preprint-arXiv-2312-12491-2023"><a href="#Akio-Kodaira-Chenfeng-Xu-Toshiki-Hazama-Takanori-Yoshimoto-Kohei-Ohno-Shogo-Mitsuhori-Soichi-Sug-ano-Hanying-Cho-Zhijian-Liu-and-Kurt-Keutzer-Streamdiffusion-A-pipeline-level-solution-for-real-time-interactive-generation-arXiv-preprint-arXiv-2312-12491-2023" class="headerlink" title="Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sug- ano, Hanying Cho, Zhijian Liu, and Kurt Keutzer. Streamdiffusion: A pipeline-level solution for real-time interactive generation. arXiv preprint arXiv:2312.12491, 2023."></a>Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sug- ano, Hanying Cho, Zhijian Liu, and Kurt Keutzer. Streamdiffusion: A pipeline-level solution for real-time interactive generation. arXiv preprint arXiv:2312.12491, 2023.</h3><h3 id="ELIC-Efficient-Learned-Image-Compression-with-Unevenly-Grouped-Space-Channel-Contextual-Adaptive-Coding"><a href="#ELIC-Efficient-Learned-Image-Compression-with-Unevenly-Grouped-Space-Channel-Contextual-Adaptive-Coding" class="headerlink" title="ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding"></a>ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding</h3><h3 id="taesd"><a href="#taesd" class="headerlink" title="taesd"></a>taesd</h3><p>TAESD is very tiny autoencoder which uses the same “latent API” as Stable Diffusion’s VAE. TAESD can decode Stable Diffusion’s latents into full-size images at zero cost.<br>SD VAE*	TAESD<br>Parameters in Encoder	34,163,592	1,222,532<br>Parameters in Decoder	49,490,179	1,222,531</p>
<h3 id="Video-Compression-With-Rate-Distortion-Autoencoders"><a href="#Video-Compression-With-Rate-Distortion-Autoencoders" class="headerlink" title="Video Compression With Rate-Distortion Autoencoders"></a>Video Compression With Rate-Distortion Autoencoders</h3><p>PixelCNN 和 Hyperprior 都可以给出精准的p(z)<br>交叉熵：<br>$H(q,p) &#x3D; -\sum_{x}q(x)log p(x)$.<br>用错误或者近似的模型编码真实数据<br>H(q,p) &#x3D; H(q) + KL(q||p)<br>对于VAE模型</p>
<ul>
<li>后验（近似后验）<br>q(z|x) 来自VAE的编码器</li>
<li>先验<br>p(z)来自code model（PixelCNN&#x2F;hyperprior）</li>
</ul>
<h1 id="对于hyperprior里p-z-（先验）通常不是N-0-I-所以闭式KL不再适用，而是直接用Rate交叉熵。-R≈E-−logp-y-​∣z-E-−logp-z-而不是：-mathrm-KL-left-mathcal-N-left-boldsymbol-mu-operatorname-diag-boldsymbol-sigma-2-right-middle-mathcal-N-mathbf-0-mathbf-I-right"><a href="#对于hyperprior里p-z-（先验）通常不是N-0-I-所以闭式KL不再适用，而是直接用Rate交叉熵。-R≈E-−logp-y-​∣z-E-−logp-z-而不是：-mathrm-KL-left-mathcal-N-left-boldsymbol-mu-operatorname-diag-boldsymbol-sigma-2-right-middle-mathcal-N-mathbf-0-mathbf-I-right" class="headerlink" title="对于hyperprior里p(z)（先验）通常不是N(0,I),所以闭式KL不再适用，而是直接用Rate交叉熵。$R≈E[−logp(y^​∣z^)]+E[−logp(z^)]$. 而不是：$$\mathrm{KL}!\left(\mathcal{N}!\left(\boldsymbol{\mu}, \operatorname{diag}(\boldsymbol{\sigma}^2)\right);\middle|;\mathcal{N}(\mathbf{0}, \mathbf{I})\right)"></a>对于hyperprior里p(z)（先验）通常不是N(0,I),所以闭式KL不再适用，而是直接用Rate交叉熵。$R≈E[−logp(y^​∣z^)]+E[−logp(z^)]$. 而不是：<br>$$<br>\mathrm{KL}!\left(<br>\mathcal{N}!\left(\boldsymbol{\mu}, \operatorname{diag}(\boldsymbol{\sigma}^2)\right)<br>;\middle|;<br>\mathcal{N}(\mathbf{0}, \mathbf{I})<br>\right)</h1><h2 id="frac-1-2-sum-j-1-d-left-mu-j-2-sigma-j-2"><a href="#frac-1-2-sum-j-1-d-left-mu-j-2-sigma-j-2" class="headerlink" title="\frac{1}{2}\sum_{j&#x3D;1}^{d}\left(\mu_j^2+\sigma_j^2"></a>\frac{1}{2}<br>\sum_{j&#x3D;1}^{d}<br>\left(<br>\mu_j^2<br>+<br>\sigma_j^2</h2><h2 id="log-sigma-j-2"><a href="#log-sigma-j-2" class="headerlink" title="\log \sigma_j^2"></a>\log \sigma_j^2</h2><p>1<br>\right).<br>$$</p>
<p>对真实分布取期望：<br>$Eq​[B]​&#x3D;Eq​[−log2​p(y<del>​,z</del>)]&#x3D;H(q)+KL(q(y<del>​,z</del>)∥p(y<del>​,z</del>))​$.<br>训练时直接最小化交叉熵就等价于最小化 KL<br>$min​Eq​[−logp]⟺min​KL(q∥p)$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/28/12-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/28/12-28/" class="post-title-link" itemprop="url">12-28</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-28 01:29:08 / Modified: 01:39:09" itemprop="dateCreated datePublished" datetime="2025-12-28T01:29:08+08:00">2025-12-28</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <p>所有snr的<br>awgn错误生成10个左右</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/21/12-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/21/12-21/" class="post-title-link" itemprop="url">12-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-21 12:35:56" itemprop="dateCreated datePublished" datetime="2025-12-21T12:35:56+08:00">2025-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-12-27 20:24:51" itemprop="dateModified" datetime="2025-12-27T20:24:51+08:00">2025-12-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="steps"><a href="#steps" class="headerlink" title="steps"></a>steps</h2><h3 id="step1"><a href="#step1" class="headerlink" title="step1"></a>step1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python <span class="string">&quot;/Huang_group/zyyz/Projects/Yolo13/yolov13-main/map4_to_jpg.py&quot;</span>  <span class="string">&quot;/Huang_group/zyyz/Projects/Wireless-Channel/simulation/ori_video/ori_2x_2_en_ori/0/eb157ff9-d7b3-45a8-bd57-c2b0871183f9/1920x1056_ori.mp4&quot;</span>  -o <span class="string">&quot;/Huang_group/zyyz/Projects/Wireless-Channel/simulation/test_1221/eb157ff9-d7b3-45a8-bd57-c2b0871183f9/jpg&quot;</span></span><br></pre></td></tr></table></figure>




<h3 id="step2"><a href="#step2" class="headerlink" title="step2"></a>step2</h3><p>python &#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Yolo13&#x2F;yolov13-main&#x2F;FrameToGoPs.py</p>
<h3 id="step3"><a href="#step3" class="headerlink" title="step3"></a>step3</h3><p>.&#x2F;main.sh</p>
<h3 id="step4"><a href="#step4" class="headerlink" title="step4"></a>step4</h3><p>python &#x2F;Huang_group&#x2F;zyyz&#x2F;tianyi&#x2F;new_cosmos&#x2F;export_token_map.py</p>
<h3 id="step5"><a href="#step5" class="headerlink" title="step5"></a>step5</h3><p>python &#x2F;Huang_group&#x2F;zyyz&#x2F;tianyi&#x2F;new_cosmos&#x2F;mask_tile_1217_hcy_E.py</p>
<h3 id="step6"><a href="#step6" class="headerlink" title="step6"></a>step6</h3><p>.&#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Wireless-Channel&#x2F;ZYYZ_power&#x2F;run_all_gops.sh</p>
<h3 id="step7"><a href="#step7" class="headerlink" title="step7"></a>step7</h3><p> python .&#x2F;mask_tile_1210_hcy_decode.py –config &#x2F;Huang_group&#x2F;zyyz&#x2F;tianyi&#x2F;new_cosmos&#x2F;test_config&#x2F;cosmos_2x_ori_en.yaml</p>
<h2 id="H264-H265"><a href="#H264-H265" class="headerlink" title="H264 H265"></a>H264 H265</h2><h3 id="视频生成"><a href="#视频生成" class="headerlink" title="视频生成"></a>视频生成</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 475kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_475   --input test_1221  --bitrate 415 --batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 450kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_450   --input test_1221  --bitrate 390 --batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 425kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_425   --input test_1221  --bitrate 370  --batch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 400kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_400   --input test_1221  --bitrate 340  --batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 375kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_375   --input test_1221  --bitrate 315  --batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 350kbps</span></span><br><span class="line">python compress_and_upscale.py     --out /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_350   --input test_1221  --bitrate 290  --batch</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="加噪"><a href="#加噪" class="headerlink" title="加噪"></a>加噪</h3><h4 id="265"><a href="#265" class="headerlink" title="265"></a>265</h4><ul>
<li><p>单一视频<br>python &#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Wireless-Channel&#x2F;VVC_encoder_202512242304_BER_H265.py <br>–input &#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Wireless-Channel&#x2F;simulation&#x2F;encoded_videos_kbps_425&#x2F;b4988fb8-bcdb-4b80-adee-0cdccfa8470d&#x2F;h265&#x2F;b4988fb8-bcdb-4b80-adee-0cdccfa8470d_1920x1056_h265_bicubic_413k.mp4 <br>–output &#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Wireless-Channel&#x2F;simulation&#x2F;encoded_videos_kbps_425_ber0.05_1224&#x2F;0efe4907-c085-4ea5-9c70-c7bad0bb5493&#x2F;snr0.mp4 <br>–error_rate 0.01</p>
</li>
<li><p>batch<br>python &#x2F;Huang_group&#x2F;zyyz&#x2F;Projects&#x2F;Wireless-Channel&#x2F;VVC_encoder_202512242304_BER_H265.py</p>
</li>
</ul>
<h4 id="264"><a href="#264" class="headerlink" title="264"></a>264</h4><ul>
<li>单一视频</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python /Huang_group/zyyz/Projects/Wireless-Channel/VVC_encoder_202512242243_BER_H264.py \</span><br><span class="line">  --input /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_425/4b9ad4dd-d069-4e13-928b-3505124e1bba/h264/4b9ad4dd-d069-4e13-928b-3505124e1bba_1920x1056_h264_bicubic_430k.mp4 \</span><br><span class="line">  --output /Huang_group/zyyz/Projects/Wireless-Channel/simulation/encoded_videos_kbps_425_ber0.05_1224/0efe4907-c085-4ea5-9c70-c7bad0bb5493/snr-4.mp4 \</span><br><span class="line">  --error_rate 0.05 \</span><br><span class="line">  --slices 1000</span><br></pre></td></tr></table></figure>



<ul>
<li>batch</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python /Huang_group/zyyz/Projects/Wireless-Channel/VVC_encoder_202512242243_BER_H264.py</span><br></pre></td></tr></table></figure>


<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&#x2F;Users&#x2F;hechenyi&#x2F;Desktop&#x2F;视频&#x2F;test_1225&#x2F;Results</p>
<h3 id="视频集"><a href="#视频集" class="headerlink" title="视频集"></a>视频集</h3><ul>
<li>指标 vs snr<br>&#x2F;Users&#x2F;hechenyi&#x2F;Desktop&#x2F;vmaf&#x2F;python&#x2F;vmaf&#x2F;test_1225</li>
<li>指标 vs bandwidh<br>&#x2F;Users&#x2F;hechenyi&#x2F;Desktop&#x2F;vmaf&#x2F;python&#x2F;vmaf&#x2F;test_1225</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/21/12-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/21/12-20/" class="post-title-link" itemprop="url">12-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-21 01:47:42 / Modified: 01:52:03" itemprop="dateCreated datePublished" datetime="2025-12-21T01:47:42+08:00">2025-12-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="“Latent-Diffusion-Model-Based-Denoising-Receiver-for-6G-Semantic-Communication-From-Stochastic-Differential-Theory-to-Application”"><a href="#“Latent-Diffusion-Model-Based-Denoising-Receiver-for-6G-Semantic-Communication-From-Stochastic-Differential-Theory-to-Application”" class="headerlink" title="“Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application”"></a>“Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application”</h3><ul>
<li>信道输出：<br>y &#x3D; z + n</li>
<li>经过scaling 与 SNR-t 映射<br>$x_t​ &#x3D; αy$</li>
<li>然后<br>$ \hat z &#x3D; reverse(x_t​,t)$</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/19/12-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/19/12-19/" class="post-title-link" itemprop="url">12-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-19 14:23:12 / Modified: 14:36:46" itemprop="dateCreated datePublished" datetime="2025-12-19T14:23:12+08:00">2025-12-19</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="data-sheet"><a href="#data-sheet" class="headerlink" title="data sheet"></a>data sheet</h2><table>
<thead>
<tr>
<th>参数</th>
<th>符号</th>
<th>取值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>信息比特数</td>
<td>(K)</td>
<td><strong>256 bit</strong></td>
<td>每个 packet &#x2F; codeword 含 4×4 个 token，每个 16 bit</td>
</tr>
<tr>
<td>码长</td>
<td>(D)</td>
<td><strong>512 bit</strong></td>
<td>每个 codeword 的编码后比特数</td>
</tr>
<tr>
<td>码率</td>
<td>($R_c$)</td>
<td><strong>0.5</strong></td>
<td>(R_c &#x3D; K&#x2F;D &#x3D; 256&#x2F;512)</td>
</tr>
<tr>
<td>调制阶数</td>
<td>(M)</td>
<td><strong>2（BPSK）</strong></td>
<td>每个符号 2 bit</td>
</tr>
<tr>
<td>每符号比特</td>
<td>($\log_2 M$)</td>
<td><strong>1 bit&#x2F;symbol</strong></td>
<td>QPSK</td>
</tr>
<tr>
<td>资源块带宽</td>
<td>($f_s$)</td>
<td><strong>20 khz</strong></td>
<td>每个子载波每秒 1 万个符号</td>
</tr>
<tr>
<td>资源块数</td>
<td>(B)</td>
<td><strong>30</strong></td>
<td>并行 30 条子载波</td>
</tr>
<tr>
<td>总吞吐率</td>
<td>($R_{\text{ch}}$)</td>
<td><strong>300 kbps</strong></td>
<td>由上面参数共同决定</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/18/12-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/18/12-18/" class="post-title-link" itemprop="url">12-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-12-18 17:03:30" itemprop="dateCreated datePublished" datetime="2025-12-18T17:03:30+08:00">2025-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-01-03 16:22:03" itemprop="dateModified" datetime="2026-01-03T16:22:03+08:00">2026-01-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="reading"><a href="#reading" class="headerlink" title="reading"></a>reading</h2><h3 id="”DEEP-COMPRESSION-AUTOENCODER-FOR-EFFICIENT-HIGH-RESOLUTION-DIFFUSION-MODELS“"><a href="#”DEEP-COMPRESSION-AUTOENCODER-FOR-EFFICIENT-HIGH-RESOLUTION-DIFFUSION-MODELS“" class="headerlink" title="”DEEP COMPRESSION AUTOENCODER FOR EFFICIENT HIGH-RESOLUTION DIFFUSION MODELS“"></a>”DEEP COMPRESSION AUTOENCODER FOR EFFICIENT HIGH-RESOLUTION DIFFUSION MODELS“</h3><p>首次引入了DC-AE。<br>降低HW来减少token数目</p>
<h4 id="reconstruction-用于semantic-communication"><a href="#reconstruction-用于semantic-communication" class="headerlink" title="reconstruction(用于semantic communication)"></a>reconstruction(用于semantic communication)</h4><p>Reconstructed images by DC-AE demonstrate a better visual quality than SD-VAE’s reconstructed images. In particular, for the f64 and f128 autoencoders, DC-AE still maintains a good visual quality for small text and the human face.<br>也就是说dc-ae的编解码器的重建效果会更好</p>
<h3 id="Adversarial-Diffusion-Distillation"><a href="#Adversarial-Diffusion-Distillation" class="headerlink" title="Adversarial Diffusion Distillation"></a>Adversarial Diffusion Distillation</h3><p>扩散模型（Diffusion Models, DMs）在文本生成图像任务中具有极高的生成质量和语义一致性，但其推理过程需要几十步迭代采样，难以满足实时生成需求；<br>而 GAN 具有单步生成、速度快的优势，却在高分辨率、复杂文本条件下的图像质量与稳定性方面明显落后于扩散模型。<br>ADD 的学生模型本身仍采用 U-Net 扩散模型结构，因此：</p>
<ul>
<li><p>可单步生成（类似 GAN）</p>
</li>
<li><p>也支持多步迭代精修（扩散模型特性）</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/sd-turbo">https://huggingface.co/stabilityai/sd-turbo</a><br>SD-turbo is a distilled version of stable diffusion 2.1, trained for real-time synthesis. It uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.</p>
<h3 id="Open-Sora-2-0-Training-a-Commercial-Level-Video-Generation-Model-in-200k"><a href="#Open-Sora-2-0-Training-a-Commercial-Level-Video-Generation-Model-in-200k" class="headerlink" title="Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k"></a>Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k</h3><p>用DC-AE实现了视频编码器</p>
<h3 id="StableCodec-Taming-One-Step-Diffusion-for-Extreme-Image-Compression"><a href="#StableCodec-Taming-One-Step-Diffusion-for-Extreme-Image-Compression" class="headerlink" title="StableCodec: Taming One-Step Diffusion for Extreme Image Compression"></a>StableCodec: Taming One-Step Diffusion for Extreme Image Compression</h3><p>将SD-Turbo成功引入图像压缩。<br>讲latent进一步下采样到64x &#x2F; 256x<br>结合深层变换网络 + 自回归熵模型<br>引入一个以率失真优化为目标的高码率编码器（来自 ELIC）用来补充SD-VAE中缺失的像素级结构信息</p>
<h3 id="D-B-Kurka-and-D-G-̈-und-̈-uz-“Deepjscc-f-Deep-joint-source-channel-coding-of-images-with-feedback-”-IEEE-J-Sel-Areas-Inf-Theory-vol-1-no-1-pp-178–193-Apr-2020"><a href="#D-B-Kurka-and-D-G-̈-und-̈-uz-“Deepjscc-f-Deep-joint-source-channel-coding-of-images-with-feedback-”-IEEE-J-Sel-Areas-Inf-Theory-vol-1-no-1-pp-178–193-Apr-2020" class="headerlink" title="D. B. Kurka and D. G ̈ und  ̈ uz, “Deepjscc-f: Deep joint source-channel coding of images with feedback,” IEEE J. Sel. Areas Inf. Theory, vol. 1, no. 1, pp. 178–193, Apr. 2020."></a>D. B. Kurka and D. G ̈ und  ̈ uz, “Deepjscc-f: Deep joint source-channel coding of images with feedback,” IEEE J. Sel. Areas Inf. Theory, vol. 1, no. 1, pp. 178–193, Apr. 2020.</h3><p>可以作为DeepJSCC的baseline</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/12/17/12-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/12/17/12-17/" class="post-title-link" itemprop="url">12-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-12-17 11:05:58 / Modified: 20:57:01" itemprop="dateCreated datePublished" datetime="2025-12-17T11:05:58+08:00">2025-12-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="联通-ViVo"><a href="#联通-ViVo" class="headerlink" title="联通 ViVo"></a>联通 ViVo</h2><p>端-边-云 三层智能体协同架构<br>实现用户实时网络需求感知、网络资源动态调度</p>
<h3 id="终端智能体（User-Intent-Agent）：手机终端"><a href="#终端智能体（User-Intent-Agent）：手机终端" class="headerlink" title="终端智能体（User Intent Agent）：手机终端"></a>终端智能体（User Intent Agent）：手机终端</h3><p>功能： 不仅是识别APP，而是识别“任务上下文”。<br>关键细节：以游戏场景为例，识别当前是高交互游戏场景（如MOBA团战）还是弱交互场景（如挂机）。识别设备物理状态（电池&lt;10%，机身温度&gt;40℃），决定是否发起“计算卸载”请求。<br>模型部署：<br>运行small model（如MobileBert剪枝版）。负责将复杂的屏幕内容和用户操作，瞬间“转译”为网络能听懂的结构化需求（如 QCI&#x2F;5QI 等级请求）。</p>
<h3 id="网侧边缘智能体-Edge-RAN-Agent-：基站"><a href="#网侧边缘智能体-Edge-RAN-Agent-：基站" class="headerlink" title="网侧边缘智能体 (Edge RAN Agent) ：基站"></a>网侧边缘智能体 (Edge RAN Agent) ：基站</h3><p>功能： 在基站侧实现快速网络资源调配。<br>关键细节： 部署于 BBU&#x2F;DU (基带处理单元)，实时监控 RB (资源块) 利用率和空口干扰，实时判断基站当前的资源紧张程度、是否有弱覆盖、是否出现突发拥塞。当终端发起高优请求时，它能立即执行抢占式调度或双连接 (DC) 激活，无需回传核心网，时延控制在 10ms 以内。<br>模型部署：<br>运行 Small Model 。实时判断基站当前的资源紧张程度、是否有弱覆盖、是否出现突发拥塞、边缘算力是否足够，以实时响应用户需求。</p>
<h3 id="云端智能体-Global-Policy-Agent-：核心网"><a href="#云端智能体-Global-Policy-Agent-：核心网" class="headerlink" title="云端智能体 (Global Policy Agent)：核心网"></a>云端智能体 (Global Policy Agent)：核心网</h3><p>功能： 跨域网络调度编排与规则仲裁。<br>关键细节： 处理跨小区移动性管理（如预测用户行进路线，提前通知下一个基站预留资源）。处理公平性策略，防止单个高优用户挤占过多公共资源</p>
<p>运行 Large Model (垂直行业大模型)。负责复杂故障的根因分析 (RCA) 和长期策略优化。</p>
<h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>“外挂软件”运行机制 ：感知 -&gt; 决策 -&gt; 执行</p>
<ul>
<li>感知输入: 智能体通过 MR流数据&#x2F;DPI&#x2F;网管PM指标 实时“旁路监听”网络状态。</li>
<li>智能决策: 部署于联通MEC&#x2F;中台，基于强化学习模型，结合终端意图进行策略计算。</li>
<li>参数注入 : 不修改基站代码，而是通过 MML指令&#x2F;北向API 动态下发配置参数。</li>
</ul>
<p>关键参数与控制能力 (核心：把参数作为Agent的“执行动作”)</p>
<ul>
<li><p>移动性参数 —— 切换<br>核心参数： CIO (小区个体偏移), A3 Offset, TTT (触发时间)。<br>Agent动作： “诱导”切换。<br>作用机制： 当Agent预测到前方拥塞或弱覆盖时，动态调大邻区CIO，欺骗基站算法，强制终端提前或延后切换。<br>解决痛点： 解决高铁&#x2F;高速场景下的“切换不及时”导致的掉话。</p>
</li>
<li><p>QoS&#x2F;调度参数—— 优先<br>核心参数： 5QI (QoS等级), ARP (保留优先级), AMBR (最大比特率)。<br>Agent动作： 动态优先级提升。<br>作用机制： 当识别到VIP用户或其他关键任务时，通过核心网PCF接口临时修改专承载参数，让基站调度器优先分配RB资源。<br>解决痛点： 解决重载场景下关键业务卡顿</p>
</li>
<li><p>覆盖&#x2F;干扰参数—— 信号形状<br>核心参数： Antenna Tilt (电子下倾角), Beam Pattern ID (波束场景ID)。<br>Agent动作： 场景化波束重构。<br>作用机制： 针对特定时间段（如演唱会），调用预设波束权值模板。<br>解决痛点： 解决低空干扰大或地面覆盖盲区问题。</p>
</li>
</ul>
<h3 id="To-do"><a href="#To-do" class="headerlink" title="To do"></a>To do</h3><h4 id="诊断agent"><a href="#诊断agent" class="headerlink" title="诊断agent"></a>诊断agent</h4><p>在端侧或网络侧执行执行问题识别，即负责发现终端、网络侧问题。</p>
<ul>
<li>Ph1 “诊断agent”以“场景识别” 为主， 但“场景识别” 不局限在“高速移动场景”，覆盖进入电梯、地库、人口密集场所的识别，为后期场景扩展做好准备。</li>
<li>Ph2 “诊断agent” 可深入到性能层面问题的识别和发现（需要大模型理解通信和协议的能力） 。</li>
</ul>
<h4 id="执行agent："><a href="#执行agent：" class="headerlink" title="执行agent："></a>执行agent：</h4><p>基于端侧或网络侧诊断agent输出结果，执行优化动作。</p>
<ul>
<li>Ph1：简化执行agent难度，执行基于特定问题的特定优化动作。</li>
<li>Ph2：提升“执行Agent”能力，实现更多样化、自主的优化策略（需要大模型理解通信和协议的能力）。<br>执行agent可分别部署在端侧和网络侧，也可以只部署在网络侧；</li>
</ul>
<h2 id="simulation"><a href="#simulation" class="headerlink" title="simulation"></a>simulation</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/9/">9</a><a class="extend next" rel="next" href="/blog/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">84</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
