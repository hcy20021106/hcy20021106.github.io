<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hechenyi.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://hechenyi.github.io/blog/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hechenyi.github.io/blog/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/29/channel-coding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/29/channel-coding/" class="post-title-link" itemprop="url">channel coding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-29 10:50:15 / Modified: 10:58:27" itemprop="dateCreated datePublished" datetime="2025-08-29T10:50:15+08:00">2025-08-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><ul>
<li><strong>信道编码 (Channel Coding)</strong>：在数字通信中，为了抵抗信道噪声和衰落，将信息比特加冗余形成码字。</li>
<li><strong>目标</strong>：<ul>
<li>提高抗噪能力</li>
<li>减少误码率</li>
<li>在有限信道资源下尽可能可靠传输</li>
</ul>
</li>
</ul>
<h2 id="2-块码-Block-Codes"><a href="#2-块码-Block-Codes" class="headerlink" title="2. 块码 (Block Codes)"></a>2. 块码 (Block Codes)</h2><ul>
<li>描述形式：<strong>(N, L) 块码</strong><ul>
<li>N：信息比特数</li>
<li>L：编码比特数（码字长度）</li>
<li>码率：Rc &#x3D; N &#x2F; L</li>
</ul>
</li>
<li>功能：<ul>
<li>将 N 个信息比特编码成 L 个比特</li>
<li>添加冗余，提高鲁棒性</li>
</ul>
</li>
</ul>
<h3 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h3><ul>
<li>LDPC 编码<ul>
<li>输入：100 个信息比特</li>
<li>输出：200 个编码比特</li>
<li>Rc &#x3D; 100 &#x2F; 200 &#x3D; 1&#x2F;2</li>
</ul>
</li>
<li>输出的 200 个编码比特还属于比特序列，未调制前不能直接发送</li>
</ul>
<h2 id="3-调制-Modulation"><a href="#3-调制-Modulation" class="headerlink" title="3. 调制 (Modulation)"></a>3. 调制 (Modulation)</h2><ul>
<li>将编码比特映射为可在物理信道上发送的信号</li>
<li>例子：16-QAM<ul>
<li>每个符号承载 4 个比特</li>
<li>200 个编码比特 → 50 个 QAM 符号</li>
</ul>
</li>
<li>调制后的符号是复数信号点 (I&#x2F;Q)</li>
</ul>
<h2 id="4-端到端流程-传统数字通信"><a href="#4-端到端流程-传统数字通信" class="headerlink" title="4. 端到端流程 (传统数字通信)"></a>4. 端到端流程 (传统数字通信)</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">源编码器（JPEG/BPG）</span><br><span class="line">|</span><br><span class="line">信道编码器（LDPC/Polar）</span><br><span class="line">|</span><br><span class="line">调制器(QAM/BPSK)</span><br><span class="line">|</span><br><span class="line">信道传输</span><br><span class="line">|</span><br><span class="line">解码器</span><br><span class="line">|</span><br><span class="line">信道解码器</span><br><span class="line">|</span><br><span class="line">源解码器</span><br></pre></td></tr></table></figure>

<h2 id="5-通用-N-L-块码理解"><a href="#5-通用-N-L-块码理解" class="headerlink" title="5. 通用(N,L)块码理解"></a>5. 通用(N,L)块码理解</h2><ul>
<li>论文中常用抽象描述：<ul>
<li>“通用信道编码器” &#x3D;&#x3D; 抽象块码模块</li>
<li>可以选择任意具体实现：<ul>
<li>LDPC</li>
<li>Polar Code</li>
<li>BCH</li>
</ul>
</li>
</ul>
</li>
<li>输入： N比特</li>
<li>输出： L个比特</li>
<li>调制映射后形成物理符号发送</li>
</ul>
<h2 id="6-JSCC与深度学习端到端通信"><a href="#6-JSCC与深度学习端到端通信" class="headerlink" title="6. JSCC与深度学习端到端通信"></a>6. JSCC与深度学习端到端通信</h2><ul>
<li><p><strong>端到端联合源信道编码（JSCC）</strong></p>
<ul>
<li>不再分开源编码、信道编码和调制</li>
<li>一个深度网络直接学习：<ul>
<li>压缩特征</li>
<li>抗噪鲁棒性</li>
<li>可调制映射</li>
</ul>
</li>
</ul>
</li>
<li><p>例：MDJCM</p>
<ul>
<li>Encoder：输出可在信道直接传输的符号序列</li>
<li>Decoder：直接恢复原始数据</li>
<li>不依赖传统信道编码器</li>
</ul>
</li>
</ul>
<h2 id="7-笔记总结"><a href="#7-笔记总结" class="headerlink" title="7. 笔记总结"></a>7. 笔记总结</h2><ol>
<li>信道编码器是为了在数字传输中增加冗余保护比特</li>
<li>(N, L) 块码是最常用抽象形式</li>
<li>输出编码比特在调制前仍是比特，调制后成为符号（复数信号）</li>
<li>JSCC&#x2F;MDJCM 是端到端学习方法，可替代传统源编码 + 信道编码 + 调制流程</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/28/JSCC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/28/JSCC/" class="post-title-link" itemprop="url">JSCC</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-28 19:27:17 / Modified: 21:48:39" itemprop="dateCreated datePublished" datetime="2025-08-28T19:27:17+08:00">2025-08-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/5g/" itemprop="url" rel="index"><span itemprop="name">5g</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="JSCC模型说明"><a href="#JSCC模型说明" class="headerlink" title="JSCC模型说明"></a>JSCC模型说明</h2><p>JSCC（Joint Source-Channel Coding）数字联合信源信道编码，特点包括：</p>
<ul>
<li><strong>端到端训练</strong>：支持latent feature -&gt; 调制符号的可微训练</li>
<li><strong>软条件化</strong>（Soft Conditioning）：编码器可根据调制阶数（M-QAM）和信道SNR自适应调制特征  </li>
<li><strong>噪声近似训练</strong>：训练阶段使用连续噪声替代调制&#x2F;解调操作，使梯度可回传</li>
</ul>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="1-输入信号处理"><a href="#1-输入信号处理" class="headerlink" title="1. 输入信号处理"></a>1. 输入信号处理</h3><ul>
<li><strong>AnalysisTransform (NTC)</strong>：将原始图像降维到 latent feature，输出形状 <code>[B, C, H&#39;, W&#39;]</code>  </li>
<li>降维作用：<ul>
<li>将图像信息压缩成潜在特征  </li>
<li>便于 JSCC 编码器处理</li>
</ul>
</li>
</ul>
<h3 id="2-JSCC编码器（JSCCEncoder）"><a href="#2-JSCC编码器（JSCCEncoder）" class="headerlink" title="2. JSCC编码器（JSCCEncoder）"></a>2. JSCC编码器（JSCCEncoder）</h3><p>主要组成：</p>
<ul>
<li><strong>Swin Transformer Blocks</strong>：提取高级特征  </li>
<li><strong>Rate Adaption Layer</strong>：根据熵模型和信道需求动态调整编码长度  </li>
<li><strong>MergingNet (Modulation Conditional Module, MCM)</strong>：<ul>
<li>输入：latent feature + 调制阶数 M  </li>
<li>对特征进行 <strong>软条件化</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x * softplus(M_onehot1) + M_onehot2</span><br></pre></td></tr></table></figure></li>
<li>作用：让编码器特征适应不同调制阶数</li>
</ul>
</li>
<li><strong>AF (Adaptive Feature Layer)</strong>：<ul>
<li>输入：latent feature + 信道 SNR  </li>
<li>对特征进行缩放和平移：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = z * scale + shift</span><br></pre></td></tr></table></figure></li>
<li>作用：信道条件自适应，提升鲁棒性</li>
</ul>
</li>
<li>输出：经过软条件化和可选调制处理的连续 latent feature</li>
</ul>
<h3 id="3-调制近似"><a href="#3-调制近似" class="headerlink" title="3. 调制近似"></a>3. 调制近似</h3><ul>
<li>将连续latent feature 映射成离散QAM符号</li>
<li>训练阶段使用<strong>均匀噪声近似调制</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = sqrt(3*power / (2*M-2))</span><br><span class="line">u ~ Uniform(-d, d)</span><br><span class="line">channel_input += u</span><br></pre></td></tr></table></figure>
<p>其中M是调制阶数，d是半符号间距，用来保证噪声幅度与符号间距一致</p>
<h3 id="4-调制与信道"><a href="#4-调制与信道" class="headerlink" title="4. 调制与信道"></a>4. 调制与信道</h3><h4 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h4><ul>
<li>连续latent feature -》 正常QAM调制</li>
<li>经过AWGN或其他信道传输</li>
</ul>
<h4 id="训练阶段："><a href="#训练阶段：" class="headerlink" title="训练阶段："></a>训练阶段：</h4><ul>
<li>使用连续噪声近似调制，使梯度可回传</li>
</ul>
<p>并且支持不同调制阶数的条件训练</p>
<h3 id="5-JSCC解码器"><a href="#5-JSCC解码器" class="headerlink" title="5. JSCC解码器"></a>5. JSCC解码器</h3><ul>
<li>将收到的信道符号（或连续近似特征）解码为latent feature</li>
<li>可结合hyperprior</li>
</ul>
<h2 id="理论支持"><a href="#理论支持" class="headerlink" title="理论支持"></a>理论支持</h2><h3 id="符号平均功率归一化："><a href="#符号平均功率归一化：" class="headerlink" title="符号平均功率归一化："></a>符号平均功率归一化：</h3><p>对于 M-QAM（平方型），设符号幅度按标准格排列：</p>
<ul>
<li>水平&#x2F;垂直方向的符号坐标:{$-\sqrt M + 1$, $-\sqrt M + 3$, … , $\sqrt M - 1$}. </li>
<li>平均符号功率$P_{avg}$为：$P_{avg} &#x3D; \frac{2(M-1)}{3}$</li>
</ul>
<h3 id="噪声近似"><a href="#噪声近似" class="headerlink" title="噪声近似"></a>噪声近似</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = sqrt(3*power / (2*modulation_order - 2))</span><br><span class="line">u ~ Uniform(-d, d)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>d对应符号间距的一半</li>
<li>对于均匀量化<br>$x_{quantized} ​≈ x_{continuous}​ + u,u ∼ Uniform(−d,d)$</li>
<li>这样训练时梯度可回传，同时模拟调制离散化误差</li>
</ul>
<h3 id="软条件化"><a href="#软条件化" class="headerlink" title="软条件化"></a>软条件化</h3><ul>
<li>调制阶数和信道SNR作为条件输入</li>
<li>通过线性层 + 非线性激活（Softplus &#x2F; Sigmoid）调整 latent feature</li>
<li>增强编码器对不同调制阶数和信道条件的适应能力</li>
</ul>
<h3 id="端到端可微训练"><a href="#端到端可微训练" class="headerlink" title="端到端可微训练"></a>端到端可微训练</h3><ul>
<li>连续噪声替代离散调制，使梯度可回传</li>
<li>后续可结合 STE 或软硬退火进行微调</li>
</ul>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><h3 id="阶段一：source-coding-training"><a href="#阶段一：source-coding-training" class="headerlink" title="阶段一：source coding training"></a>阶段一：source coding training</h3><ul>
<li><strong>目标</strong>：单独训练基于学习的图像压缩器，使其具备良好的表示能力。</li>
<li><strong>固定参数</strong>：冻结 JSC 编码器与解码器 (θf, ϕf)。</li>
<li><strong>训练参数</strong>：θg, θh, ϕg, ϕh。</li>
<li><strong>损失函数</strong>：<br>L(x, x̂, λ) &#x3D; R + λD(x, x̂)<br>其中 R 为比特率，D 为失真 (如 MSE)，λ 为拉格朗日乘子。</li>
<li><strong>说明</strong>：这一阶段只关注图像压缩，不涉及信道。</li>
</ul>
<h3 id="阶段二：JSC-encoder和JSC-decoder训练"><a href="#阶段二：JSC-encoder和JSC-decoder训练" class="headerlink" title="阶段二：JSC encoder和JSC decoder训练"></a>阶段二：JSC encoder和JSC decoder训练</h3><ul>
<li><strong>目标</strong>：在端到端系统中引入“连续噪声近似调制”，使系统在训练时保持可微，便于联合优化。</li>
<li><strong>初始化</strong>：加载阶段一训练好的参数。</li>
<li><strong>训练参数</strong>：θg, θh, θf, ϕg, ϕh, ϕf (全模型参与)。</li>
<li><strong>步骤</strong>：<ol>
<li>从训练数据中采样图像 x。</li>
<li>从目标范围内均匀采样 SNR。</li>
<li>随机选择调制阶数 M。</li>
<li>根据 M 与功率约束 Es 计算调制间距 d。</li>
<li>使用 <strong>连续噪声近似</strong> 通过信道。</li>
</ol>
</li>
<li><strong>损失函数</strong>：<br>L(x, x̂, λ, M, SNR) &#x3D; R + λ [ D(x, x̂) + D(x, x̂MD) ]<br>其中 x̂ 为压缩器重建图像，x̂MD 为经过端到端 JSCC 模块后的图像。</li>
</ul>
<h2 id="阶段三：基于-STE-的精调-STE-based-Finetuning"><a href="#阶段三：基于-STE-的精调-STE-based-Finetuning" class="headerlink" title="阶段三：基于 STE 的精调 (STE-based Finetuning)"></a>阶段三：基于 STE 的精调 (STE-based Finetuning)</h2><ul>
<li><strong>目标</strong>：解决训练与测试阶段之间的不匹配（真实调制是确定性的，不是加噪声），提升模型在实际信道上的性能。</li>
<li><strong>初始化</strong>：加载阶段二训练好的参数。</li>
<li><strong>固定参数</strong>：冻结发射端参数 (θg, θh, θf)。</li>
<li><strong>训练参数</strong>：ϕg, ϕh, ϕf (接收端)。</li>
<li><strong>步骤</strong>：<ol>
<li>重复阶段二的数据采样步骤（x, M, SNR, d）。</li>
</ol>
</li>
<li><strong>损失函数</strong>：<br>与阶段二相同：<br>L(x, x̂, λ, M, SNR) &#x3D; R + λ [ D(x, x̂) + D(x, x̂MD) ]</li>
</ul>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>其中JSC编码器&#x2F;解码器是一个神经网络模块，以SNR和mcs为输入，学习如何把特征编码成可传输的信号，再从信号中恢复。<br></p>
</blockquote>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/28/One-Shot-Coding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/28/One-Shot-Coding/" class="post-title-link" itemprop="url">One-Shot Coding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-28 15:07:00 / Modified: 15:14:32" itemprop="dateCreated datePublished" datetime="2025-08-28T15:07:00+08:00">2025-08-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/5G/" itemprop="url" rel="index"><span itemprop="name">5G</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>一次性编码，不需要序列式迭代或多步计算，输出是用向量表示</p>
<h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>将离散信息转为神经网络可处理的连续特征。因此one-shot coding后可以用MLP来处理。<br>类似NLP里的word embedding。<br>One-shot具有可微性（一对一映射），因此可以进行反向传播。</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>信号量化具有不可微性（多对一映射）<br></p>
</blockquote>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/27/Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/27/Reinforcement-Learning/" class="post-title-link" itemprop="url">Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-27 17:28:49 / Modified: 19:57:27" itemprop="dateCreated datePublished" datetime="2025-08-27T17:28:49+08:00">2025-08-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="强化学习概念"><a href="#强化学习概念" class="headerlink" title="强化学习概念"></a>强化学习概念</h2><ul>
<li>RL 是一种机器学习方法，通过与环境交互的试错经验学习最优行为（policy），以最大化累计奖励（accumulated reward）</li>
<li>与监督学习不同：RL不需要“正确答案”做训练，而是通过奖励反馈自主学习</li>
</ul>
<p>RL系统组成：</p>
<ul>
<li>Agent: 学习者和决策者</li>
<li>Environment: 智能体作用的外部系统</li>
<li>State($s_t$)： 智能体感知到的环境信息</li>
<li>Action($a_t$)：智能体在状态下采取的操作</li>
<li>Reward($r_{t+1}$)：动作的即时反馈</li>
<li>Goal: 找到最佳策略𝜋，实现状态 → 动作的最优映射，使累积奖励最大化。</li>
</ul>
<h2 id="Q-learning算法"><a href="#Q-learning算法" class="headerlink" title="Q-learning算法"></a>Q-learning算法</h2><p>核心思想：通过不断更新动作价值函数Q(s,a)来学习最佳策略<br>Q值更新公式：<br>$Q(s_t, a_t) &lt;- (1 - \alpha) * Q(s_t, a_t) + \alpha * [ r_{t+1} + \gamma * max_{a_{t+1}​∈A} * Q(s_{t+1}, a_{t+1}) ]$。</p>
<p>参数说明：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>$Q(s_t, a_t)$</td>
<td>状态 $s_t$ 下执行动作 $a_t$ 的当前 Q 值</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>学习率（0~1），控制新信息更新 Q 值的权重</td>
</tr>
<tr>
<td>$r_{t+1}$</td>
<td>执行动作 $a_t$ 后在下一个状态获得的即时奖励</td>
</tr>
<tr>
<td>$\gamma$</td>
<td>折扣因子（0~1），衡量未来奖励的重要性</td>
</tr>
<tr>
<td>$s_{t+1}$</td>
<td>执行动作后的下一状态</td>
</tr>
<tr>
<td>$a_{t+1}$</td>
<td>下一步选择的动作</td>
</tr>
<tr>
<td>$A_{\text{max}}$</td>
<td>在下一状态下可选择的最大 Q 值动作集合</td>
</tr>
<tr>
<td></td>
<td>指示函数，如果 $a_{t+1}$ 是最大动作之一，则为 1，否则为 0</td>
</tr>
</tbody></table>
<h2 id="QL-AMC"><a href="#QL-AMC" class="headerlink" title="QL-AMC"></a>QL-AMC</h2><p>基于Q-learning的自适应调制编码，用于在无线通信系统中在不同信道条件下动态选择最优MCS</p>
<h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>传统方法：<br>固定查表（look-up table）：根据SNR对应的BLER曲线预先生成CQI -&gt; MCS的映射</p>
<h3 id="QL-AMC方案"><a href="#QL-AMC方案" class="headerlink" title="QL-AMC方案"></a>QL-AMC方案</h3><p>Q(s,a): 动作-价值函数，表示在动态<strong>s</strong>下选择动作<strong>a</strong>的价值<br>State: 状态集合，在这里状态是CQI（Channel Quality Indicator），即信道质量指标<br>Action：动作集合，这里是可选的MCS（调制与编码方案）</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>Q可以理解为通过RL来推导CQI-&gt;MCS的映射，因此不局限于表格的推荐，而是根据环境反馈动态优化。</p>
</blockquote>
<hr>
<p>Reward function:</p>
<ul>
<li><p>R1 基于BLER的非线形奖励。</p>
<ul>
<li>μ: 每个符号的比特数 (bits per modulation symbol)</li>
<li>ν: 编码率 (code rate)</li>
<li>BLER: 当前传输块误块率</li>
<li>BLER_T: 系统目标 BLER (例如 10% 对 eMBB).</li>
</ul>
  <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">R1 = μ * ν     , if BLER &lt;= BLER_T</span><br><span class="line">R1 = -1        , else</span><br></pre></td></tr></table></figure>
<p>  如果MCS满足BLER目标，奖励等于理论传输速率μ*ν<br>  否则给惩罚值-1，避免过激MCS</p>
</li>
<li><p>R2 基于光谱效率的奖励。</p>
<ul>
<li>奖励考虑实际成功传输比特数</li>
<li>agent 会尝试最大化光谱效率.</li>
</ul>
  <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R2 = (1 - BLER) * μ * ν</span><br></pre></td></tr></table></figure></li>
</ul>
<p>说明：</p>
<ul>
<li>Q(s, a) 更新时结合 当前奖励 r 和 未来潜在收益 max_a Q(s’, a’)</li>
<li>随着循环进行，Q表会收敛，最终为每个CQI提供最优MCS，示例如下：</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">         MCS1   MCS2   MCS3  ...  </span><br><span class="line">CQI0      Q0,0   Q0,1   Q0,2</span><br><span class="line">CQI1      Q1,0   Q1,1   Q1,2</span><br><span class="line">CQI2      Q2,0   Q2,1   Q2,2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<ul>
<li>最后的Q表的用途是用于决策.</li>
</ul>
<p>$ a* &#x3D; argmax_a Q(s, a)$.<br>也就是对当前CQI s，选择Q值最大的MCS</p>
<ul>
<li>学习经验<br>Q表保留了 RL agent对环境的经验，反映了不同信道条件下选择MCS的最优策略</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/27/EVM-based-SNR-Calculation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/27/EVM-based-SNR-Calculation/" class="post-title-link" itemprop="url">EVM-based SNR Calculation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-27 16:47:08 / Modified: 16:52:20" itemprop="dateCreated datePublished" datetime="2025-08-27T16:47:08+08:00">2025-08-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/5G/" itemprop="url" rel="index"><span itemprop="name">5G</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="1-PSD-Based-SNR-Estimation-ANN-Method"><a href="#1-PSD-Based-SNR-Estimation-ANN-Method" class="headerlink" title="1. PSD-Based SNR Estimation (ANN Method)"></a>1. PSD-Based SNR Estimation (ANN Method)</h2><ul>
<li><p><strong>概念</strong>: PSD (Power Spectral Density) 表示接收信号功率在频率域的分布。</p>
</li>
<li><p>接收信号 r(t) &#x3D; s(t) + n(t)，其中 s(t) 是有效信号，n(t) 是噪声。</p>
</li>
<li><p>PSD 包含了信号功率和噪声功率。</p>
</li>
<li><p>ANN 方法流程:</p>
<ol>
<li>接收信号 -&gt; 提取 PSD 特征。</li>
<li>ANN 输入 PSD -&gt; 输出 SNR 分类。</li>
<li>SNR 分类映射到最优 MCS（调制与编码方案）。</li>
</ol>
</li>
<li><p>优点:</p>
<ul>
<li>不依赖解调符号或误差向量。</li>
<li>高速移动环境中稳健（PSD 不受多普勒效应显著影响）。</li>
<li>推理计算量低，实时性好。</li>
</ul>
</li>
<li><p>平均功率计算:</p>
<ul>
<li>平均功率 $P &#x3D; ∫ S_x(f) df$（连续信号）或 $P &#x3D; (1&#x2F;N) Σ |X[k]|^2$（离散信号）。</li>
<li>PSD 积分得到总功率，可用于 SNR 计算: $SNR &#x3D; P_signal &#x2F; P_noise$。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-EVM-Based-SNR-Estimation"><a href="#2-EVM-Based-SNR-Estimation" class="headerlink" title="2. EVM-Based SNR Estimation"></a>2. EVM-Based SNR Estimation</h2><ul>
<li><strong>EVM (Error Vector Magnitude)</strong> 衡量接收符号与理想符号的误差：<br>$EVM &#x3D; sqrt(Σ |r_i - s_i|^2 &#x2F; Σ |s_i|^2)$<ul>
<li>$r_i$: 接收符号</li>
<li>$s_i$: 理想符号（通常是 pilot&#x2F;reference symbols）</li>
</ul>
</li>
<li>流程:<ol>
<li>接收 pilot 符号。</li>
<li>计算误差向量 $e &#x3D; r_p - s_p$。</li>
<li>得到 EVM -&gt; 间接推算 SNR -&gt; 映射到 CQI -&gt; 选择 MCS。</li>
</ol>
</li>
<li>局限:<ul>
<li>依赖 pilot。</li>
<li>高速移动或多普勒环境下 EVM 会受到星座漂移影响，SNR 估计不稳定。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-LS-Based-SNR-Estimation"><a href="#3-LS-Based-SNR-Estimation" class="headerlink" title="3. LS-Based SNR Estimation"></a>3. LS-Based SNR Estimation</h2><ul>
<li><strong>LS (Least Squares) 方法</strong>用于信道估计:<ul>
<li>基本公式: $h_hat &#x3D; r_p &#x2F; s_p$（简单 LS）</li>
<li>信道估计后计算 SNR: $SNR &#x3D; E[|h s|^2] &#x2F; E[|r - h s|^2]$</li>
</ul>
</li>
<li>特点:<ul>
<li>通过信道增益和噪声功率直接计算 SNR。</li>
<li>也依赖 pilot。</li>
<li>高速移动环境中相对稳健，但需要频繁更新信道估计。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-区别总结-EVM-vs-LS-vs-PSD-ANN"><a href="#4-区别总结-EVM-vs-LS-vs-PSD-ANN" class="headerlink" title="4. 区别总结: EVM vs LS vs PSD+ANN"></a>4. 区别总结: EVM vs LS vs PSD+ANN</h2><table>
<thead>
<tr>
<th>特性</th>
<th>EVM-based</th>
<th>LS-based</th>
<th>PSD+ANN</th>
</tr>
</thead>
<tbody><tr>
<td>原理</td>
<td>接收符号 vs 理想符号误差</td>
<td>信道估计 + 噪声功率</td>
<td>PSD 特征 + ANN 分类</td>
</tr>
<tr>
<td>输入</td>
<td>Pilot &#x2F; Reference</td>
<td>Pilot &#x2F; Reference</td>
<td>接收信号 PSD</td>
</tr>
<tr>
<td>输出</td>
<td>SNR (间接)</td>
<td>SNR (直接)</td>
<td>SNR 分类</td>
</tr>
<tr>
<td>高速移动适应性</td>
<td>差</td>
<td>较好</td>
<td>很好，鲁棒</td>
</tr>
<tr>
<td>复杂度</td>
<td>低</td>
<td>中等</td>
<td>低，推理快</td>
</tr>
<tr>
<td>使用场景</td>
<td>AMC &#x2F; CQI映射</td>
<td>信道估计、AMC、功率控制</td>
<td>AMC &#x2F; CQI映射</td>
</tr>
</tbody></table>
<hr>
<h2 id="5-AMC应用总结"><a href="#5-AMC应用总结" class="headerlink" title="5. AMC应用总结"></a>5. AMC应用总结</h2><ul>
<li><p>CQI驱动 AMC:</p>
<ul>
<li>基站根据 UE 反馈的 CQI 动态选择 MCS。</li>
<li>高 SNR → 高阶调制 + 高码率 → 高吞吐量。</li>
<li>低 SNR → 低阶调制 + 低码率 → 保证可靠性。</li>
<li>优点: 动态适应信道，频谱效率高。</li>
<li>缺点: 需要反馈，系统复杂。</li>
</ul>
</li>
<li><p>固定 MCS:</p>
<ul>
<li>不考虑信道变化，固定使用某个 MCS。</li>
<li>简单，但在差信道下 BLER 高，信道好时未充分利用频谱。</li>
<li>吞吐量和可靠性都不如 CQI 驱动方法。</li>
</ul>
</li>
<li><p>ANN+PSD 方法:</p>
<ul>
<li>跳过 EVM 或 LS 复杂计算。</li>
<li>利用 PSD 特征直接预测 SNR 分类。</li>
<li>在高速移动信道下鲁棒性更强。</li>
<li>可快速选择最优 MCS，提高系统吞吐量。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><ol>
<li>PSD 中包含信号和噪声，可以通过积分或求和得到平均功率，从而估计 SNR。</li>
<li>EVM 基于 pilot 符号计算误差向量，间接得到 SNR。</li>
<li>LS 基于 pilot 符号做信道估计，再计算 SNR。</li>
<li>ANN+PSD 方法在高速移动场景下性能优于传统 EVM 方法，计算量低且稳健。</li>
<li>CQI 驱动 AMC 与固定 MCS 的区别在于系统能否动态适应信道变化，前者性能更优。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/26/Semantic-Communication/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/26/Semantic-Communication/" class="post-title-link" itemprop="url">Semantic Communication</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-26 19:06:04 / Modified: 19:13:43" itemprop="dateCreated datePublished" datetime="2025-08-26T19:06:04+08:00">2025-08-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input video</span><br><span class="line">1080p         -&gt;              360p        -&gt;            token   -&gt; decoder  </span><br><span class="line">8x1920x1080   downsampling     640x360    encoder(8x16x16)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/20/DeepAutoencoder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/20/DeepAutoencoder/" class="post-title-link" itemprop="url">DeepAutoencoder</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-20 23:05:37 / Modified: 23:36:09" itemprop="dateCreated datePublished" datetime="2025-08-20T23:05:37+08:00">2025-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>使用网络结构来降维，降维的网络被称为Encoder，这个Encoder就是很多个隐藏层网络的网络结构，它也有一个对应的Decoder，也是网络结构。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input Image (28 x 28 = 784) -&gt; Encoder -&gt; codeword (&lt; 784)</span><br><span class="line"></span><br><span class="line">codeword -&gt; Decoder -&gt; Output Image</span><br></pre></td></tr></table></figure>
<p>Encoder和Decoder需要learn together</p>
<h2 id="以CSI压缩为例"><a href="#以CSI压缩为例" class="headerlink" title="以CSI压缩为例"></a>以CSI压缩为例</h2><p>在无线通信中，为了进一步压缩CSI矩阵，可以使用基于深度自动编码器的策略</p>
<ul>
<li><ol>
<li>编码压缩：</li>
</ol>
<ul>
<li>输入： 稀疏化后的CSI矩阵Ha</li>
<li>编码器$E(\theta_1, ⋅ )$将$H_a$压缩成低维码字向量v：<br>  $v &#x3D; E(\theta_1, H_a)$</li>
<li>$\theta_1$是编码器网络的参数，需要通过训练学习</li>
</ul>
</li>
<li><ol start="2">
<li>反馈传输</li>
</ol>
<ul>
<li>压缩后的码字v通过uplink反馈给BS</li>
<li>这种低维表示大幅减少uplink反馈开销</li>
</ul>
</li>
<li><ol start="3">
<li>解码恢复</li>
</ol>
<ul>
<li>基站接收到v后，通过解码器$D(\theta_2, ⋅)$恢复CSI矩阵：<br>  $\hat{H}_a &#x3D; D(\theta_2,v)$</li>
</ul>
</li>
<li><ol start="4">
<li>训练目标</li>
</ol>
<ul>
<li>编码器和解码器共同训练，目标是最小化重建误差：<br>  $min_{\theta_1, \theta_2} E || H_a - \hat{H}_a ||^2$。<br>  这个也是均方差，因此也是loss function，其中E表示对训练样本取平均，</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/20/CSI%E5%8E%8B%E7%BC%A9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/20/CSI%E5%8E%8B%E7%BC%A9/" class="post-title-link" itemprop="url">CSI压缩</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-20 18:43:08 / Modified: 22:12:16" itemprop="dateCreated datePublished" datetime="2025-08-20T18:43:08+08:00">2025-08-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/5G/" itemprop="url" rel="index"><span itemprop="name">5G</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="CSI基本表示"><a href="#CSI基本表示" class="headerlink" title="CSI基本表示"></a>CSI基本表示</h2><p>CSI用来描述无线信道的频率响应。<br>在OFDM系统中，CSI常用矩阵表示：</p>
<ul>
<li>行方向：天线</li>
<li>列方向：子载波</li>
<li>例如：$H ∈ C^{Nc × Nt}$，其中<ul>
<li>Nc &#x3D; 天线数</li>
<li>Nt &#x3D; 子载波数.</li>
</ul>
</li>
</ul>
<h2 id="时域和频域的关系"><a href="#时域和频域的关系" class="headerlink" title="时域和频域的关系"></a>时域和频域的关系</h2><p>H[t] -&gt; DFT -&gt; 频域CSI。<br>DFT表达式:  </p>
<p>$H[k] &#x3D; \sum_{l&#x3D;0}^{L-1} h[l] \cdot e^{-j \frac{2\pi k l}{N}}$.<br>也可以将其写成矩阵形式：<br>H &#x3D; W h<br>其中 W 是 DFT 矩阵，$W ∈ C^{N_c × L}$, $h ∈ C^{L × N_t}$, $H ∈ C^{N_c × N_t}$. </p>
<h2 id="CSI的角度-延迟域变换"><a href="#CSI的角度-延迟域变换" class="headerlink" title="CSI的角度-延迟域变换"></a>CSI的角度-延迟域变换</h2><p>$\hat{H}_~ &#x3D; F_c * H * F_t^H$. </p>
<ul>
<li>Fc: Nc x Nc 的 DFT 矩阵，作用在行方向（天线 -&gt; 角度域）</li>
<li>Ft: Nt x Nt 的 DFT 矩阵，作用在列方向（子载波 -&gt; 延迟域）</li>
</ul>
<p>第一步是计算H与$F_t^H$进行右乘，相当于对每一行（固定天线）都会把“跨子载波的频域响应”做IDFT（因为是对Ft求共轭处理），得到“跨延迟抽头的时延响应”，然后第二步是将计算得到的与Fc进行左乘，相当于对每一列（固定延迟抽头）做DFT，把天线域的信号转换到角度域。  </p>
<h2 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h2><p>$\hat{H}_~$在角度-延迟域往往呈现出稀疏性：</p>
<ul>
<li>多径信道只在延迟bins和角度bins上有能量</li>
<li>其他位置接近0</li>
</ul>
<p>正是这种稀疏性，使得CSI可以高效压缩和反馈</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/12/%E6%B3%A2%E6%9D%9F%E8%B5%8B%E5%BD%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/12/%E6%B3%A2%E6%9D%9F%E8%B5%8B%E5%BD%A2/" class="post-title-link" itemprop="url">波束赋形</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-12 15:30:59 / Modified: 15:32:25" itemprop="dateCreated datePublished" datetime="2025-08-12T15:30:59+08:00">2025-08-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <p>了解发射端数字基带信号txSigSTS与射频波束赋形矩阵mFrf相乘的物理意义，以及波束赋形在无线通信发射链路中的位置和作用。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在多天线无线通信系统中，为了提升信号质量、增强覆盖能力或实现空间复用，常采用波束赋形技术。波束赋形可分为数字波束赋形和射频（模拟）波束赋形。</p>
<h2 id="发射端信号处理流程"><a href="#发射端信号处理流程" class="headerlink" title="发射端信号处理流程"></a>发射端信号处理流程</h2><p>一般发射端信号处理主要包括：</p>
<ul>
<li>数字基带处理：编码、调制、空间复用（多数据流处理）</li>
<li>数字波束赋形：基带域信号预编码</li>
<li>数字-模拟转换（DAC）</li>
<li>射频波束赋形：通过相移器及功率放大器对每个天线的信号加权，控制信号方向和形态</li>
<li>天线发射</li>
</ul>
<h2 id="矩阵乘法-txSig-txSigSTS-mFrf-的意义"><a href="#矩阵乘法-txSig-txSigSTS-mFrf-的意义" class="headerlink" title="矩阵乘法 txSig &#x3D; txSigSTS * mFrf 的意义"></a>矩阵乘法 txSig &#x3D; txSigSTS * mFrf 的意义</h2><ul>
<li>txSigSTS: 形状为 (样本数 × 数据流数) 的数字基带信号矩阵，代表每个独立数据流对应的基带时间序列。</li>
<li>mFrf: 射频波束赋形矩阵，形状为 (数据流数 × 天线数)，其每个元素为对应数据流在每根天线上的复数加权系数（幅度和相位）。</li>
</ul>
<p>通过矩阵乘法，txSigSTS乘以mFrf后，得到 (样本数 × 天线数) 维度的信号矩阵txSig，即为分配给每根天线的发送信号。</p>
<h2 id="射频波束赋形的作用"><a href="#射频波束赋形的作用" class="headerlink" title="射频波束赋形的作用"></a>射频波束赋形的作用</h2><p>射频波束赋形实现了：</p>
<ul>
<li>信号在空间方向上的加权叠加</li>
<li>聚焦信号能量向目标方向</li>
<li>减少干扰，提升信道容量与传输质量</li>
<li>在多流传输中实现信号分离</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>射频波束赋形是发射端信号链中从基带信号到天线发射信号转换的关键步骤，通常是发射端的最后一个信号处理阶段。其核心即是将数字基带多流信号通过加权矩阵映射到每个物理天线上，形成定向或多用户信号发射。</p>
<p>附注：  </p>
<ul>
<li>mFrf的设计通常基于信道估计和优化算法（如正交匹配追踪OMP），以实现最佳波束指向和多流分离。  </li>
<li>此步骤是多输入多输出（MIMO）和混合波束赋形系统的基础。</li>
</ul>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/09/tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/09/tensorflow/" class="post-title-link" itemprop="url">tensorflow</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-09 16:16:56" itemprop="dateCreated datePublished" datetime="2025-08-09T16:16:56+08:00">2025-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-19 18:41:29" itemprop="dateModified" datetime="2025-08-19T18:41:29+08:00">2025-08-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="Keras-TensorFlow-Quick-Start-Guide-🚀"><a href="#Keras-TensorFlow-Quick-Start-Guide-🚀" class="headerlink" title="Keras &#x2F; TensorFlow Quick Start Guide 🚀"></a>Keras &#x2F; TensorFlow Quick Start Guide 🚀</h1><p>本文记录了 <strong>Keras (TensorFlow)</strong> 的常用基本概念与操作，方便快速查阅。</p>
<hr>
<h2 id="📐-常见数据的-Tensor-维度"><a href="#📐-常见数据的-Tensor-维度" class="headerlink" title="📐 常见数据的 Tensor 维度"></a>📐 常见数据的 Tensor 维度</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>维度说明</th>
<th>Shape 示例</th>
</tr>
</thead>
<tbody><tr>
<td>Scalar（标量）</td>
<td>0-D Tensor</td>
<td><code>( )</code></td>
</tr>
<tr>
<td>Vector（向量）</td>
<td>1-D Tensor</td>
<td><code>(N,)</code></td>
</tr>
<tr>
<td>Audio（音频）</td>
<td>1-D 或 2-D（多通道）</td>
<td><code>(samples,)</code> or <code>(samples, channels)</code></td>
</tr>
<tr>
<td>Grayscale Image</td>
<td>2-D 或 3-D（加 batch）</td>
<td><code>(H, W, 1)</code> or <code>(N, H, W, 1)</code></td>
</tr>
<tr>
<td>RGB Image</td>
<td>3-D（H,W,C）或 4-D（N,H,W,C）</td>
<td><code>(H, W, 3)</code> or <code>(N, H, W, 3)</code></td>
</tr>
<tr>
<td>Video</td>
<td>4-D（T,H,W,C）或 5-D（N,T,H,W,C）</td>
<td><code>(T, H, W, C)</code> or <code>(N, T, H, W, C)</code></td>
</tr>
</tbody></table>
<blockquote>
<p>✅ <strong>常用格式缩写</strong>  </p>
<ul>
<li><code>N</code>: Batch size  </li>
<li><code>C</code>: Channels  </li>
<li><code>H</code>: Height  </li>
<li><code>W</code>: Width  </li>
<li><code>T</code>: Time steps &#x2F; frames<br>⚠️ <strong>注意</strong>：Keras 默认通道在最后 <code>(N, H, W, C)</code>。</li>
</ul>
</blockquote>
<hr>
<h2 id="🔄-常用张量操作"><a href="#🔄-常用张量操作" class="headerlink" title="🔄 常用张量操作"></a>🔄 常用张量操作</h2><table>
<thead>
<tr>
<th>功能</th>
<th>Keras &#x2F; TF 示例</th>
</tr>
</thead>
<tbody><tr>
<td>添加维度</td>
<td><code>tf.expand_dims(x, axis)</code></td>
</tr>
<tr>
<td>去除维度</td>
<td><code>tf.squeeze(x, axis)</code></td>
</tr>
<tr>
<td>交换维度</td>
<td><code>tf.transpose(x, perm)</code></td>
</tr>
<tr>
<td>改变形状</td>
<td><code>tf.reshape(x, new_shape)</code></td>
</tr>
<tr>
<td>拼接张量</td>
<td><code>tf.concat([t1, t2], axis)</code></td>
</tr>
<tr>
<td>转换数据类型</td>
<td><code>tf.cast(x, tf.float32)</code></td>
</tr>
<tr>
<td>自动求导</td>
<td><code>tf.GradientTape()</code></td>
</tr>
</tbody></table>
<hr>
<h2 id="⚙️-Keras-TensorFlow-操作示例"><a href="#⚙️-Keras-TensorFlow-操作示例" class="headerlink" title="⚙️ Keras &#x2F; TensorFlow 操作示例"></a>⚙️ Keras &#x2F; TensorFlow 操作示例</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机 RGB 图片 Tensor: (H,W,C)</span></span><br><span class="line">x = tf.random.normal((<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看形状</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)  <span class="comment"># (224, 224, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接</span></span><br><span class="line">x1 = tf.zeros([2, 1, 3])</span><br><span class="line">x2 = tf.zeros([2, 3, 3])</span><br><span class="line">x3 = tf.zeros([2, 2, 3])</span><br><span class="line">x_cat = tf.concat([x1, x2, x3], axis=1)  <span class="comment"># (2, 6, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换维度</span></span><br><span class="line">x_transposed = tf.transpose(x, perm=[1, 0, 2])  <span class="comment"># (W, H, C)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展平</span></span><br><span class="line">x_flatten = tf.reshape(x, [-1])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 / 去除维度</span></span><br><span class="line">x_batched = tf.expand_dims(x, axis=0)   <span class="comment"># (1, H, W, C)</span></span><br><span class="line">x_squeezed = tf.squeeze(x_batched, axis=0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据类型转换</span></span><br><span class="line">x_double = tf.cast(x, tf.float64)</span><br><span class="line">x_int = tf.cast(x, tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动求导</span></span><br><span class="line">w = tf.Variable(tf.random.normal([5]))</span><br><span class="line">with tf.GradientTape() as tape:</span><br><span class="line">    loss = tf.reduce_sum(w ** 2)</span><br><span class="line">grads = tape.gradient(loss, w)</span><br><span class="line"><span class="built_in">print</span>(grads)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Keras-模型训练核心流程"><a href="#Keras-模型训练核心流程" class="headerlink" title="Keras 模型训练核心流程"></a>Keras 模型训练核心流程</h2><p>import tensorflow as tf<br>from tensorflow import keras<br>from tensorflow.keras import layers</p>
<h1 id="1-定义模型"><a href="#1-定义模型" class="headerlink" title="1. 定义模型"></a>1. 定义模型</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">    layers.Dense(64, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(100,)),</span><br><span class="line">    layers.Dense(10, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h1 id="2-编译模型"><a href="#2-编译模型" class="headerlink" title="2. 编译模型"></a>2. 编译模型</h1><p>在编译模型时指定损失函数和优化器。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.compile(</span><br><span class="line">    optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">    loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">    metrics=[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h1 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h1><ul>
<li>fit()默认在 训练集 (x&#x3D;train_generator[d]) 上计算 loss 并进行梯度下降更新权重。</li>
<li>同时，如果提供了 validation_data，在每个 epoch 结束时，会额外在验证集<br>(valid_generator[d]) 上计算一次 loss (val_loss) 和指标 (如 val_accuracy)。</li>
<li>训练过程中显示的 loss 就是 训练集的 loss，val_loss 才是验证集的 loss。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span> = model.fit(</span><br><span class="line">    train_dataset,                    <span class="comment"># 训练数据 (X_train, y_train) 或 tf.data.Dataset</span></span><br><span class="line">    validation_data=val_dataset,      <span class="comment"># 验证集 (X_val, y_val) 或 tf.data.Dataset</span></span><br><span class="line">    epochs=20,</span><br><span class="line">    callbacks=[</span><br><span class="line">        keras.callbacks.EarlyStopping(monitor=<span class="string">&#x27;val_loss&#x27;</span>, patience=5),</span><br><span class="line">        keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">&#x27;val_loss&#x27;</span>, <span class="built_in">factor</span>=0.5, patience=3)</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="4-评估与预测"><a href="#4-评估与预测" class="headerlink" title="4. 评估与预测"></a>4. 评估与预测</h1><p>model.evaluate(test_dataset)<br>preds &#x3D; model.predict(new_data)</p>
<h2 id="Keras-LSTM模型结构示例（截取核心代码）"><a href="#Keras-LSTM模型结构示例（截取核心代码）" class="headerlink" title="Keras LSTM模型结构示例（截取核心代码）"></a>Keras LSTM模型结构示例（截取核心代码）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras.models import Model</span><br><span class="line">from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, BatchNormalization, Dropout</span><br><span class="line">from tensorflow.keras.optimizers import Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入定义</span></span><br><span class="line">seq_in = Input(shape=(10240, 1), name=<span class="string">&#x27;seq_in&#x27;</span>)   <span class="comment"># 时间序列输入，10240时间步，每步1维特征</span></span><br><span class="line">seq_p = Input(shape=(32,), name=<span class="string">&#x27;seq_p&#x27;</span>)          <span class="comment"># 辅助特征输入，固定32维</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># LSTM层，输出维度32</span></span><br><span class="line">lstm_out = LSTM(32, return_sequences=False, name=<span class="string">&#x27;lstm_1&#x27;</span>)(seq_in)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接LSTM输出与辅助输入</span></span><br><span class="line">next_layer_in = Concatenate(axis=1)([lstm_out, seq_p])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层堆叠（假设nn为列表，如[64, 32]）</span></span><br><span class="line"><span class="keyword">for</span> i, n <span class="keyword">in</span> enumerate(nn):</span><br><span class="line">    dense = Dense(n, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>, name=f<span class="string">&#x27;fc_dense&#123;i&#125;&#x27;</span>)(next_layer_in)</span><br><span class="line">    <span class="keyword">if</span> useBN:</span><br><span class="line">        dense = BatchNormalization()(dense)</span><br><span class="line">    next_layer_in = dense</span><br><span class="line">    <span class="keyword">if</span> i &lt; len(nn) - 1:</span><br><span class="line">        next_layer_in = Dropout(dropout_rate)(next_layer_in)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">encoder = Dense(simParams[<span class="string">&#x27;nSubCarr&#x27;</span>], activation=<span class="string">&#x27;linear&#x27;</span>, kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>, name=<span class="string">&#x27;fc_regressor&#x27;</span>)(next_layer_in)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义</span></span><br><span class="line">CSI_predictor = Model([seq_in, seq_p], encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">opt = Adam(lr=lr)</span><br><span class="line">CSI_predictor.compile(optimizer=opt, loss=<span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line">CSI_predictor.summary()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输入格式说明。</p>
<table>
<thead>
<tr>
<th>输入名</th>
<th>形状示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>seq_in</code></td>
<td><code>(batch_size, 10240, 1)</code></td>
<td>时间序列输入，长度10240的序列，每时间步1维特征</td>
</tr>
<tr>
<td><code>seq_p</code></td>
<td><code>(batch_size, 32)</code></td>
<td>辅助特征，无时间步，长度32的向量</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/5/">5</a><a class="extend next" rel="next" href="/blog/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
