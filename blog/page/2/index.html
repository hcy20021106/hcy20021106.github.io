<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hechenyi.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://hechenyi.github.io/blog/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hechenyi.github.io/blog/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/05/Python%E8%AF%BB%E5%8F%96mat%E6%96%87%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/05/Python%E8%AF%BB%E5%8F%96mat%E6%96%87%E4%BB%B6/" class="post-title-link" itemprop="url">Python读取mat文件</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-08-05 00:09:44" itemprop="dateCreated datePublished" datetime="2025-08-05T00:09:44+08:00">2025-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-09 10:27:48" itemprop="dateModified" datetime="2025-08-09T10:27:48+08:00">2025-08-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="读取-MATLAB-mat-文件（带有结构体字段）"><a href="#读取-MATLAB-mat-文件（带有结构体字段）" class="headerlink" title="读取 MATLAB .mat 文件（带有结构体字段）"></a>读取 MATLAB <code>.mat</code> 文件（带有结构体字段）</h2><p>我们经常遇到 MATLAB 中保存的结构体 <code>.mat</code> 文件，例如包含 <code>real</code> 和 <code>imag</code> 字段的 CSI 或 preamble 数据。在 Python 中可以用 <code>h5py</code> + <code>numpy</code> 来处理：</p>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install h5py numpy torch</span><br></pre></td></tr></table></figure>

<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">mat_path = <span class="string">&#x27;your_file_path.mat&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> h5py.File(mat_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># 访问结构体数组引用（例如 usr_data）</span></span><br><span class="line">    hDp_ref = f[<span class="string">&#x27;usr_data&#x27;</span>]</span><br><span class="line">    hDp_real_refs = hDp_ref[:]  <span class="comment"># 是一组 object reference</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取 preamble</span></span><br><span class="line">    preamble_ref = hDp_real_refs[<span class="number">0</span>].item()         <span class="comment"># 解引用</span></span><br><span class="line">    preamble_dataset = f[preamble_ref]             <span class="comment"># 获取对应的数据集</span></span><br><span class="line">    preamble_struct = np.array(preamble_dataset)   <span class="comment"># 转成 numpy 数组</span></span><br><span class="line">    preamble = preamble_struct[<span class="string">&#x27;real&#x27;</span>] + <span class="number">1j</span> * preamble_struct[<span class="string">&#x27;imag&#x27;</span>]  <span class="comment"># 合成为复数</span></span><br><span class="line">    preamble_tensor = torch.tensor(preamble, dtype=torch.complex64)    <span class="comment"># 转为 PyTorch 复数张量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取 CSI（假设是第二个）</span></span><br><span class="line">    csi_ref = hDp_real_refs[<span class="number">1</span>].item()</span><br><span class="line">    csi_dataset = f[csi_ref]</span><br><span class="line">    csi_struct = np.array(csi_dataset)</span><br><span class="line">    csi = csi_struct[<span class="string">&#x27;real&#x27;</span>] + <span class="number">1j</span> * csi_struct[<span class="string">&#x27;imag&#x27;</span>]</span><br><span class="line">    csi_tensor = torch.tensor(csi, dtype=torch.complex64)</span><br></pre></td></tr></table></figure>

<h3 id="常见说明"><a href="#常见说明" class="headerlink" title="常见说明"></a>常见说明</h3><ul>
<li><code>h5py.File(..., &#39;r&#39;)</code>：读取 <code>.mat</code> 文件（v7.3 格式，基于 HDF5）。</li>
<li><code>hDp_ref[:]</code>：获取结构体中嵌套的 object reference。</li>
<li><code>.item()</code>：从 NumPy 的 <code>object</code> 中提取真实的引用。</li>
<li><code>[&#39;real&#39;] + 1j * [&#39;imag&#39;]</code>：组合为复数。</li>
<li>使用 <code>torch.tensor(..., dtype=torch.complex64)</code> 转为复数张量，方便用于神经网络输入。</li>
</ul>
<p>在 preamble_struct &#x3D; np.array(preamble_dataset) numpy数组中，它的每一个元素仍然是一个类字典对象，拥有字段，比如 ‘real’ 和 ‘imag’，这两个字段就是你要提取的实部和虚部数据。  </p>
<p>在 PyTorch 中，复数张量（dtype&#x3D;torch.complex64 或 complex128）不是单独的 real&#x2F;imag 张量，而是一个 内建的复数类型，其中每个元素都包含实部和虚部。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([1 + 2j, 3 + 4j], dtype=torch.complex64)，</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结构上：</p>
<p>x[0] 是 (1+2j)，即：</p>
<p>x[0].real &#x3D;&#x3D; 1.0</p>
<p>x[0].imag &#x3D;&#x3D; 2.0</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/02/CI-CD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/02/CI-CD/" class="post-title-link" itemprop="url">CI/CD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-02 13:32:29 / Modified: 14:35:07" itemprop="dateCreated datePublished" datetime="2025-08-02T13:32:29+08:00">2025-08-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%B6%E4%BB%96/" itemprop="url" rel="index"><span itemprop="name">其他</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="Jenkins"><a href="#Jenkins" class="headerlink" title="Jenkins"></a>Jenkins</h1><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><p>GitLab -&gt; Jenkins -&gt; Docker 构建 -&gt; 推送到JFrog Artifactory -&gt; K8s 滚动部署</p>
<h2 id="详细流程"><a href="#详细流程" class="headerlink" title="详细流程"></a>详细流程</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[开发者] git push</span><br><span class="line">        ↓</span><br><span class="line">[GitLab Webhook] 通知 Jenkins</span><br><span class="line">        ↓</span><br><span class="line">[Jenkins Pipeline]</span><br><span class="line">   1. 拉取 GitLab 源码</span><br><span class="line">   2. 构建 Docker 镜像（基于 Dockerfile）</span><br><span class="line">   3. 登录 JFrog Artifactory 仓库</span><br><span class="line">   4. 推送镜像到 Artifactory Docker 仓库</span><br><span class="line">   5. 使用 kubeconfig/kubectl 更新 Kubernetes Deployment 中的镜像</span><br><span class="line">        ↓</span><br><span class="line">[Kubernetes]</span><br><span class="line">   6. 自动滚动更新容器，部署新版应用</span><br></pre></td></tr></table></figure>
<h2 id="使用组件及角色"><a href="#使用组件及角色" class="headerlink" title="使用组件及角色"></a>使用组件及角色</h2><table>
<thead>
<tr>
<th>组件</th>
<th>角色说明</th>
</tr>
</thead>
<tbody><tr>
<td>GitLab</td>
<td>源码托管、触发构建 Webhook</td>
</tr>
<tr>
<td>Jenkins</td>
<td>核心 CI&#x2F;CD 执行器，运行流水线</td>
</tr>
<tr>
<td>Docker</td>
<td>构建容器镜像</td>
</tr>
<tr>
<td>JFrog Artifactory</td>
<td>存储容器镜像（Docker Registry）</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>运行你的后端服务，部署并自动滚动更新</td>
</tr>
</tbody></table>
<h2 id="示例Jenkinsfile"><a href="#示例Jenkinsfile" class="headerlink" title="示例Jenkinsfile"></a>示例Jenkinsfile</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line"></span><br><span class="line">    environment &#123;</span><br><span class="line">        ARTIFACTORY_DOCKER_CREDENTIALS = <span class="string">&#x27;artifactory-docker-creds&#x27;</span> // Jenkins中配置</span><br><span class="line">        ARTIFACTORY_DOCKER_REGISTRY = <span class="string">&#x27;jfrog.mycompany.com&#x27;</span></span><br><span class="line">        ARTIFACTORY_REPO = <span class="string">&#x27;docker-local&#x27;</span>  // 你在 Artifactory 中的 Docker 仓库名</span><br><span class="line">        IMAGE_NAME = <span class="string">&#x27;myapp&#x27;</span></span><br><span class="line">        IMAGE_TAG = <span class="string">&quot;<span class="variable">$&#123;BUILD_NUMBER&#125;</span>&quot;</span>      // 每次构建使用不同 tag</span><br><span class="line"></span><br><span class="line">        DEPLOYMENT_NAME = <span class="string">&quot;myapp-deployment&quot;</span></span><br><span class="line">        CONTAINER_NAME = <span class="string">&quot;myapp-container&quot;</span></span><br><span class="line">        NAMESPACE = <span class="string">&quot;test&quot;</span></span><br><span class="line">        KUBECONFIG_CRED = <span class="string">&#x27;kubeconfig-file&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(<span class="string">&#x27;Clone Code&#x27;</span>) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                checkout scm</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        stage(<span class="string">&#x27;Build Docker Image&#x27;</span>) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                script &#123;</span><br><span class="line">                    docker.withRegistry(<span class="string">&quot;https://<span class="variable">$&#123;ARTIFACTORY_DOCKER_REGISTRY&#125;</span>&quot;</span>, ARTIFACTORY_DOCKER_CREDENTIALS) &#123;</span><br><span class="line">                        def image = docker.build(<span class="string">&quot;<span class="variable">$&#123;ARTIFACTORY_DOCKER_REGISTRY&#125;</span>/<span class="variable">$&#123;ARTIFACTORY_REPO&#125;</span>/<span class="variable">$&#123;IMAGE_NAME&#125;</span>:<span class="variable">$&#123;IMAGE_TAG&#125;</span>&quot;</span>)</span><br><span class="line">                        image.push()</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        stage(<span class="string">&#x27;Update K8s Deployment&#x27;</span>) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                withCredentials([file(credentialsId: KUBECONFIG_CRED, variable: <span class="string">&#x27;KUBECONFIG&#x27;</span>)]) &#123;</span><br><span class="line">                    sh <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">                    kubectl --kubeconfig=$KUBECONFIG -n $&#123;NAMESPACE&#125; set image deployment/$&#123;DEPLOYMENT_NAME&#125; $&#123;CONTAINER_NAME&#125;=$&#123;ARTIFACTORY_DOCKER_REGISTRY&#125;/$&#123;ARTIFACTORY_REPO&#125;/$&#123;IMAGE_NAME&#125;:$&#123;IMAGE_TAG&#125;</span></span><br><span class="line"><span class="string">                    kubectl --kubeconfig=$KUBECONFIG -n $&#123;NAMESPACE&#125; rollout status deployment/$&#123;DEPLOYMENT_NAME&#125;</span></span><br><span class="line"><span class="string">                    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="准备工作（一次性配置）"><a href="#准备工作（一次性配置）" class="headerlink" title="准备工作（一次性配置）"></a>准备工作（一次性配置）</h2><ul>
<li>GitLab 配置好Webhook：指向Jenkins Job URL</li>
<li>Artifactory 建好Docker仓库（如docker-local）</li>
<li>Jenkins凭据配置<ul>
<li>artifactory-docker-creds （用户名&#x2F;密码或API Token）</li>
<li>kubeconfig-file（K8s访问配置文件）</li>
</ul>
</li>
<li>Jenkins安装插件<ul>
<li>Docker plugin</li>
<li>Git plugin</li>
<li>Pipeline plugin</li>
<li>Credentials plugin</li>
</ul>
</li>
</ul>
<h2 id="部署效果"><a href="#部署效果" class="headerlink" title="部署效果"></a>部署效果</h2><p>只需一条git push:</p>
<ul>
<li>Jenkins 自动构建镜像</li>
<li>推送Artifactory</li>
<li>更新K8s应用，自动滚动升级</li>
</ul>
<h1 id="Ansible"><a href="#Ansible" class="headerlink" title="Ansible"></a>Ansible</h1><p>Ansible 角色 （Role）是对自动化任务的模块化封装，方便复用和管理。一个完整的role通常包含以下目录和文件，每个目录复制不同的职责：</p>
<h2 id="defaults"><a href="#defaults" class="headerlink" title="defaults"></a>defaults</h2><ul>
<li>作用：存放角色的默认变量</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">core_executable: <span class="string">&quot;&#123;&#123; free5gc_src.dest_dir &#125;&#125;/run.sh&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">core_apn: <span class="string">&quot;internet&quot;</span></span><br><span class="line">core_sst: <span class="string">&quot;1&quot;</span></span><br><span class="line">core_sd: <span class="string">&quot;000000&quot;</span></span><br><span class="line"></span><br><span class="line">core_mcc: <span class="string">&quot;999&quot;</span></span><br><span class="line">core_mnc: <span class="string">&quot;70&quot;</span></span><br><span class="line"></span><br><span class="line">core_plmn: <span class="string">&quot;&#123;&#123;core_mcc&#125;&#125;&#123;&#123;core_mcn&#125;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">core_tac: <span class="string">&quot;000001&quot;</span></span><br><span class="line"></span><br><span class="line">core_simcard_inventory_file: deploy-config-simcards.yml</span><br><span class="line">amf_ipv4_net: <span class="string">&quot;10.0.2.10&quot;</span></span><br><span class="line">upf_ipv4_net: <span class="string">&quot;10.0.2.12&quot;</span></span><br><span class="line"></span><br><span class="line">sim_cards:</span><br><span class="line">- name: <span class="string">&quot;Sim26&quot;</span></span><br><span class="line">  imsi: <span class="string">&quot;imsi-999700000128110&quot;</span></span><br><span class="line">  key: <span class="string">&quot;E501DCD94E0912B04AE041619DA43421&quot;</span></span><br><span class="line">  opc: <span class="string">&quot;13082466301232D8B8BB299F1867A861&quot;</span></span><br><span class="line">  apn: <span class="string">&quot;internet&quot;</span></span><br><span class="line">  sst: <span class="string">&quot;1&quot;</span></span><br><span class="line">  sd: <span class="string">&quot;000000&quot;</span></span><br><span class="line">  gpsis: <span class="string">&quot;msisdn-0900000005&quot;</span></span><br><span class="line"></span><br><span class="line">- name: <span class="string">&quot;Sim28&quot;</span></span><br><span class="line">  imsi: <span class="string">&quot;imsi-001010000105331&quot;</span></span><br><span class="line">  key: <span class="string">&quot;22231214410527097343456132856119&quot;</span></span><br><span class="line">  opc: <span class="string">&quot;35400111987345158373161168419158&quot;</span></span><br><span class="line">  apn: <span class="string">&quot;internet&quot;</span></span><br><span class="line">  sst: <span class="string">&quot;1&quot;</span></span><br><span class="line">  sd: <span class="string">&quot;000000&quot;</span></span><br><span class="line">  gpsis: <span class="string">&quot;msisdn-0900000006&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="vars"><a href="#vars" class="headerlink" title="vars"></a>vars</h2><ul>
<li>作用： 存放角色的变量（优先级高于defaults）</li>
<li>用于定义角色强制需要的变量，通常不建议轻易覆盖</li>
</ul>
<h2 id="tasks"><a href="#tasks" class="headerlink" title="tasks"></a>tasks</h2><ul>
<li>作用： 角色的核心任务清单，定义执行的操作步骤</li>
<li>文件一般为main.yml</li>
<li>例如安装软件、复制文件、配置服务等</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- name: Install Corepack</span><br><span class="line">  become: <span class="built_in">yes</span></span><br><span class="line">  npm:</span><br><span class="line">    name: corepack</span><br><span class="line">    global: <span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line">- name: Enable corepark</span><br><span class="line">  <span class="built_in">command</span>: corepack <span class="built_in">enable</span></span><br><span class="line">  <span class="comment"># when: yarn_version.rc != 0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- name: <span class="string">&quot;INSTALL-TASKS-gtp5g-SOURCE - Run gtp5g build&quot;</span></span><br><span class="line">  tags: gtp5g build</span><br><span class="line">  <span class="comment"># become_user: &quot;&#123;&#123; normal_user_id &#125;&#125;&quot;</span></span><br><span class="line">  become: <span class="built_in">yes</span></span><br><span class="line">  <span class="built_in">command</span>: <span class="built_in">sudo</span> make</span><br><span class="line">  args:</span><br><span class="line">    <span class="built_in">chdir</span>: <span class="string">&quot;&#123;&#123; gtp5g_src.dest_dir &#125;&#125;&quot;</span></span><br><span class="line">  when: gtp5g_module.rc != 0</span><br><span class="line"></span><br><span class="line">- name: <span class="string">&quot;INSTALL-TASKS-gtp5g-SOURCE - Run gtp5g install&quot;</span></span><br><span class="line">  tags: gtp5g install</span><br><span class="line">  become: <span class="built_in">yes</span></span><br><span class="line">  <span class="built_in">command</span>:   make install</span><br><span class="line">  args:</span><br><span class="line">    <span class="built_in">chdir</span>: <span class="string">&quot;&#123;&#123; gtp5g_src.dest_dir &#125;&#125;&quot;</span></span><br><span class="line">  when: gtp5g_module.rc != 0</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="templates"><a href="#templates" class="headerlink" title="templates"></a>templates</h2><ul>
<li>作用： 存放jinja2模板文件，用于生成动态配置文件</li>
<li>在模版文件中可以使用变量、 条件语句等</li>
<li>在tasks中通过template模板调用</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">info:</span><br><span class="line">  version: 1.0.9</span><br><span class="line">  description: AMF initial <span class="built_in">local</span> configuration</span><br><span class="line"></span><br><span class="line">configuration:</span><br><span class="line">  amfName: AMF <span class="comment"># the name of this AMF</span></span><br><span class="line">  ngapIpList:  <span class="comment"># the IP list of N2 interfaces on this AMF</span></span><br><span class="line">    - &#123;&#123; amf_ipv4_net &#125;&#125;</span><br><span class="line">  ngapPort: 38412 <span class="comment"># the SCTP port listened by NGAP</span></span><br><span class="line">  sbi: <span class="comment"># Service-based interface information</span></span><br><span class="line">    scheme: http <span class="comment"># the protocol for sbi (http or https)</span></span><br><span class="line">    registerIPv4: 127.0.0.18 <span class="comment"># IP used to register to NRF</span></span><br><span class="line">    bindingIPv4: 127.0.0.18  <span class="comment"># IP used to bind the service</span></span><br><span class="line">    port: 8000 <span class="comment"># port used to bind the service</span></span><br><span class="line">    tls: <span class="comment"># the local path of TLS key</span></span><br><span class="line">      pem: cert/amf.pem <span class="comment"># AMF TLS Certificate</span></span><br><span class="line">      key: cert/amf.key <span class="comment"># AMF TLS Private key</span></span><br><span class="line">  serviceNameList: <span class="comment"># the SBI services provided by this AMF, refer to TS 29.518</span></span><br><span class="line">    - namf-comm <span class="comment"># Namf_Communication service</span></span><br><span class="line">    - namf-evts <span class="comment"># Namf_EventExposure service</span></span><br><span class="line">    - namf-mt   <span class="comment"># Namf_MT service</span></span><br><span class="line">    - namf-loc  <span class="comment"># Namf_Location service</span></span><br><span class="line">    - namf-oam  <span class="comment"># OAM service</span></span><br><span class="line">  servedGuamiList: <span class="comment"># Guami (Globally Unique AMF ID) list supported by this AMF</span></span><br><span class="line">    <span class="comment"># &lt;GUAMI&gt; = &lt;MCC&gt;&lt;MNC&gt;&lt;AMF ID&gt;</span></span><br><span class="line">    - plmnId: <span class="comment"># Public Land Mobile Network ID, &lt;PLMN ID&gt; = &lt;MCC&gt;&lt;MNC&gt;</span></span><br><span class="line">        mcc: &#123;&#123; core_mcc &#125;&#125; <span class="comment"># Mobile Country Code (3 digits string, digit: 0~9)</span></span><br><span class="line">        mnc: &#123;&#123; core_mnc &#125;&#125; <span class="comment"># Mobile Network Code (2 or 3 digits string, digit: 0~9)</span></span><br><span class="line">      amfId: cafe00 <span class="comment"># AMF identifier (3 bytes hex string, range: 000000~FFFFFF)</span></span><br><span class="line">  supportTaiList:  <span class="comment"># the TAI (Tracking Area Identifier) list supported by this AMF</span></span><br><span class="line">    - plmnId: <span class="comment"># Public Land Mobile Network ID, &lt;PLMN ID&gt; = &lt;MCC&gt;&lt;MNC&gt;</span></span><br><span class="line">        mcc: &#123;&#123; core_mcc &#125;&#125; <span class="comment"># Mobile Country Code (3 digits string, digit: 0~9)</span></span><br><span class="line">        mnc: &#123;&#123; core_mnc &#125;&#125; <span class="comment"># Mobile Network Code (2 or 3 digits string, digit: 0~9)</span></span><br><span class="line">      <span class="built_in">tac</span>: &#123;&#123; core_tac &#125;&#125; <span class="comment"># Tracking Area Code (3 bytes hex string, range: 000000~FFFFFF)</span></span><br><span class="line">  plmnSupportList: <span class="comment"># the PLMNs (Public land mobile network) list supported by this AMF</span></span><br><span class="line">    - plmnId: <span class="comment"># Public Land Mobile Network ID, &lt;PLMN ID&gt; = &lt;MCC&gt;&lt;MNC&gt;</span></span><br><span class="line">        mcc: &#123;&#123; core_mcc &#125;&#125; <span class="comment"># Mobile Country Code (3 digits string, digit: 0~9)</span></span><br><span class="line">        mnc: &#123;&#123; core_mnc &#125;&#125; <span class="comment"># Mobile Network Code (2 or 3 digits string, digit: 0~9)</span></span><br><span class="line">      snssaiList: <span class="comment"># the S-NSSAI (Single Network Slice Selection Assistance Information) list supported by this AMF</span></span><br><span class="line">        - sst: &#123;&#123; core_sst &#125;&#125; <span class="comment"># Slice/Service Type (uinteger, range: 0~255)</span></span><br><span class="line">          sd: &#123;&#123; core_sd &#125;&#125; <span class="comment"># Slice Differentiator (3 bytes hex string, range: 000000~FFFFFF)</span></span><br><span class="line">        - sst: 1 <span class="comment"># Slice/Service Type (uinteger, range: 0~255)</span></span><br><span class="line">          sd: 112233 <span class="comment"># Slice Differentiator (3 bytes hex string, range: 000000~FFFFFF)</span></span><br><span class="line">  supportDnnList:  <span class="comment"># the DNN (Data Network Name) list supported by this AMF</span></span><br><span class="line">    - internet</span><br><span class="line">  nrfUri: http://127.0.0.10:8000 <span class="comment"># a valid URI of NRF</span></span><br><span class="line">  nrfCertPem: cert/nrf.pem <span class="comment"># NRF Certificate</span></span><br><span class="line">  security:  <span class="comment"># NAS security parameters</span></span><br><span class="line">    integrityOrder: <span class="comment"># the priority of integrity algorithms</span></span><br><span class="line">      - NIA2</span><br><span class="line">      <span class="comment"># - NIA0</span></span><br><span class="line">    cipheringOrder: <span class="comment"># the priority of ciphering algorithms</span></span><br><span class="line">      - NEA0</span><br><span class="line">      - NEA2</span><br><span class="line">  networkName:  <span class="comment"># the name of this core network</span></span><br><span class="line">    full: free5GC</span><br><span class="line">    short: free</span><br><span class="line">  ngapIE: <span class="comment"># Optional NGAP IEs</span></span><br><span class="line">    mobilityRestrictionList: <span class="comment"># Mobility Restriction List IE, refer to TS 38.413</span></span><br><span class="line">      <span class="built_in">enable</span>: <span class="literal">true</span> <span class="comment"># append this IE in related message or not</span></span><br><span class="line">    maskedIMEISV: <span class="comment"># Masked IMEISV IE, refer to TS 38.413</span></span><br><span class="line">      <span class="built_in">enable</span>: <span class="literal">true</span> <span class="comment"># append this IE in related message or not</span></span><br><span class="line">    redirectionVoiceFallback: <span class="comment"># Redirection Voice Fallback IE, refer to TS 38.413</span></span><br><span class="line">      <span class="built_in">enable</span>: <span class="literal">false</span> <span class="comment"># append this IE in related message or not</span></span><br><span class="line">  nasIE: <span class="comment"># Optional NAS IEs</span></span><br><span class="line">    networkFeatureSupport5GS: <span class="comment"># 5gs Network Feature Support IE, refer to TS 24.501</span></span><br><span class="line">      <span class="built_in">enable</span>: <span class="literal">true</span> <span class="comment"># append this IE in Registration accept or not</span></span><br><span class="line">      length: 1 <span class="comment"># IE content length (uinteger, range: 1~3)</span></span><br><span class="line">      imsVoPS: 0 <span class="comment"># IMS voice over PS session indicator (uinteger, range: 0~1)</span></span><br><span class="line">      emc: 0 <span class="comment"># Emergency service support indicator for 3GPP access (uinteger, range: 0~3)</span></span><br><span class="line">      emf: 0 <span class="comment"># Emergency service fallback indicator for 3GPP access (uinteger, range: 0~3)</span></span><br><span class="line">      iwkN26: 0 <span class="comment"># Interworking without N26 interface indicator (uinteger, range: 0~1)</span></span><br><span class="line">      mpsi: 0 <span class="comment"># MPS indicator (uinteger, range: 0~1)</span></span><br><span class="line">      emcN3: 0 <span class="comment"># Emergency service support indicator for Non-3GPP access (uinteger, range: 0~1)</span></span><br><span class="line">      mcsi: 0 <span class="comment"># MCS indicator (uinteger, range: 0~1)</span></span><br><span class="line">  t3502Value: 720  <span class="comment"># timer value (seconds) at UE side</span></span><br><span class="line">  t3512Value: 3600 <span class="comment"># timer value (seconds) at UE side</span></span><br><span class="line">  non3gppDeregTimerValue: 3240 <span class="comment"># timer value (seconds) at UE side</span></span><br><span class="line">  <span class="comment"># retransmission timer for paging message</span></span><br><span class="line">  t3513:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Deregistration Request message</span></span><br><span class="line">  t3522:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Registration Accept message</span></span><br><span class="line">  t3550:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Configuration Update Command message</span></span><br><span class="line">  t3555:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Authentication Request/Security Mode Command message</span></span><br><span class="line">  t3560:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Notification message</span></span><br><span class="line">  t3565:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  <span class="comment"># retransmission timer for NAS Identity Request message</span></span><br><span class="line">  t3570:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span>     <span class="comment"># true or false</span></span><br><span class="line">    expireTime: 6s   <span class="comment"># default is 6 seconds</span></span><br><span class="line">    maxRetryTimes: 4 <span class="comment"># the max number of retransmission</span></span><br><span class="line">  locality: area1 <span class="comment"># Name of the location where a set of AMF, SMF, PCF and UPFs are located</span></span><br><span class="line">  sctp: <span class="comment"># set the sctp server setting &lt;optinal&gt;, once this field is set, please also add maxInputStream, maxOsStream, maxAttempts, maxInitTimeOut</span></span><br><span class="line">    numOstreams: 3 <span class="comment"># the maximum out streams of each sctp connection</span></span><br><span class="line">    maxInstreams: 5 <span class="comment"># the maximum in streams of each sctp connection</span></span><br><span class="line">    maxAttempts: 2 <span class="comment"># the maximum attempts of each sctp connection</span></span><br><span class="line">    maxInitTimeout: 2 <span class="comment"># the maximum init timeout of each sctp connection</span></span><br><span class="line">  defaultUECtxReq: <span class="literal">false</span> <span class="comment"># the default value of UE Context Request to decide when triggering Initial Context Setup procedure</span></span><br><span class="line"></span><br><span class="line">logger: <span class="comment"># log output setting</span></span><br><span class="line">  <span class="built_in">enable</span>: <span class="literal">true</span> <span class="comment"># true or false</span></span><br><span class="line">  level: info <span class="comment"># how detailed to output, value: trace, debug, info, warn, error, fatal, panic</span></span><br><span class="line">  reportCaller: <span class="literal">false</span> <span class="comment"># enable the caller report or not, value: true or false</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/08/01/MIMOBeamforming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/08/01/MIMOBeamforming/" class="post-title-link" itemprop="url">MIMOBeamforming</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-08-01 00:58:24 / Modified: 01:21:46" itemprop="dateCreated datePublished" datetime="2025-08-01T00:58:24+08:00">2025-08-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="MIMO-OFDM-波束成形系统理论介绍"><a href="#MIMO-OFDM-波束成形系统理论介绍" class="headerlink" title="MIMO-OFDM 波束成形系统理论介绍"></a>MIMO-OFDM 波束成形系统理论介绍</h1><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>现代无线通信系统（如 5G NR、LTE、WLAN）普遍采用 <strong>MIMO-OFDM</strong> 技术，因其对频率选择性衰落信道的鲁棒性及高数据速率的支持。随着应用需求的增加，系统配置日益复杂，天线阵列规模和子载波数量大幅提升，多天线、多流传输成为主流。</p>
<h2 id="2-天线阵列与波束成形"><a href="#2-天线阵列与波束成形" class="headerlink" title="2. 天线阵列与波束成形"></a>2. 天线阵列与波束成形</h2><p>通过多个天线元素组成的阵列，系统可实现<strong>空间复用</strong>和<strong>波束成形</strong>。波束成形（Beamforming）是利用空间信号处理技术调整发射信号的相位和幅度，使信号在目标方向上增强，同时抑制干扰，提高信噪比（SNR）和系统性能（如降低误码率 BER）。<br>在多天线系统中，除了数字基带层面的预编码外，通常还会应用 射频波束成形矩阵 (Frf)，这是在模拟射频前端对信号进行加权处理，实现对天线阵列的物理波束成形。</p>
<p>射频波束成形通过对数字基带信号乘以 Frf 矩阵，将信号映射到各个天线单元。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">% RF beamforming: Apply Frf to the digital signal            </span><br><span class="line">% Each antenna element is connected to each data stream</span><br><span class="line">txSig = txSigSTS*mFrf;</span><br></pre></td></tr></table></figure>

<h2 id="3-波束成形的关键流程"><a href="#3-波束成形的关键流程" class="headerlink" title="3. 波束成形的关键流程"></a>3. 波束成形的关键流程</h2><h3 id="接收端-Channel-Sounding"><a href="#接收端-Channel-Sounding" class="headerlink" title="接收端(Channel Sounding)"></a>接收端(Channel Sounding)</h3><p>Preamble Tx -&gt; MIMO Channel -&gt; OFDM Rx -&gt; MIMO Channel Estimate -&gt; SVD -&gt; nfrf</p>
<h3 id="发射端-Precoding-from-channel-sounding"><a href="#发射端-Precoding-from-channel-sounding" class="headerlink" title="发射端 (Precoding from channel sounding)"></a>发射端 (Precoding from channel sounding)</h3><p>系统建模时，将天线阵列位置、天线方向图和空间信道特性纳入设计，采用如下流程：</p>
<p>数据编码 → 预编码（基于信道测量）→ OFDM调制 → 发射波束成形 → MIMO信道 → 接收波束成形 → OFDM解调 → 数据恢复</p>
<h3 id="具体步骤说明："><a href="#具体步骤说明：" class="headerlink" title="具体步骤说明："></a>具体步骤说明：</h3><ol>
<li><p><strong>编码与调制</strong><br>发送数据先经过信源编码和信道编码提升可靠性，再映射成数字调制符号。</p>
</li>
<li><p><strong>信道测量与预编码</strong><br>通过信道探测（Channel Sounding）获取信道状态信息（CSI），基于 CSI 设计预编码矩阵，实现多流的空间分离和干扰抑制。</p>
</li>
<li><p><strong>OFDM 调制</strong><br>将频域符号映射到多个正交子载波，经过 IFFT 生成时域 OFDM 符号，并添加循环前缀以抵抗符号间干扰。</p>
</li>
<li><p><strong>发射波束成形</strong><br>通过射频预编码矩阵对数字基带信号进行加权组合，形成定向的传输波束，提高发射信号的方向增益。</p>
</li>
<li><p><strong>MIMO 空间信道</strong><br>模拟多径衰落、散射及路径损耗，采用 WINNER II 等复杂空间信道模型，真实反映无线环境。</p>
</li>
<li><p><strong>接收波束成形</strong><br>接收端根据信道特性调整接收波束，最大化信号质量。</p>
</li>
<li><p><strong>OFDM 解调与数据恢复</strong><br>移除循环前缀，FFT 变换回频域，进行符号检测和译码，恢复原始数据。</p>
</li>
</ol>
<h2 id="4-系统优势"><a href="#4-系统优势" class="headerlink" title="4. 系统优势"></a>4. 系统优势</h2><ul>
<li><strong>抗频率选择性衰落</strong>：OFDM 将宽带信道分解为多个平坦子信道。</li>
<li><strong>空间复用</strong>：多天线支持多数据流并行传输，提升频谱效率。</li>
<li><strong>波束成形提升 SNR</strong>：信号能量聚焦于接收端，提高可靠性。</li>
<li><strong>灵活信道模型</strong>：支持多种空间信道模型，便于性能评估和系统设计。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/07/21/pandas/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/07/21/pandas/" class="post-title-link" itemprop="url">pandas</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-07-21 20:17:58 / Modified: 22:26:15" itemprop="dateCreated datePublished" datetime="2025-07-21T20:17:58+08:00">2025-07-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="pandas-基础语法速查"><a href="#pandas-基础语法速查" class="headerlink" title="pandas 基础语法速查"></a>pandas 基础语法速查</h1><p>以下内容基于对 pandas 基础用法的常见问题整理，方便快速入门和复习。</p>
<hr>
<h2 id="1-查看数据"><a href="#1-查看数据" class="headerlink" title="1. 查看数据"></a>1. 查看数据</h2><ul>
<li><p><code>df.head()</code><br>返回数据框前 5 行（默认），可以用 <code>df.head(n)</code> 查看前 n 行。<br>用于快速预览数据。</p>
</li>
<li><p><code>df.iloc[:, :-1].values</code><br>用 <code>.iloc</code> 按位置索引，<code>:,:-1</code> 表示所有行，除最后一列以外的所有列。<br><code>.values</code> 返回对应的 NumPy 数组。</p>
</li>
<li><p><code>df.values</code><br>将整个 DataFrame 转换为 NumPy 数组，包含所有数据（数值和非数值都可能存在）。</p>
</li>
</ul>
<hr>
<h2 id="2-处理缺失值"><a href="#2-处理缺失值" class="headerlink" title="2. 处理缺失值"></a>2. 处理缺失值</h2><ul>
<li><code>df.fillna(0, inplace=True)</code><br>将所有缺失值（NaN）替换为 0，<code>inplace=True</code> 代表直接修改原 DataFrame。</li>
</ul>
<hr>
<h2 id="3-唯一值和类别编码"><a href="#3-唯一值和类别编码" class="headerlink" title="3. 唯一值和类别编码"></a>3. 唯一值和类别编码</h2><ul>
<li><p><code>df.Summary.unique()</code><br>返回列 <code>Summary</code> 中所有唯一的值（去重后的列表）。</p>
</li>
<li><p><code>df.LoudCover.unique()</code><br>类似，返回 <code>LoudCover</code> 列所有唯一值。</p>
</li>
<li><p>Label Encoding  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">  labelencoder = LabelEncoder()</span><br><span class="line">  df[<span class="string">&#x27;Summary&#x27;</span>] = labelencoder.fit_transform(df[<span class="string">&#x27;Summary&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 4. 相关性分析</span></span><br><span class="line">- df.corr()</span><br><span class="line">计算数值列之间的相关系数矩阵（默认是 Pearson 相关系数）。</span><br><span class="line"></span><br><span class="line">注意：执行 df.corr() 前，确保 DataFrame 中只包含数值列，否则可能报错。</span><br><span class="line">- 热力图可视化</span><br><span class="line">```bash</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sn</span><br><span class="line">sn.heatmap(df.select_dtypes(include=[<span class="string">&#x27;number&#x27;</span>]).corr())</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="5-数据拼接与转换"><a href="#5-数据拼接与转换" class="headerlink" title="5. 数据拼接与转换"></a>5. 数据拼接与转换</h2><ul>
<li>pd.concat(cols, axis &#x3D; 1)<br>将列表 cols 中的多个 DataFrame 按列合并成一个大 DataFrame。</li>
<li>.shift(n)<br>对时间序列数据进行平移（滞后或提前）。<br>例如：df.shift(1) 会将所有数据下移一行，原第一行变为 NaN。</li>
</ul>
<h2 id="6-Numpy与Pandas转换"><a href="#6-Numpy与Pandas转换" class="headerlink" title="6. Numpy与Pandas转换"></a>6. Numpy与Pandas转换</h2><ul>
<li>DataFrame转为NumPy数组</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = df.values</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>只保留数值列转为数组</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = df.select_dtypes(include=[<span class="string">&#x27;number&#x27;</span>]).values</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/07/21/%E6%95%B0%E5%AD%97%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/07/21/%E6%95%B0%E5%AD%97%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">数字信号处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-07-21 15:13:01 / Modified: 16:01:38" itemprop="dateCreated datePublished" datetime="2025-07-21T15:13:01+08:00">2025-07-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="数字信号处理-—-DFT和DTFT简介"><a href="#数字信号处理-—-DFT和DTFT简介" class="headerlink" title="数字信号处理 — DFT和DTFT简介"></a>数字信号处理 — DFT和DTFT简介</h1><h2 id="离散时间傅里叶变换（DTFT）"><a href="#离散时间傅里叶变换（DTFT）" class="headerlink" title="离散时间傅里叶变换（DTFT）"></a>离散时间傅里叶变换（DTFT）</h2><p>DTFT（Discrete-Time Fourier Transform）是离散时间信号的傅里叶变换，定义为将离散时间信号映射到连续的频率域。<br>它的数学表达式为：</p>
<p>$$<br>X(\omega) &#x3D; \sum_{n&#x3D;-\infty}^{\infty} x[n] e^{-j \omega n}<br>$$</p>
<ul>
<li>其中，$x[n]$ 是离散时间信号，  </li>
<li>$\omega$ 是连续频率变量，取值范围为 $-\pi \leq \omega \leq \pi$。  </li>
<li>DTFT结果是频率的连续函数。</li>
</ul>
<p>DTFT适合分析无限长或有限长信号的频率特性。</p>
<hr>
<h2 id="连续时间信号采样与离散时间傅里叶变换-DTFT-推导"><a href="#连续时间信号采样与离散时间傅里叶变换-DTFT-推导" class="headerlink" title="连续时间信号采样与离散时间傅里叶变换 (DTFT) 推导"></a>连续时间信号采样与离散时间傅里叶变换 (DTFT) 推导</h2><p>设连续时间信号的傅里叶变换为：</p>
<p>$$<br>X_s(j\omega) &#x3D; \int_{-\infty}^{\infty} x_s(t) \cdot e^{-j \omega t} , dt<br>$$</p>
<p>其中，采样后的信号 $x_s(t)$ 可以表示为：</p>
<p>$$<br>x_s(t) &#x3D; \sum_{k&#x3D;-\infty}^{\infty} x(k T_s) \cdot \delta(t - k T_s)<br>$$</p>
<p>代入傅里叶变换：</p>
<p>$$<br>\begin{aligned}<br>X_s(j\omega) &amp;&#x3D; \int_{-\infty}^{\infty} \left( \sum_{k&#x3D;-\infty}^{\infty} x(k T_s) \cdot \delta(t - k T_s) \right) e^{-j \omega t} , dt \<br>&amp;&#x3D; \sum_{k&#x3D;-\infty}^{\infty} x(k T_s) \cdot e^{-j \omega k T_s} \int_{-\infty}^{\infty} \delta(t - k T_s) , dt \quad \text{(利用线性性质和 $x(t)\delta(t - t_0) &#x3D; x(t_0) \delta(t - t_0)$)} \<br>&amp;&#x3D; \sum_{k&#x3D;-\infty}^{\infty} x(k T_s) \cdot e^{-j \omega k T_s}<br>\end{aligned}<br>$$</p>
<p>令</p>
<ul>
<li>$\Omega &#x3D; \omega T_s$ （连续时间角频率转换为数字频率），  </li>
<li>$x[k] &#x3D; x(k T_s)$ （离散时间信号样本），</li>
</ul>
<p>则上述式子可写为：</p>
<p>$$<br>X(e^{j \Omega}) &#x3D; \sum_{k&#x3D;-\infty}^{\infty} x[k] \cdot e^{-j \Omega k}<br>$$</p>
<p>这就是离散时间傅里叶变换（DTFT）的定义。</p>
<hr>
<p><strong>总结：</strong></p>
<ul>
<li>通过采样，将连续时间信号转换为离散时间信号；  </li>
<li>连续时间傅里叶变换 $X_s(j\omega)$ 可表达为离散时间傅里叶变换 $X(e^{j \Omega})$ 的形式，连接了连续与离散频域。</li>
</ul>
<h2 id="离散傅里叶变换（DFT）"><a href="#离散傅里叶变换（DFT）" class="headerlink" title="离散傅里叶变换（DFT）"></a>离散傅里叶变换（DFT）</h2><p>DFT（Discrete Fourier Transform）是DTFT的离散版本，适用于有限长度的序列。<br>它将一个长度为$N$的序列转换为长度为$N$的频域序列，计算公式为：</p>
<p>$$<br>X[k] &#x3D; \sum_{n&#x3D;0}^{N-1} x[n] e^{-j \frac{2\pi}{N} k n}, \quad k&#x3D;0,1,\ldots,N-1<br>$$</p>
<ul>
<li>$x[n]$ 是长度为$N$的离散信号，  </li>
<li>$X[k]$ 是离散的频率样本点，  </li>
<li>DFT的频率点是均匀分布在 $[0, 2\pi)$ 上。</li>
</ul>
<p>DFT广泛应用于数字信号处理中的频谱分析和滤波。</p>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li><strong>DTFT</strong>：频率连续，信号长度可以无限，理论分析工具。  </li>
<li><strong>DFT</strong>：频率离散，信号长度有限，数值计算工具，常用快速算法FFT实现。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/07/07/%E5%85%B6%E4%BB%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/07/07/%E5%85%B6%E4%BB%96/" class="post-title-link" itemprop="url">其他</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-07-07 17:20:33" itemprop="dateCreated datePublished" datetime="2025-07-07T17:20:33+08:00">2025-07-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-07-19 14:44:22" itemprop="dateModified" datetime="2025-07-19T14:44:22+08:00">2025-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">通信原理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="周期冲激序列的傅立叶变换"><a href="#周期冲激序列的傅立叶变换" class="headerlink" title="周期冲激序列的傅立叶变换"></a>周期冲激序列的傅立叶变换</h2><p>求冲激串 $ \sum_{k&#x3D;-\infty}^{\infty}\delta(t-kT) $ 的傅立叶变换。</p>
<hr>
<h3 id="🚩-思路一：傅立叶级数展开"><a href="#🚩-思路一：傅立叶级数展开" class="headerlink" title="🚩 思路一：傅立叶级数展开"></a>🚩 思路一：傅立叶级数展开</h3><ul>
<li>时间域是周期信号，周期 $T$，所以可以写成傅立叶级数：</li>
</ul>
<p>$$<br>x(t) &#x3D; \sum_{n&#x3D;-\infty}^{\infty} c_n e^{j n \Omega_s t}, \quad \Omega_s &#x3D; \frac{2\pi}{T}.<br>$$</p>
<ul>
<li>求傅立叶系数，对于冲激串：</li>
</ul>
<p>$$<br>c_n &#x3D; \frac{1}{T} \int_{-T&#x2F;2}^{T&#x2F;2} \sum_{k&#x3D;-\infty}^{\infty} \delta(t - kT) e^{-j n \Omega_s t} dt.<br>$$</p>
<p>只会取到 $k &#x3D; 0$：</p>
<p>$$<br>c_n &#x3D; \frac{1}{T}.<br>$$</p>
<ul>
<li>所以：</li>
</ul>
<p>$$<br>x(t) &#x3D; \frac{1}{T} \sum_{n&#x3D;-\infty}^{\infty} e^{j n \Omega_s t}.<br>$$</p>
<ul>
<li>利用傅立叶变换性质，复指数 $e^{j n \Omega_s t}$ 的傅立叶变换：</li>
</ul>
<p>$$<br>\mathcal{F}[e^{j n \Omega_s t}] &#x3D; 2\pi \delta(\omega - n\Omega_s).<br>$$</p>
<ul>
<li>所以：</li>
</ul>
<p>$$<br>X(j\omega) &#x3D; \frac{1}{T} \sum_{n&#x3D;-\infty}^{\infty} 2\pi \delta(\omega - n\Omega_s)<br>&#x3D; \frac{2\pi}{T} \sum_{n&#x3D;-\infty}^{\infty} \delta(\omega - n\Omega_s).<br>$$</p>
<hr>
<h3 id="🚩-思路二：直接利用时移性质"><a href="#🚩-思路二：直接利用时移性质" class="headerlink" title="🚩 思路二：直接利用时移性质"></a>🚩 思路二：直接利用时移性质</h3><hr>
<blockquote>
<p><strong>Notice</strong><br><br>时移性质是指信号在时域中的时间延迟会导致其傅立叶变换在相位上发生线性变化。<br><br>f(t)沿时间右轴t0得到f(t-t0),则傅立叶变换F(w)将在所有频率点上乘上一个相位因子$e^{-jwt_0}$<br></p>
</blockquote>
<hr>
<ul>
<li>单个冲激的傅立叶变换：</li>
</ul>
<p>$$<br>\mathcal{F}[\delta(t)] &#x3D; 1, \quad \mathcal{F}[\delta(t - kT)] &#x3D; e^{-j \omega kT}.<br>$$</p>
<ul>
<li>求和：</li>
</ul>
<p>$$<br>X(j\omega) &#x3D; \sum_{k&#x3D;-\infty}^{\infty} e^{-j \omega kT}<br>&#x3D; \sum_{k&#x3D;-\infty}^{\infty} e^{-j k \omega T}.<br>$$</p>
<ul>
<li>这本质上是一个无限等比数列在单位圆上的和，用冲激串采样定理：</li>
</ul>
<p>$$<br>\sum_{k&#x3D;-\infty}^{\infty} e^{-j k \omega T}<br>&#x3D; 2\pi \sum_{n&#x3D;-\infty}^{\infty} \delta(\omega - n\Omega_s) \cdot \frac{1}{T}.<br>$$</p>
<ul>
<li>所以同样得到：</li>
</ul>
<p>$$<br>X(j\omega) &#x3D; \frac{2\pi}{T} \sum_{n&#x3D;-\infty}^{\infty} \delta(\omega - n\Omega_s).<br>$$</p>
<hr>
<h2 id="✅-总结"><a href="#✅-总结" class="headerlink" title="✅ 总结"></a>✅ 总结</h2><ul>
<li><p>时间域：周期冲激串，周期 $T$</p>
</li>
<li><p>频域：等间隔频域冲激串，间隔 $\Omega_s &#x3D; \frac{2\pi}{T}$，幅度系数 $\frac{2\pi}{T}$</p>
</li>
</ul>
<h2 id="拉丁字母"><a href="#拉丁字母" class="headerlink" title="拉丁字母"></a>拉丁字母</h2><table>
<thead>
<tr>
<th>LaTeX代码</th>
<th>渲染效果</th>
</tr>
</thead>
<tbody><tr>
<td>$\alpha$</td>
<td>α</td>
</tr>
<tr>
<td>$\beta$</td>
<td>β</td>
</tr>
<tr>
<td>$\gamma$</td>
<td>γ</td>
</tr>
<tr>
<td>$\delta$</td>
<td>δ</td>
</tr>
<tr>
<td>$\epsilon$</td>
<td>ε</td>
</tr>
<tr>
<td>$\zeta$</td>
<td>ζ</td>
</tr>
<tr>
<td>$\eta$</td>
<td>η</td>
</tr>
<tr>
<td>$\theta$</td>
<td>θ</td>
</tr>
<tr>
<td>$\iota$</td>
<td>ι</td>
</tr>
<tr>
<td>$\kappa$</td>
<td>κ</td>
</tr>
<tr>
<td>$\lambda$</td>
<td>λ</td>
</tr>
<tr>
<td>$\mu$</td>
<td>μ</td>
</tr>
<tr>
<td>$\nu$</td>
<td>ν</td>
</tr>
<tr>
<td>$\xi$</td>
<td>ξ</td>
</tr>
<tr>
<td>$\pi$</td>
<td>π</td>
</tr>
<tr>
<td>$\rho$</td>
<td>ρ</td>
</tr>
<tr>
<td>$\sigma$</td>
<td>σ</td>
</tr>
<tr>
<td>$\varsigma$</td>
<td>ς</td>
</tr>
<tr>
<td>$\tau$</td>
<td>τ</td>
</tr>
<tr>
<td>$\upsilon$</td>
<td>υ</td>
</tr>
<tr>
<td>$\phi$</td>
<td>φ</td>
</tr>
<tr>
<td>$\varphi$</td>
<td>ϕ</td>
</tr>
<tr>
<td>$\chi$</td>
<td>χ</td>
</tr>
<tr>
<td>$\psi$</td>
<td>ψ</td>
</tr>
<tr>
<td>$\omega$</td>
<td>ω</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/07/02/%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/07/02/%E5%BE%AA%E7%8E%AF%E5%8D%B7%E7%A7%AF/" class="post-title-link" itemprop="url">循环卷积</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-07-02 06:39:15 / Modified: 06:56:43" itemprop="dateCreated datePublished" datetime="2025-07-02T06:39:15+08:00">2025-07-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">通信原理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="循环卷积定义"><a href="#循环卷积定义" class="headerlink" title="循环卷积定义"></a>循环卷积定义</h2><p>给定两个长度为 ( N ) 的序列 ( x[n] ) 和 ( h[n] )，它们的循环卷积定义为：</p>
<p>$$<br>y_{\text{circ}}[n] &#x3D; \sum_{m&#x3D;0}^{N-1} x[m] \cdot h[(n - m) \bmod N], \quad n&#x3D;0,1,\ldots,N-1<br>$$</p>
<p>其中，( \bmod ) 表示取模运算，序列被视为周期为 ( N )。</p>
<h2 id="循环卷积与-DFT-的关系"><a href="#循环卷积与-DFT-的关系" class="headerlink" title="循环卷积与 DFT 的关系"></a>循环卷积与 DFT 的关系</h2><p>离散傅里叶变换（DFT）定义为：</p>
<p>$$<br>X[k] &#x3D; \sum_{n&#x3D;0}^{N-1} x[n] \cdot e^{-j \frac{2\pi}{N} kn}, \quad k&#x3D;0,1,\ldots,N-1<br>$$</p>
<p>逆变换为：</p>
<p>$$<br>x[n] &#x3D; \frac{1}{N} \sum_{k&#x3D;0}^{N-1} X[k] \cdot e^{j \frac{2\pi}{N} kn}<br>$$</p>
<p>关键结论：</p>
<p>两个长度为 ( N ) 的序列 ( x[n] ) 和 ( h[n] ) 的循环卷积 ( y_{\text{circ}}[n] ) 的 DFT 等于它们的 DFT 乘积：</p>
<p>$$<br>Y[k] &#x3D; X[k] \cdot H[k]<br>$$</p>
<p>反过来，循环卷积可通过 IDFT 计算：</p>
<p>$$<br>y_{\text{circ}}[n] &#x3D; \text{IDFT}{ X[k] \cdot H[k] }<br>$$</p>
<h2 id="傅立叶变换等于-DFT-的说明"><a href="#傅立叶变换等于-DFT-的说明" class="headerlink" title="傅立叶变换等于 DFT 的说明"></a>傅立叶变换等于 DFT 的说明</h2><ul>
<li>离散时间傅立叶变换（DTFT）是连续频率的变换。</li>
<li>DFT 是对长度为 ( N ) 的序列 DTFT 在均匀采样点 ( \omega &#x3D; \frac{2\pi k}{N} ) 上的采样。</li>
<li>因此，DFT 是 DTFT 的离散版本。</li>
</ul>
<h2 id="线性卷积与循环卷积的区别与联系"><a href="#线性卷积与循环卷积的区别与联系" class="headerlink" title="线性卷积与循环卷积的区别与联系"></a>线性卷积与循环卷积的区别与联系</h2><ul>
<li>线性卷积长度为 ( N_x + N_h - 1 )，无周期性。</li>
<li>循环卷积长度固定为 ( N )，信号视为周期为 ( N )。</li>
<li>对 ( x[n], h[n] ) 零填充至长度 ( N \geq N_x + N_h -1 )，线性卷积结果等于循环卷积的前 ( N_x + N_h -1 ) 项。</li>
</ul>
<h2 id="在-OFDM-中的应用"><a href="#在-OFDM-中的应用" class="headerlink" title="在 OFDM 中的应用"></a>在 OFDM 中的应用</h2><ul>
<li>OFDM 信号长度为 ( N )，添加长度大于等于 ( L-1 ) 的循环前缀（CP），其中 ( L ) 是信道长度。</li>
<li>CP 使得线性卷积等价于循环卷积。</li>
<li>接收端去除 CP 后，信号通过循环卷积传输，可用 FFT 频域乘法实现均衡。</li>
</ul>
<h2 id="关键公式汇总"><a href="#关键公式汇总" class="headerlink" title="关键公式汇总"></a>关键公式汇总</h2><p>循环卷积：</p>
<p>$$<br>y_{\text{circ}}[n] &#x3D; \sum_{m&#x3D;0}^{N-1} x[m] \cdot h[(n - m) \bmod N]<br>$$</p>
<p>DFT 计算循环卷积：</p>
<p>$$<br>Y[k] &#x3D; X[k] \cdot H[k]<br>$$</p>
<p>$$<br>y_{\text{circ}}[n] &#x3D; \text{IDFT}{ Y[k] }<br>$$</p>
<h2 id="代码示范"><a href="#代码示范" class="headerlink" title="代码示范"></a>代码示范</h2><p>下面是一个最小可运行的示例，演示 <strong>OFDM 信号添加循环前缀后经过线性卷积，与去除 CP 后与循环卷积一致</strong> 的原理。</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# ----------------------------
# 参数设置
N = 8          # IFFT/FFT 大小（子载波数）
L = 3          # 信道脉冲响应长度
CP = L - 1     # 循环前缀长度，必须 &gt;= L - 1

# ----------------------------
# 生成随机 OFDM 符号（频域）
# np.random.seed(0)  # 不设置种子，每次随机
X = np.random.randn(N) + 1j*np.random.randn(N)

# IFFT 变换到时域
x = np.fft.ifft(X)

# 添加循环前缀：复制末尾 CP 位到前面
x_cp = np.concatenate([x[-CP:], x])

# ----------------------------
# 信道冲激响应
h = np.array([0.5, 0.3, 0.2])

# 线性卷积：通过信道传输
y = np.convolve(x_cp, h)

# 接收端去除 CP：保留长度 N 的有效符号
y_cp_removed = y[CP : CP + N]

# ----------------------------
# 循环卷积（频域等效）
# 零填充信道到长度 N
h_padded = np.pad(h, (0, N - L))
H = np.fft.fft(h_padded)

# 频域点乘
Y_freq = np.fft.fft(x) * H

# IFFT 得到循环卷积结果
y_circular = np.fft.ifft(Y_freq)

# ----------------------------
# 可视化结果对比
fig, axs = plt.subplots(4, 1, figsize=(10, 10))

# IFFT 后的时域符号
axs[0].stem(np.arange(N), np.real(x), basefmt=&quot; &quot;, use_line_collection=True)
axs[0].set_title(&#39;1. IFFT 时域信号 (未加 CP)&#39;)
axs[0].grid()

# 加 CP 后的时域符号
axs[1].stem(np.arange(len(x_cp)), np.real(x_cp), basefmt=&quot; &quot;, use_line_collection=True)
axs[1].set_title(f&#39;2. 加 CP 后时域信号 (CP length = &#123;CP&#125;)&#39;)
axs[1].axvspan(0, CP-1, color=&#39;orange&#39;, alpha=0.3, label=&#39;Cyclic Prefix&#39;)
axs[1].legend()
axs[1].grid()

# 通过信道后的线性卷积
axs[2].stem(np.arange(len(y)), np.real(y), basefmt=&quot; &quot;, use_line_collection=True)
axs[2].set_title(&#39;3. 经过信道: 线性卷积&#39;)
axs[2].axvspan(0, CP-1, color=&#39;orange&#39;, alpha=0.3, label=&#39;CP to discard&#39;)
axs[2].legend()
axs[2].grid()

# 去除 CP 后 vs 循环卷积结果
axs[3].stem(np.arange(N), np.real(y_cp_removed), markerfmt=&#39;bo&#39;,
            label=&#39;Linear conv. + CP removed&#39;,
            basefmt=&quot; &quot;, use_line_collection=True)
axs[3].stem(np.arange(N), np.real(y_circular), markerfmt=&#39;rx&#39;,
            label=&#39;Circular conv.&#39;,
            basefmt=&quot; &quot;, use_line_collection=True)
axs[3].set_title(&#39;4. 对比: 线性卷积去 CP vs. 循环卷积&#39;)
axs[3].legend()
axs[3].grid()

plt.tight_layout()
plt.show()

# ----------------------------
# 计算误差（验证一致性）
error = np.linalg.norm(y_cp_removed - y_circular)
print(f&quot;Difference (CP removed vs. circular): &#123;error:.3e&#125;&quot;)
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/06/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/06/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" class="post-title-link" itemprop="url">线性代数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-25 22:52:26" itemprop="dateCreated datePublished" datetime="2025-06-25T22:52:26+08:00">2025-06-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-20 13:53:04" itemprop="dateModified" datetime="2025-08-20T13:53:04+08:00">2025-08-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="线性方程组与矩阵运算总结"><a href="#线性方程组与矩阵运算总结" class="headerlink" title="线性方程组与矩阵运算总结"></a>线性方程组与矩阵运算总结</h1><p>本笔记总结了关于线性方程组求解及矩阵运算的几个关键知识点，涵盖：</p>
<ul>
<li>MATLAB 中 <code>\</code> 与 <code>/</code> 操作符的含义</li>
<li>对角矩阵的求逆方法</li>
<li>噪声环境下线性模型的最小二乘估计</li>
</ul>
<hr>
<h2 id="1-线性方程组求解：-AX-B"><a href="#1-线性方程组求解：-AX-B" class="headerlink" title="1. 线性方程组求解：$AX &#x3D; B$"></a>1. 线性方程组求解：$AX &#x3D; B$</h2><p>我们常见的线性模型为：</p>
<p>$$<br>AX &#x3D; B<br>$$</p>
<p>目标是求出未知矩阵或向量 $X$。</p>
<hr>
<h3 id="1-1-MATLAB-中的-与-含义"><a href="#1-1-MATLAB-中的-与-含义" class="headerlink" title="1.1 MATLAB 中的 \ 与 / 含义"></a>1.1 MATLAB 中的 <code>\</code> 与 <code>/</code> 含义</h3><table>
<thead>
<tr>
<th>表达式</th>
<th>数学意义</th>
<th>求解方式</th>
</tr>
</thead>
<tbody><tr>
<td><code>X = A \ B</code></td>
<td>解 $AX &#x3D; B$</td>
<td>左除（推荐使用）</td>
</tr>
<tr>
<td><code>X = B / A</code></td>
<td>解 $XA &#x3D; B$</td>
<td>右除（适用于对偶情形）</td>
</tr>
</tbody></table>
<h4 id="情况说明："><a href="#情况说明：" class="headerlink" title="情况说明："></a>情况说明：</h4><ul>
<li>如果 $A$ 是<strong>方阵且可逆</strong>：<ul>
<li><code>A \ B</code> 等价于 $A^{-1} B$</li>
<li><code>B / A</code> 等价于 $B A^{-1}$</li>
</ul>
</li>
<li>如果 $A$ 是<strong>非方阵或奇异</strong>：<ul>
<li>MATLAB 会自动采用<strong>最小二乘解</strong>：<br>$$<br>\hat{X} &#x3D; \arg\min_X |AX - B|_2<br>$$</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-对角矩阵的求逆"><a href="#2-对角矩阵的求逆" class="headerlink" title="2. 对角矩阵的求逆"></a>2. 对角矩阵的求逆</h2><p>给定一个 $n \times n$ 的对角矩阵 $D$：</p>
<p>$$<br>D &#x3D; \begin{bmatrix}<br>d_1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; d_2 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; \cdots &amp; d_n<br>\end{bmatrix}<br>$$</p>
<h3 id="2-1-可逆条件"><a href="#2-1-可逆条件" class="headerlink" title="2.1 可逆条件"></a>2.1 可逆条件</h3><ul>
<li>如果所有 $d_i \ne 0$，则 $D$ 可逆。</li>
<li>若任一 $d_i &#x3D; 0$，则 $D$ 不可逆。</li>
</ul>
<h3 id="2-2-逆矩阵表达式"><a href="#2-2-逆矩阵表达式" class="headerlink" title="2.2 逆矩阵表达式"></a>2.2 逆矩阵表达式</h3><p>$$<br>D^{-1} &#x3D; \begin{bmatrix}<br>\frac{1}{d_1} &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; \frac{1}{d_2} &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; \cdots &amp; \frac{1}{d_n}<br>\end{bmatrix}<br>$$</p>
<p>即：<strong>对角矩阵的逆，仍是对角矩阵，每个对角元素取倒数即可</strong>。</p>
<hr>
<h2 id="3-线性模型与最小二乘法"><a href="#3-线性模型与最小二乘法" class="headerlink" title="3. 线性模型与最小二乘法"></a>3. 线性模型与最小二乘法</h2><p>在现实中常见的线性模型为：</p>
<p>$$<br>Y &#x3D; H \cdot X + N<br>$$</p>
<p>其中：</p>
<ul>
<li>$Y$：观测值（已知）</li>
<li>$H$：待估计矩阵</li>
<li>$X$：输入矩阵或导频（已知）</li>
<li>$N$：噪声项</li>
</ul>
<h3 id="3-1-无噪声时（理想情况）"><a href="#3-1-无噪声时（理想情况）" class="headerlink" title="3.1 无噪声时（理想情况）"></a>3.1 无噪声时（理想情况）</h3><p>$$<br>Y &#x3D; H \cdot X<br>$$</p>
<p>若 $X$ 可逆，则可直接求解：</p>
<p>$$<br>H &#x3D; Y X^{-1}<br>$$</p>
<h3 id="3-2-含噪声时（实际情况）"><a href="#3-2-含噪声时（实际情况）" class="headerlink" title="3.2 含噪声时（实际情况）"></a>3.2 含噪声时（实际情况）</h3><p>$$<br>Y &#x3D; H \cdot X + N<br>$$</p>
<p>此时无法直接求逆，只能求<strong>最小二乘解</strong>：</p>
<p>$$<br>\hat{H} &#x3D; \arg\min_H | Y - H X |_F^2<br>$$</p>
<p>求解方式如下：</p>
<ul>
<li><p>若 $X$ 为满秩方阵：</p>
<p>$$<br>\hat{H} &#x3D; Y X^{-1}<br>$$</p>
</li>
<li><p>若 $X$ 非方阵或秩不足，使用伪逆（Moore-Penrose）：</p>
<p>$$<br>\hat{H} &#x3D; Y X^\dagger &#x3D; Y (X^H X)^{-1} X^H<br>$$</p>
</li>
</ul>
<hr>
<h2 id="4-MATLAB-示例代码片段"><a href="#4-MATLAB-示例代码片段" class="headerlink" title="4. MATLAB 示例代码片段"></a>4. MATLAB 示例代码片段</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 1. 解 AX = B 的最小二乘解</span></span><br><span class="line">X = A \ B;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 2. 解 XA = B 的最小二乘解</span></span><br><span class="line">X = B / A;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 3. 对角矩阵求逆</span></span><br><span class="line">D = <span class="built_in">diag</span>([d1, d2, d3]);</span><br><span class="line">D_inv = <span class="built_in">diag</span>(<span class="number">1</span> ./ <span class="built_in">diag</span>(D));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="矩阵特征值的推导过程"><a href="#矩阵特征值的推导过程" class="headerlink" title="矩阵特征值的推导过程"></a>矩阵特征值的推导过程</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>设 A 是一个 n×n 的方阵，如果存在非零向量 v 和标量 λ，使得：</p>
<pre><code>A v = λ v
</code></pre>
<p>则 λ 称为矩阵 A 的一个特征值，v 称为对应的特征向量。</p>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>从特征值定义出发：</p>
<pre><code>A v = λ v
</code></pre>
<p>可以写成：</p>
<pre><code>A v = λ I v
</code></pre>
<p>将等式两边移项，得到：</p>
<pre><code>A v - λ I v = 0
</code></pre>
<p>提取 v：</p>
<pre><code>(A - λ I) v = 0
</code></pre>
<p>这是一个齐次线性方程组。</p>
<h2 id="非零解条件"><a href="#非零解条件" class="headerlink" title="非零解条件"></a>非零解条件</h2><p>为了使该方程有非零解 v ≠ 0，矩阵 (A - λ I) 必须是奇异矩阵，即其行列式为零：</p>
<pre><code>det(A - λ I) = 0
</code></pre>
<p>这称为特征方程，解这个方程可以得到矩阵 A 的所有特征值 λ。</p>
<hr>
<p>总结：</p>
<pre><code>A v = λ v  ⇒  (A - λ I) v = 0  
有非零解 ⇔ det(A - λ I) = 0
</code></pre>
<h1 id="共轭矩阵"><a href="#共轭矩阵" class="headerlink" title="共轭矩阵"></a>共轭矩阵</h1><p>A的共轭矩阵记为$A^H$.<br>求法：<br>也称为Hermite阵。Hermite阵中每一个第i 行第j 列的元素都与第j 行第i 列的元素的共轭相等（然而矩阵A的共轭矩阵并非Hermite阵）。自共轭矩阵是矩阵本身先转置再把矩阵中每个元素取共轭得到的矩阵。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/06/17/PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/06/17/PyTorch/" class="post-title-link" itemprop="url">PyTorch</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-17 20:57:35" itemprop="dateCreated datePublished" datetime="2025-06-17T20:57:35+08:00">2025-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-30 21:36:25" itemprop="dateModified" datetime="2025-06-30T21:36:25+08:00">2025-06-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h1 id="PyTorch-Quick-Start-Guide-🚀"><a href="#PyTorch-Quick-Start-Guide-🚀" class="headerlink" title="PyTorch Quick Start Guide 🚀"></a>PyTorch Quick Start Guide 🚀</h1><h2 id="📚-基本概念"><a href="#📚-基本概念" class="headerlink" title="📚 基本概念"></a>📚 基本概念</h2><h3 id="📐-常见数据的-Tensor-维度"><a href="#📐-常见数据的-Tensor-维度" class="headerlink" title="📐 常见数据的 Tensor 维度"></a>📐 常见数据的 Tensor 维度</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>维度说明</th>
<th>示例 Shape</th>
</tr>
</thead>
<tbody><tr>
<td>Scalar（标量）</td>
<td>0-D Tensor</td>
<td><code>torch.tensor(3.14)</code></td>
</tr>
<tr>
<td>Vector（向量）</td>
<td>1-D Tensor</td>
<td><code>(N,)</code> → <code>[1, 2, 3]</code></td>
</tr>
<tr>
<td>Audio（音频）</td>
<td>1-D 或 2-D（多通道）</td>
<td><code>(samples,)</code> or <code>(channels, samples)</code></td>
</tr>
<tr>
<td>Grayscale Image</td>
<td>2-D 或 3-D（加 batch）</td>
<td><code>(H, W)</code> or <code>(N, 1, H, W)</code></td>
</tr>
<tr>
<td>RGB Image</td>
<td>3-D（C,H,W）或 4-D（N,C,H,W）</td>
<td><code>(3, H, W)</code> or <code>(N, 3, H, W)</code></td>
</tr>
<tr>
<td>Video</td>
<td>4-D（T,C,H,W）或 5-D（N,T,C,H,W）</td>
<td><code>(frames, 3, H, W)</code></td>
</tr>
</tbody></table>
<blockquote>
<p>✅ <strong>常用格式缩写</strong>  </p>
<ul>
<li><code>N</code>: Batch size  </li>
<li><code>C</code>: Channels  </li>
<li><code>H</code>: Height  </li>
<li><code>W</code>: Width  </li>
<li><code>T</code>: Time steps &#x2F; frames</li>
</ul>
</blockquote>
<hr>
<h2 id="常用函数小结"><a href="#常用函数小结" class="headerlink" title="常用函数小结"></a>常用函数小结</h2><table>
<thead>
<tr>
<th>函数</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>unsqueeze(dim)</td>
<td>插入新维度</td>
</tr>
<tr>
<td>squeeze(dim)</td>
<td>去除大小为1的维度</td>
</tr>
<tr>
<td>transpose(a,b)</td>
<td>交换两个维度</td>
</tr>
<tr>
<td>.view()</td>
<td>改变形状</td>
</tr>
<tr>
<td>.reshape()</td>
<td>改变形状</td>
</tr>
</tbody></table>
<h2 id="⚙️-常用操作示例"><a href="#⚙️-常用操作示例" class="headerlink" title="⚙️ 常用操作示例"></a>⚙️ 常用操作示例</h2><pre><code class="language-python">import torch

# 创建一个随机的 RGB 图片 Tensor: (C,H,W)
x = torch.randn(3, 224, 224)
y = torch.zeros([2,3])
# 输出size, e.g., torch.Size([2,3])
x.shape
# cat: concatenate multiple tensors
x = torch.zeros([2,1,3])
y = torch.zeros([2,3,3])
z = torch.zeros([2,2,3])
w = torch.cat([x,y,z], dim = 1)
# torch.Size([2,6,3])
w.shape

# 查看形状
print(x.shape)  # torch.Size([3, 224, 224])

# 交换维度: transpose
x_transposed = x.transpose(1, 2)  # (C, W, H)

# 重新排列所有维度: permute
x_permuted = x.permute(1, 2, 0)   # (H, W, C)

# 展平为一维
x_flatten = x.view(-1)

# 添加 batch 维度
x_batched = x.unsqueeze(0)  # (1, C, H, W)

# 去除维度为 1 的维度
x_squeezed = x_batched.squeeze(0)  # (C, H, W)

# 数据类型转换
x_double = x.double()
x_int = x.int()

# 基本运算
y = torch.randn_like(x)
z = x + y  # element-wise 加法

# 移动到 GPU
x_cuda = x.to(&#39;cuda&#39;)

# 自动求导示例
w = torch.randn(5, requires_grad=True)
loss = (w ** 2).sum()
loss.backward()
print(w.grad)
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/06/16/MachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/06/16/MachineLearning/" class="post-title-link" itemprop="url">MachineLearning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-16 23:42:25" itemprop="dateCreated datePublished" datetime="2025-06-16T23:42:25+08:00">2025-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-26 10:03:49" itemprop="dateModified" datetime="2025-08-26T10:03:49+08:00">2025-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      
          <h2 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h2><p>Look for a function for e.g., image recognition, speech recognition.  </p>
<ul>
<li>Regression<br>The function outputs a scalar</li>
<li>Classification<br>Given options (classes), the function outputs the correct one.</li>
<li>Structured Learning</li>
</ul>
<h3 id="Linear-Model"><a href="#Linear-Model" class="headerlink" title="Linear Model"></a>Linear Model</h3><h4 id="Define-function-with-unknown-parameters"><a href="#Define-function-with-unknown-parameters" class="headerlink" title="Define function with unknown parameters"></a>Define function with unknown parameters</h4><p>Model: $y &#x3D; b + w_1x_1$ based on domain knowledge<br>w (weight) and b (bias) are unknown parameters (learn from data)<br>如果是一组包含有多个feature（例如同时考虑前七天的数据来进行预测）<br>那么$y &#x3D; b + \sum_j w_jx_j$</p>
<h4 id="Define-Loss-from-Training-Data"><a href="#Define-Loss-from-Training-Data" class="headerlink" title="Define Loss from Training Data"></a>Define Loss from Training Data</h4><p>Loss is a function of the unknown parameters: L(b,w)<br>e.g., L(0.5k, 1) corresponding to $\hat{y} &#x3D; 0.5k + 1x_1$.<br>$\hat{y}$ is predicted value, y is ground truth (target)<br>我们定义一个loss function为:<br>$e_n &#x3D; |y_n - \hat{y_n}|$  </p>
<p>Loss: $L(b, w) &#x3D; \frac{1}{N} \sum_{n&#x3D;1}^{N} \left| y_n - \hat{y}_n \right|$</p>
<p>其中y为真实值，$\hat{y}$为预测值,这里我们定义的Loss是mean absolute error （MAE），如果是mean square error（MSE），则是差值的的平方。</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>除了常见的 MAE（平均绝对误差）和 MSE（均方误差）之外，NMSE（归一化均方误差）也是一种常用的评价指标。NMSE<br><br>是将均方误差归一化到真实信号的能量上，计算公式为：<br><br>$\text{NMSE} &#x3D; \frac{\sum ( \hat{y}_i - y_i )^2}{\sum y_i^2}$<br><br>它衡量的是预测误差相对于真实信号强度的比例，适合用于信号处理、通信等领域，可以更公平地反映预测性能，特别是在信号幅度变化较大时。<br></p>
</blockquote>
<hr>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>我们通过最小化损失函数 <em>L(b, w)</em> 来学习未知参数 b 和 w。优化目标为：</p>
<p>$b^<em>, w^</em> &#x3D; \arg\min_{b, w} L(b, w)$</p>
<p>我们可以使用梯度下降法进行优化，参数更新如下：</p>
<p>$w \leftarrow w - \eta \frac{\partial L}{\partial w}, \quad<br>b \leftarrow b - \eta \frac{\partial L}{\partial b}$</p>
<p>其中 $\eta$ 为学习率。对于 MAE 损失，由于其不可导点较多，可以使用次梯度法；对于 MSE 损失，则可以直接使用标准梯度下降算法。</p>
<h3 id="Sophisticated-Models"><a href="#Sophisticated-Models" class="headerlink" title="Sophisticated Models"></a>Sophisticated Models</h3><p>Beacuse linear models have severe limitation (model bias), so we include more sophisticated models.</p>
<h4 id="Sigmoid-function（logistic-regression）"><a href="#Sigmoid-function（logistic-regression）" class="headerlink" title="Sigmoid function（logistic regression）"></a>Sigmoid function（logistic regression）</h4><p>$y &#x3D; b + wx_1 &#x3D;&gt; y &#x3D; b + \sum_i c_i \cdot \sigma(b_i + w_ix_1)$<br>注：不同的w，b，c可以让sigmoid曲线可以拟合不同的曲线。</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>输入层: $x_j (j &#x3D; 1…d)$<br><br>   ↓ <br><br>隐藏层: $h_i &#x3D; \sigma(b_i + \sum w_ij x_j) (i &#x3D; 1…N)$<br><br>   ↓ <br><br>输出层: $y &#x3D; b + \sum c_i h_i$ (标量输出)<br></p>
</blockquote>
<hr>
<p>因此这里设计到deep learning中的<strong>hiden node</strong>的概念(hiden node的数量可以理解为用来拟合的ReLU曲线数量)，隐藏点越多，能够拟合出来的function越准确。<br>注意这里是多个sigmoid的和，因为实际情况可能是由多个sigmoid拟合而成。<br>相对应的，对于$y &#x3D; b + \sum_j w_jx_j$, 则有$y &#x3D; b + \sum_i c_i \cdot \sigma(b_i + \sum_j w_{ij} x_j)$  </p>
<p>示例：i &#x3D; 1,2,3; j &#x3D; 1,2,3 时的显式形式<br>$$<br>r_1 &#x3D; b_1 + w_{11}x_1 + w_{12}x_2 + w_{13}x_3<br>$$</p>
<p>$$<br>r_2 &#x3D; b_2 + w_{21}x_1 + w_{22}x_2 + w_{23}x_3<br>$$</p>
<p>$$<br>r_3 &#x3D; b_3 + w_{31}x_1 + w_{32}x_2 + w_{33}x_3<br>$$</p>
<p>向量化（矩阵）形式<br>$$<br>\mathbf{r} &#x3D; W \mathbf{x} + \mathbf{b}<br>$$</p>
<p>然后对生成的r做sigmoid函数，矩阵表达形式如下</p>
<p>$$<br>\mathbf{a} &#x3D; \sigma(\mathbf{r})<br>$$</p>
<p>最后的输出y矩阵表达形式如下：<br>$$<br>y &#x3D; b + \mathbf{c}^T \mathbf{a}<br>$$</p>
<p>Optimization部分，先将所有unknown parameters放到$\mathbf{\theta}$向量中<br>$$<br>\mathbf{\theta} &#x3D; \left[ b,\ c_1,\ \dots,\ c_i,\ b_1,\ \dots,\ b_i,\ w_{11},\ \dots,\ w_{ij} \right]<br>$$</p>
<p>定义Loss function $L(\theta)$,最后想要取得的 $\mathbf{\theta^{*}} &#x3D; \arg\min_{\theta} L$  </p>
<p>因此我们同样的使用梯度下降更新去得到最后的$\mathbf{\theta^{*}}$<br>$$<br>\theta \leftarrow \theta - \eta \cdot \nabla_\theta L(\theta)<br>$$  </p>
<p><strong>Batch Size</strong>, <strong>Epoch</strong>, <strong>N</strong><br>1,000 examples (N &#x3D; 1,000), Batch size is 100, so 100 update in 1 epoch (10 Batch).</p>
<h3 id="Use-Quadratic-and-Higher-Order-Terms"><a href="#Use-Quadratic-and-Higher-Order-Terms" class="headerlink" title="Use Quadratic and Higher-Order Terms"></a>Use Quadratic and Higher-Order Terms</h3><p>To increase model expressiveness, you can replace the linear input with polynomial terms.</p>
<p>For example, a quadratic input:<br>$$<br>y &#x3D; b + w_1 \cdot x_1 + w_2 \cdot x_1^2 + w_3 \cdot x_1^3 + … + w_n \cdot x_1^n<br>$$</p>
<h4 id="Overfitting-Risk"><a href="#Overfitting-Risk" class="headerlink" title="Overfitting Risk"></a>Overfitting Risk</h4><ul>
<li>Adding higher-degree polynomials increases the parameter space.</li>
<li>The model can fit training data too well, including noise.</li>
<li>This leads to poor generalization on unseen data.</li>
</ul>
<h5 id="Rugularization-to-Prevent-Overfitting"><a href="#Rugularization-to-Prevent-Overfitting" class="headerlink" title="Rugularization to Prevent Overfitting"></a>Rugularization to Prevent Overfitting</h5><p>Add a penalty term to the loss function:</p>
<p>为了防止过拟合，通常在损失函数中加入正则化项，对权重进行惩罚。<br>常见的有 <strong>L2 正则化（权重衰减）</strong>：</p>
<p>$\text{Loss} &#x3D; \text{MSE} + \lambda \sum_{i} w_i^2$</p>
<ul>
<li>其中，$\lambda$ 是正则化系数（也叫 <strong>weight decay</strong>）。</li>
<li>通过限制权重 $w_i 的大小，减少模型对高阶项的过度依赖，从而降低过拟合风险。</li>
</ul>
<h5 id="训练时如何加上正则化相"><a href="#训练时如何加上正则化相" class="headerlink" title="训练时如何加上正则化相"></a>训练时如何加上正则化相</h5><p>训练模型时，需要在参数更新步骤中加上正则化项。<br>以 <strong>L2 正则化</strong> 为例，每次参数更新公式：</p>
<p>$w :&#x3D; w - \eta (\nabla L + \lambda w)$</p>
<ul>
<li>$\eta$：学习率（learning rate）</li>
<li>$\nabla L$：损失函数对参数的梯度</li>
<li>$\lambda w$：正则化项对参数的梯度（L2 正则化）</li>
</ul>
<h5 id="Dropout-Randomly-Dropping-Neurons"><a href="#Dropout-Randomly-Dropping-Neurons" class="headerlink" title="Dropout: Randomly Dropping Neurons"></a>Dropout: Randomly Dropping Neurons</h5><p>Dropout is a technique where randomly selected neurons are ignored during training.</p>
<ul>
<li>For each training iteration, with probability <code>p</code>, a neuron’s output is set to 0.</li>
<li>At test time, all neurons are active, but their outputs are scaled by <code>p</code>.</li>
</ul>
<p><strong>Training Phase Formula:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h̃ = h * m</span><br></pre></td></tr></table></figure>

<p>需要注意的是：这个矩阵乘法是按元素一一相乘（逐元素屏蔽）。并且dropout不会减少神经元数量和模型参数数量，只是使在训练时部分神经元临时失活。</p>
<ul>
<li><code>h</code> &#x3D; hidden activation vector  </li>
<li><code>m</code> &#x3D; Bernoulli mask vector (with probability <code>p</code>)</li>
</ul>
<p>✅ <em>Benefits</em>:</p>
<ul>
<li>Reduces co-dependence between neurons  </li>
<li>Acts like an ensemble of multiple sub-models  </li>
<li>Improves robustness and generalization</li>
</ul>
<hr>
<p>✅ 直观理解</p>
<p>每次更新时，把权重往 0 再拉一点，<br>防止它们变得太大，避免模型拟合噪声，提升泛化能力。</p>
<h3 id="PyTorch-CNN-二分类示例"><a href="#PyTorch-CNN-二分类示例" class="headerlink" title="PyTorch CNN 二分类示例"></a>PyTorch CNN 二分类示例</h3><h4 id="预处理transform"><a href="#预处理transform" class="headerlink" title="预处理transform"></a>预处理transform</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="自定义Dataset"><a href="#自定义Dataset" class="headerlink" title="自定义Dataset"></a>自定义Dataset</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class MyData(Dataset):</span><br><span class="line">    def __init__(self, root_dir, classes, transform=None):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.classes = classes</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">        self.img_paths = []</span><br><span class="line">        self.labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx, cls <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            cls_path = os.path.join(root_dir, cls)</span><br><span class="line">            <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(cls_path):</span><br><span class="line">                self.img_paths.append(os.path.join(cls_path, img_name))</span><br><span class="line">                self.labels.append(idx)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        img_path = self.img_paths[idx]</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        label = self.labels[idx]</span><br><span class="line">        <span class="built_in">return</span> img, label</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        <span class="built_in">return</span> len(self.img_paths)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="简单CNN模型"><a href="#简单CNN模型" class="headerlink" title="简单CNN模型"></a>简单CNN模型</h4><p>CNN通常用PyTorch的nn.Conv1d和nn.Conv2d或者keras实现卷积操作</p>
<ul>
<li>Conv1d.<br>一维卷积，常用于处理时间序列，文本序列等。在构建时需要设定out_channels, kernel_size, stride, padding等参数<br>输入张量维度为（batch_size, channels, length）</li>
<li>Conv2d.<br>二维卷积，常用于图像处理。在构建时同样需要设定out_channels, kernel_size, stride, padding等参数<br>输入张量形状为(batch_size, channels, height, width),</li>
</ul>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>所以在model.summay()里，第一个维度显示为<strong>None</strong>, 这就是batch size可变的占位符<br></p>
</blockquote>
<hr>
<h5 id="输入数据格式"><a href="#输入数据格式" class="headerlink" title="输入数据格式"></a>输入数据格式</h5><ul>
<li>输入图像为彩色图像（RGB），尺寸统一为 256×256 像素。</li>
<li>输入张量形状为 <code>[batch_size, 3, 256, 256]</code>，其中 <code>3</code> 是通道数（RGB），<code>batch_size</code> 是批量大小。</li>
</ul>
<h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><h5 id="卷积层和池化层流程"><a href="#卷积层和池化层流程" class="headerlink" title="卷积层和池化层流程"></a>卷积层和池化层流程</h5><hr>
<blockquote>
<p><strong>Notice</strong><br><br>池化层使用的kernel_size通常与stride步长相等<br><br>池化层的目的是对特征图进行下采样（如最大池化、平均池化），减小特征图的宽高，从而减小后续层的参数量和计算量<br><br>保留主要信息，丢弃不重要的细节<br><br>通过信息压缩和参数减小，降低模型过拟合风险<br></p>
</blockquote>
<hr>
<ul>
<li><p><strong>第一层卷积</strong>: <code>nn.Conv2d(3, 16, kernel_size=3, padding=1)</code></p>
<ul>
<li>输入通道数：3</li>
<li>输出通道数：16</li>
<li>卷积核大小：3×3</li>
<li>填充大小：1，步长默认1</li>
<li>输出尺寸计算公式：<br>$$<br>\text{Output Size} &#x3D; \left\lfloor \frac{\text{Input Size} + 2 \times \text{padding} - \text{kernel size}}{\text{stride}} \right\rfloor + 1<br>代入参数：<br>\frac{256 + 2 \times 1 - 3}{1} + 1 &#x3D; 256<br>$$</li>
<li>输出张量形状：<code>[batch_size, 16, 256, 256]</code></li>
</ul>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>第一次池化</strong>: <code>nn.MaxPool2d(2)</code></p>
<ul>
<li>池化窗口大小：2×2，步长2</li>
<li>尺寸减半，输出形状为：<code>[batch_size, 16, 128, 128]</code></li>
</ul>
</li>
<li><p><strong>第二层卷积</strong>: <code>nn.Conv2d(16, 32, kernel_size=3, padding=1)</code></p>
<ul>
<li>输入通道数：16</li>
<li>输出通道数：32</li>
<li>卷积核大小：3×3，填充1，步长1</li>
<li>输出尺寸同理计算为：<code>128</code></li>
<li>输出张量形状：<code>[batch_size, 32, 128, 128]</code></li>
</ul>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>第二次池化</strong>: <code>nn.MaxPool2d(2)</code></p>
<ul>
<li>尺寸减半，输出形状为：<code>[batch_size, 32, 64, 64]</code></li>
</ul>
</li>
</ul>
<h5 id="全连接层流程。"><a href="#全连接层流程。" class="headerlink" title="全连接层流程。"></a>全连接层流程。</h5><hr>
<blockquote>
<p><strong>Notice</strong><br><br>中间层激活函数通常用ReLU，最后分类通常使用Sigmoid函数<br></p>
</blockquote>
<hr>
<ul>
<li><p><strong>展平层</strong>: 将卷积层输出展平成一维向量<br>Flatten是将卷积层中提取到的空间特征展开为一维向量。用于后续全连接层。</p>
<p>输入维度为：<br>$$<br>32 \times 64 \times 64 &#x3D; 131072<br>$$</p>
</li>
<li><p><strong>全连接层1</strong>: <code>nn.Linear(131072, 64)</code>  </p>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>全连接层2</strong>: <code>nn.Linear(64, 1)</code></p>
</li>
<li><p><strong>Sigmoid激活</strong>: 输出概率值，范围在[0,1]之间</p>
</li>
<li><p><strong>输出</strong>: 使用 <code>.squeeze(1)</code> 将输出维度从 <code>[batch_size, 1]</code> 转换为 <code>[batch_size]</code>，便于计算损失函数。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class SimpleCNN(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,16,3,padding = 1);</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(16,32,3,padding = 1);</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(32 * 64 * 64, 64),  <span class="comment"># 256-&gt;128-&gt;64，宽高减半两次</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(64, 1),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        def forward(self, x):</span><br><span class="line">            x = self.conv(x)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">            <span class="built_in">return</span> x.squeeze(1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="准备数据和模型"><a href="#准备数据和模型" class="headerlink" title="准备数据和模型"></a>准备数据和模型</h4><h5 id="train-validate-test数据集的区别"><a href="#train-validate-test数据集的区别" class="headerlink" title="train,validate,test数据集的区别"></a>train,validate,test数据集的区别</h5><table>
<thead>
<tr>
<th>集合</th>
<th>作用</th>
<th>使用阶段</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>训练集 (Train)</td>
<td>用于模型训练，更新模型参数</td>
<td>训练阶段</td>
<td>占比最大，数据多样性丰富</td>
</tr>
<tr>
<td>验证集 (Validation)</td>
<td>用于调参和模型选择，监控过拟合情况</td>
<td>训练过程中的评估阶段</td>
<td>不参与训练，用于调节超参数和模型结构</td>
</tr>
<tr>
<td>测试集 (Test)</td>
<td>用于评估最终模型性能</td>
<td>训练完成后的性能评估</td>
<td>完全独立，不参与训练和调参</td>
</tr>
</tbody></table>
<h5 id="train-validate-test数据集的划分"><a href="#train-validate-test数据集的划分" class="headerlink" title="train,validate,test数据集的划分"></a>train,validate,test数据集的划分</h5><ul>
<li><p>创建完整数据集<br>dataset &#x3D; MyData(root_dir, classes, transform&#x3D;transform)</p>
</li>
<li><p>按比例划分训练、验证、测试集 70%, 15%, 15%<br>total_size &#x3D; len(dataset)<br>train_size &#x3D; int(0.7 * total_size)<br>val_size &#x3D; int(0.15 * total_size)<br>test_size &#x3D; total_size - train_size - val_size</p>
</li>
</ul>
<p>train_dataset, val_dataset, test_dataset &#x3D; random_split(dataset, [train_size, val_size, test_size])</p>
<ul>
<li>创建DataLoader<br>train_loader &#x3D; DataLoader(train_dataset, batch_size&#x3D;64, shuffle&#x3D;True)<br>val_loader &#x3D; DataLoader(val_dataset, batch_size&#x3D;64, shuffle&#x3D;False)<br>test_loader &#x3D; DataLoader(test_dataset, batch_size&#x3D;64, shuffle&#x3D;False)</li>
</ul>
<h4 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = 10</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = 0.0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">        x, y = x.to(device), y.float().to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item() * x.size(0)</span><br><span class="line"></span><br><span class="line">    epoch_loss = running_loss / len(dataset)</span><br><span class="line">    <span class="built_in">print</span>(f<span class="string">&quot;Epoch &#123;epoch+1&#125;/&#123;n_epochs&#125; - Loss: &#123;epoch_loss:.4f&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成！&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="CNN-ResNet-Residual-Network"><a href="#CNN-ResNet-Residual-Network" class="headerlink" title="CNN + ResNet (Residual Network)"></a>CNN + ResNet (Residual Network)</h3><p>残差结构:<br>$y &#x3D; F(x) + x$<br>解决了深层网络梯度消失、退化问题<br>RestNet还会应用在Attention&#x2F;Transformer中，例如在Transformer中大量使用了残差连接+LayerNorm，在Attention中，每个Self-Attention层、Feed Forward层外面都有一个残差。没有残差，Transformer就很难训练深层结构。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import functional as F</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class Residual(nn.Module):  </span><br><span class="line">    def __init__(self, input_channels, num_channels,</span><br><span class="line">                 use_1x1conv=False, strides=1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=3, padding=1, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=3, padding=1)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=1, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = None</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"> </span><br><span class="line">    def forward(self, X):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="built_in">return</span> F.relu(Y)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这段代码实现了RestNet的残差块:</p>
<ul>
<li>两个3x3卷积+BN</li>
<li>可选1x1卷积来匹配通道或大小</li>
<li>残差连接Y + X</li>
<li>最后经过ReLU</li>
</ul>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>1x1的卷积层通常用来调整输出的通道维度，因为1x1卷积不会局部空间特征。<br></p>
</blockquote>
<hr>
<h3 id="膨胀卷积（Dilated-Convolution）"><a href="#膨胀卷积（Dilated-Convolution）" class="headerlink" title="膨胀卷积（Dilated Convolution）"></a>膨胀卷积（Dilated Convolution）</h3><p>既可以增大感受野，又能保持输入特征图的W和H不变（通过padding和stride配合，保证输入特征图的尺寸）<br>原因：<br>在原CNN网络中，使用Maxpooling层就是通过池化操作使得后续的卷积层获得更大的感受野，但是这样会带来两个问题：</p>
<ul>
<li>池化操作会丢失一些微小信息</li>
<li>池化操作会使得下采样倍率进一步变大</li>
</ul>
<h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p>$y[i] &#x3D; \sum_{k&#x3D;0}^{K-1} w[k] * x[i + r * k] + b$. </p>
<ul>
<li>r 是 dilation rate（膨胀率）</li>
<li>当 r &#x3D; 1时，退化为普通卷积</li>
<li>当 r &gt; 1时，相当于在输入上跳步采样，扩大感受野</li>
</ul>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>在分类中，需要讲线性输出映射到概率</p>
<h4 id="Sigmoid（二分类）"><a href="#Sigmoid（二分类）" class="headerlink" title="Sigmoid（二分类）"></a>Sigmoid（二分类）</h4><p>模型：<br>$p &#x3D; \sigma(z) &#x3D; \frac{1}{1 + e^{-z}},\quad z &#x3D; b + \sum_j w_j x_j$.<br>预测类别：<br>$\hat{y} &#x3D;\begin{cases} 1 &amp; \text{if } p &gt; 0.5 \ 0 &amp; \text{otherwise} \end{cases}$.<br>Binary Cross-Entropy Loss (BCL):<br>$L(b, w) &#x3D; -\frac{1}{N} \sum_{n&#x3D;1}^{N} [, y_n \log p_n + (1 - y_n) \log (1 - p_n)$</p>
<h4 id="Softmax（多分类）"><a href="#Softmax（多分类）" class="headerlink" title="Softmax（多分类）"></a>Softmax（多分类）</h4><p>对于$K$个类别<br>$p_k &#x3D; \frac{e^{z_k}}{\sum_{j&#x3D;1}^{K} e^{z_j}},\quad z_k &#x3D; b_k + \sum_j w_{kj} x_j \quad k&#x3D;1 \dots,K$.<br>Multiclass Cross-Entropy Loss:<br>$L(b, w) &#x3D; -\frac{1}{N} \sum_{n&#x3D;1}^{N} \sum_{k&#x3D;1}^{K} y_{nk} \log p_{nk}$。<br>其中：<br>$y_{nk} &#x3D; 1$ if sample $n$ is of class $k$, otherwise $0$.<br>因此这里用了one-hot编码，因为只考虑了只有真实类别对应的$y_{nk} &#x3D; 1$,其余为0，$p_{nk}$是Softmax后预测属于类别k的概率</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><h4 id="Linear-Regression-vs-Sigmoid-Regression"><a href="#Linear-Regression-vs-Sigmoid-Regression" class="headerlink" title="Linear Regression vs Sigmoid Regression"></a>Linear Regression vs Sigmoid Regression</h4><table>
<thead>
<tr>
<th>Task</th>
<th>Output Function</th>
<th>Activation</th>
<th>Loss Function</th>
</tr>
</thead>
<tbody><tr>
<td>Regression</td>
<td>$y$</td>
<td>Linear</td>
<td>MAE &#x2F; MSE</td>
</tr>
<tr>
<td>Binary Classification</td>
<td>$p$</td>
<td>Sigmoid</td>
<td>Binary Cross-Entropy (BCE)</td>
</tr>
<tr>
<td>Multiclass Classification</td>
<td>$p_k$</td>
<td>Softmax</td>
<td>Multiclass Cross-Entropy</td>
</tr>
</tbody></table>
<h4 id="MSE-vs-Binary-Cross-Entropy"><a href="#MSE-vs-Binary-Cross-Entropy" class="headerlink" title="MSE vs Binary Cross-Entropy"></a>MSE vs Binary Cross-Entropy</h4><table>
<thead>
<tr>
<th></th>
<th>Linear Regression</th>
<th>Logistic Regression</th>
</tr>
</thead>
<tbody><tr>
<td>🎯 任务</td>
<td>预测连续值（回归）</td>
<td>预测二分类（分类）</td>
</tr>
<tr>
<td>📈 模型</td>
<td>$y_hat &#x3D; b + Σ w_j * x_j$</td>
<td>$p &#x3D; σ(z) &#x3D; 1&#x2F;(1+exp(-z)), z &#x3D; b + Σ w_j * x_j$</td>
</tr>
<tr>
<td>📊 输出</td>
<td>实数 (-∞, +∞)</td>
<td>概率 [0, 1]</td>
</tr>
<tr>
<td>📚 分布</td>
<td>假设输出服从高斯分布</td>
<td>假设输出服从伯努利分布</td>
</tr>
<tr>
<td>🔗 损失</td>
<td>均方误差 (MSE: Mean Squared Error)</td>
<td>二元交叉熵 (BCE: Binary Cross-Entropy)</td>
</tr>
<tr>
<td>⚡ 目标</td>
<td>最大化高斯似然</td>
<td>最大化伯努利似然</td>
</tr>
<tr>
<td>⚙️ 梯度</td>
<td>梯度线性，更新相对平滑</td>
<td>梯度在非饱和区间较大，下降快，但接近 0&#x2F;1 时可能饱和</td>
</tr>
<tr>
<td>总体来说，Cross-Entropy效果更好一点</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p><strong>Support Vector Machine(SVM)</strong> 是一种常见的监督学习方法，常用于二分类任务，核心思想是通过寻找一个最优的分割超平面，使得不同类别之间的间隔（margin）最大化。</p>
<h4 id="Linear-Model-1"><a href="#Linear-Model-1" class="headerlink" title="Linear Model"></a>Linear Model</h4><p>对于线性可分数据，SVM定义一个线性决策函数：<br>$$<br>f(x) &#x3D; w^T x + b<br>$$</p>
<p>其中：</p>
<ul>
<li>$w$ 是权重向量，</li>
<li>$b$ 是偏置项。</li>
</ul>
<p>分类规则：</p>
<p>$$<br>\hat{y} &#x3D; \text{sign}(f(x)) &#x3D;<br>\begin{cases}<br>+1 &amp; \text{if } f(x) \ge 0 \\<br>-1 &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</p>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>SVM 常用 <strong>合页损失（Hinge Loss）</strong>：</p>
<p>$$<br>L_n &#x3D; \max(0,, 1 - y_n f(x_n))<br>$$</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>如果$y_nf(x_n)&gt;0$则说明分类正确<br><br>但是,$y_nf(x_n)&gt;&#x3D;1$才完美，因为SVM不仅要求分类正确，还要最大化间隔，也就是安全间隔，所以在合页损失中有1<br></p>
</blockquote>
<hr>
<p>整体目标函数：</p>
<h2 id="L-w-b-frac-1-2-w-2-C-sum-n-1-N-max-0-1-y-n-w-T-x-n-b"><a href="#L-w-b-frac-1-2-w-2-C-sum-n-1-N-max-0-1-y-n-w-T-x-n-b" class="headerlink" title="$$L(w,b) &#x3D; \frac{1}{2} |w|^2 + C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b)).$$"></a>$$<br>L(w,b) &#x3D; \frac{1}{2} |w|^2 + C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b)).<br>$$</h2><blockquote>
<p><strong>Notice</strong><br><br>$\frac{1}{2}|w|^2$是正则化项，用于控制模型的复杂度，防止过拟合，权重不要太大。<br><br>$C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b))$是所有样本的合页损失的加权和。<br></p>
</blockquote>
<hr>
<h4 id="Optimization-Gradient-Descent"><a href="#Optimization-Gradient-Descent" class="headerlink" title="Optimization (Gradient Descent)"></a>Optimization (Gradient Descent)</h4><p>合页损失可以用梯度下降或随机梯度下降（SGD）优化：</p>
<p>$$<br>w \leftarrow w - \eta (w - C \sum_{n \in M} y_n x_n),<br>\quad<br>M &#x3D; { n ,|, 1 - y_n f(x_n) &gt; 0 }<br>$$</p>
<p>其中：</p>
<ul>
<li>$\eta$ 是学习率，</li>
<li>$M$ 是当前违反 margin 条件的样本集合。</li>
</ul>
<h3 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h3><p>利用贝叶斯公式根据某特征的先验概率计算出其后验概率，然后选择具有最大后验概率的类作为该特征所属的类</p>
<h4 id="条件概率。"><a href="#条件概率。" class="headerlink" title="条件概率。"></a>条件概率。</h4><p>就是在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示<br>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<h4 id="全概率"><a href="#全概率" class="headerlink" title="全概率"></a>全概率</h4><p>如果事件A1,A2,…,An构成一个完备事件都有正概率，那么对于任意一个事件B则有：<br>$P(B)&#x3D;\sum_{i&#x3D;1}^{n}P(A_i)P(B|A_i)$</p>
<h4 id="贝叶斯推断"><a href="#贝叶斯推断" class="headerlink" title="贝叶斯推断"></a>贝叶斯推断</h4><p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$ </p>
<p>$P(A_i|B) &#x3D; P(A_i)\frac{P(B|A_i)}{\sum_{i&#x3D;1}^{n}P(A_i)P(B|A_i)}$<br>其中P(A)为先验概率，$P(A|B)$为后验概率，$\frac{P(B|A)}{P(B)}$为可能性函数（Likely hood）<br>转换成分类任务的表达式：<br>$P(类别|特征) &#x3D; P(类别)\frac{P(特征|类别)}{P(特征)}$。</p>
<h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>$P(帅 性格不好 不上进) &#x3D; P(嫁)P(帅|嫁)P(性格不好|嫁)P(不上进｜嫁) +  P(不嫁)P(帅|不嫁)P(性格不好|不嫁)P(不上进｜不嫁) &#x3D; 2&#x2F;125 + 3&#x2F;125$  </p>
<p>最终计算结果：<br>P(嫁|帅 性格不好 不上进) &#x3D; P(嫁)P(帅|嫁)P(性格不好|嫁)P(不上进｜嫁) &#x2F; P(帅 性格不好 不上进) &#x3D; (2&#x2F;125) &#x2F; (2&#x2F;125 + 3&#x2F;125) &#x3D; 0.4.<br>P(嫁|帅 性格不好 不上进) &#x3D; (3&#x2F;125) &#x2F; (2&#x2F;125 + 3&#x2F;125) &#x3D; 0.6. </p>
<h4 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h4><h4 id="最大似然估计（MLE）及-GaussianNB-示例"><a href="#最大似然估计（MLE）及-GaussianNB-示例" class="headerlink" title="最大似然估计（MLE）及 GaussianNB 示例"></a>最大似然估计（MLE）及 GaussianNB 示例</h4><hr>
<p>观测数据：<br>$$<br>X &#x3D; {x_1, x_2, \ldots, x_n}<br>$$</p>
<p>参数：<br>$$<br>\theta<br>$$</p>
<p>那么数据的似然函数是：<br>$$<br>L(\theta; X) &#x3D; P(X \mid \theta)<br>$$</p>
<p>最大似然估计就是：<br>$$<br>\hat{\theta}<em>{MLE} &#x3D; \arg\max</em>{\theta} L(\theta; X)<br>$$</p>
<hr>
<p>🔬 <strong>用在 GaussianNB 的例子</strong>  </p>
<p>在高斯朴素贝叶斯中，假设每个特征<br>$$<br>X_j<br>$$<br>在类别<br>$$<br>C_k<br>$$<br>下服从正态分布。</p>
<p>那么对应参数的估计为：</p>
<ul>
<li><p>均值：<br>$$<br>\mu_{jk} &#x3D; \frac{1}{n_k} \sum_{i&#x3D;1}^{n_k} x_j^{(i)}<br>$$</p>
</li>
<li><p>方差：<br>$$<br>\sigma_{jk}^2 &#x3D; \frac{1}{n_k} \sum_{i&#x3D;1}^{n_k} \left( x_j^{(i)} - \mu_{jk} \right)^2<br>$$</p>
</li>
</ul>
<p>其中，  </p>
<ul>
<li>$n_k$ 是类别 $C_k$ 的样本数，  </li>
<li>$x_j^{(i)}$ 表示类别 $C_k$ 中第 i 个样本的第 j 个特征值。</li>
</ul>
<hr>
<h4 id="朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）"><a href="#朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）" class="headerlink" title="朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）"></a>朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）</h4><hr>
<p>给定类别集合 $A_1, A_2, \ldots, A_n$ 和观察到的特征向量 $B &#x3D; (x_1, x_2, \ldots, x_d)$，<br>后验概率计算公式为：</p>
<p>$$<br>P(A_i \mid B) &#x3D; \frac{P(A_i) , P(B \mid A_i)}{\sum_{j&#x3D;1}^n P(A_j) , P(B \mid A_j)}<br>$$</p>
<hr>
<p>在高斯朴素贝叶斯中，假设特征条件独立且服从高斯分布，<br>则类别 $A_i$ 下特征 $x_k$ 的条件概率为：</p>
<p>$$<br>P(x_k \mid A_i) &#x3D; \frac{1}{\sqrt{2\pi \sigma_{ik}^2}} \exp\left( -\frac{(x_k - \mu_{ik})^2}{2\sigma_{ik}^2} \right)<br>$$</p>
<p>其中：  </p>
<ul>
<li>$\mu_{ik}$ 和 $\sigma_{ik}^2$ 是类别 $A_i$ 下第 k 个特征的均值和方差。</li>
</ul>
<hr>
<p>根据特征独立性假设，整体似然为：</p>
<p>$$<br>P(B \mid A_i) &#x3D; \prod_{k&#x3D;1}^d P(x_k \mid A_i)<br>$$</p>
<hr>
<p>综上，后验概率计算步骤为：</p>
<ol>
<li>计算先验概率 $P(A_i)$，一般为类别样本比例；</li>
<li>利用高斯分布计算每个特征的条件概率 $P(x_k \mid A_i)$；</li>
<li>计算似然 $P(B \mid A_i) &#x3D; \prod_{k&#x3D;1}^d P(x_k \mid A_i)$；</li>
<li>代入贝叶斯公式求得后验概率：</li>
</ol>
<p>$$<br>P(A_i \mid B) &#x3D; \frac{P(A_i) \prod_{k&#x3D;1}^d P(x_k \mid A_i)}{\sum_{j&#x3D;1}^n P(A_j) \prod_{k&#x3D;1}^d P(x_k \mid A_j)}<br>$$</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>在 高斯朴素贝叶斯（Gaussian Naive Bayes） 中，每个类别（target）都会单独拟合一个高斯分布，而且是针对每个特征分别计算。<br><br>在同一个类别（target）下，多个特征是条件独立的（<strong>朴素贝叶斯的前提</strong>,便于求得<strong>联合概率</strong>）<br></p>
</blockquote>
<hr>
<h3 id="Discriminative-Model-vs-Generative-Model"><a href="#Discriminative-Model-vs-Generative-Model" class="headerlink" title="Discriminative Model vs Generative Model"></a>Discriminative Model vs Generative Model</h3><h4 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h4><table>
<thead>
<tr>
<th></th>
<th><strong>Discriminative Model (判别式模型)</strong></th>
<th><strong>Generative Model (生成式模型)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心思想</strong></td>
<td>直接学习 <strong>后验分布</strong> $p(y|x)$</td>
<td>先学习 <strong>联合分布</strong> $p(x,y)$，或条件分布 $p(x|y)$ 和先验 $p(y)$</td>
</tr>
<tr>
<td><strong>关键任务</strong></td>
<td>找到最好的决策边界，分清类别</td>
<td>理解数据是如何生成的，可用于生成新样本</td>
</tr>
<tr>
<td><strong>举个例子</strong></td>
<td>Logistic Regression, SVM, Neural Network</td>
<td>Naive Bayes, GMM, HMM, GAN, VAE</td>
</tr>
</tbody></table>
<h4 id="公式对比"><a href="#公式对比" class="headerlink" title="公式对比"></a>公式对比</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心建模</strong></td>
<td>$p(y|x)$</td>
<td>$p(x,y) &#x3D; p(x|y) p(y)$</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>最大化条件似然：$\max_\theta p(y|x)$</td>
<td>最大化联合似然：$\max_\theta p(x,y)$</td>
</tr>
</tbody></table>
<hr>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>优点</strong></td>
<td>- 不需要建模输入分布，简单高效  <br> - 一般分类准确率更高</td>
<td>- 可生成新数据  <br> - 可处理缺失值  <br> - 对小样本问题有优势</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>- 无法生成数据  <br> - 对缺失值敏感</td>
<td>- 对输入分布需要假设  <br> - 特征条件独立假设可能太强</td>
</tr>
</tbody></table>
<hr>
<h4 id="常见示例"><a href="#常见示例" class="headerlink" title="常见示例"></a>常见示例</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>线性模型</strong></td>
<td>Logistic Regression</td>
<td>Naive Bayes (Gaussian&#x2F;Multinomial)</td>
</tr>
<tr>
<td><strong>非线性</strong></td>
<td>Neural Network, SVM</td>
<td>GMM, HMM, GAN, VAE</td>
</tr>
</tbody></table>
<h3 id="Optimization-1"><a href="#Optimization-1" class="headerlink" title="Optimization"></a>Optimization</h3><h4 id="使用Hessian分析Saddle-Point和Local-Minimal"><a href="#使用Hessian分析Saddle-Point和Local-Minimal" class="headerlink" title="使用Hessian分析Saddle Point和Local Minimal"></a>使用Hessian分析Saddle Point和Local Minimal</h4><p>At critical point: $L(\theta) &#x3D; L(\theta_0) + \frac{1}{2}(\theta - \theta_0)^TH(\theta - \theta_0)$.<br>我们把u(an eigen vector of H)当作$\theta - \theta_0$,$\lambda$为eigen value. </p>
<p>那么$u^T H u &#x3D; u^T(\lambda u) &#x3D; \lambda|u|^2 $.<br>因此$\lambda$正负会直接L的increase or decrease<br>根据这个公式可知解决saddle point问题步骤如下：  </p>
<ul>
<li>找到H</li>
<li>找到H的某个负特征值</li>
<li>对应的特征向量u就是想找的下降方向</li>
<li>$\theta &#x3D; \theta_0 + u$</li>
</ul>
<h4 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h4><h5 id="Root-Mean-Square"><a href="#Root-Mean-Square" class="headerlink" title="Root Mean Square"></a>Root Mean Square</h5><p><strong>定义：</strong><br>Root Mean Square（RMS）是指一组数平方后求平均再开方，反映平均“能量”大小。</p>
<p>公式如下：</p>
<p>$\sigma_i^t &#x3D; \sqrt{\frac{1}{t+1}\sum_{i&#x3D;0}^t (g_i^t)^2}$.  </p>
<p>$\theta_i^{t+1} &#x3D; \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t$</p>
<p>在优化中，用于衡量梯度大小，作为学习率调整的依据。</p>
<h5 id="RMSProp-Root-Mean-Square-Propagation"><a href="#RMSProp-Root-Mean-Square-Propagation" class="headerlink" title="RMSProp (Root Mean Square Propagation)"></a>RMSProp (Root Mean Square Propagation)</h5><ul>
<li>梯度平方滑动平均：</li>
</ul>
<p>$\sigma_i^t &#x3D; \sqrt{\alpha (\sigma_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}$</p>
<ul>
<li>参数更新：</li>
</ul>
<p>$θ_i^{t+1} &#x3D; θ_i^t - \frac{\eta}{\sigma_i^t}g_i^t$</p>
<p>其中：</p>
<ul>
<li>ρ 是衰减率（常用 0.9）</li>
<li>η 是基础学习率</li>
<li>ε 是防止除以 0 的小常数（如 1e-8）</li>
</ul>
<h5 id="直觉理解："><a href="#直觉理解：" class="headerlink" title="直觉理解："></a>直觉理解：</h5><ul>
<li>梯度大的参数方向：步长缩小，避免震荡</li>
<li>梯度小的参数方向：步长放大，避免停滞</li>
<li>RMSProp 根据每个参数的历史梯度自动调整学习率，实现更稳定的收敛过程</li>
</ul>
<h4 id="总结对比"><a href="#总结对比" class="headerlink" title="总结对比"></a>总结对比</h4><table>
<thead>
<tr>
<th>概念</th>
<th>含义 &#x2F; 作用</th>
<th>与 RMSProp 的关系</th>
</tr>
</thead>
<tbody><tr>
<td>Adaptive Learning Rate</td>
<td>每个参数独立学习率，动态调整</td>
<td>RMSProp 是其代表性实现方法</td>
</tr>
<tr>
<td>Root Mean Square</td>
<td>用于估计梯度平均能量，控制步长</td>
<td>被 RMSProp 用于缩放学习率</td>
</tr>
<tr>
<td>RMSProp</td>
<td>自适应优化算法，防止学习率过早衰减</td>
<td>实现了 adaptive learning rate</td>
</tr>
</tbody></table>
<h5 id="Learning-Rate-Scheduling"><a href="#Learning-Rate-Scheduling" class="headerlink" title="Learning Rate Scheduling"></a>Learning Rate Scheduling</h5><h4 id="Batch-和-Momentum简要说明"><a href="#Batch-和-Momentum简要说明" class="headerlink" title="Batch 和 Momentum简要说明"></a>Batch 和 Momentum简要说明</h4><h5 id="Batch（小批量）"><a href="#Batch（小批量）" class="headerlink" title="Batch（小批量）"></a>Batch（小批量）</h5><p>在训练神经网络时，我们通常不会每次使用整个训练集来更新参数，而是将数据划分为多个“小批量”，每个 batch 包含若干样本。</p>
<p>例如：</p>
<ul>
<li>数据总量为 10,000，batch size &#x3D; 32</li>
<li>则每个 epoch 有约 312 次更新</li>
</ul>
<h6 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h6><ul>
<li>提高训练效率（相比使用整个数据集）</li>
<li>提供更稳定的梯度估计（相比单个样本）</li>
<li>有利于 GPU 并行计算</li>
</ul>
<h6 id="直觉："><a href="#直觉：" class="headerlink" title="直觉："></a>直觉：</h6><ul>
<li>小 batch：训练快但不稳定，梯度噪声大</li>
<li>大 batch：训练稳定但计算开销大，收敛可能慢</li>
</ul>
<h5 id="Momentum（动量）"><a href="#Momentum（动量）" class="headerlink" title="Momentum（动量）"></a>Momentum（动量）</h5><p>Momentum 是优化器的一种加速技巧，通过引入“动量”，在梯度方向上积累速度。</p>
<p>常规梯度下降公式：</p>
<pre><code>$θ_&#123;t+1&#125; = θ_t - η ∇L(θ_t)$
</code></pre>
<p>加入动量后的更新规则：</p>
<pre><code>$v_&#123;t+1&#125; = γ v_t - η ∇L(θ_t)$
$θ_&#123;t+1&#125; = θ_t + v_&#123;t+1&#125;$
</code></pre>
<p>其中：</p>
<ul>
<li>η 是学习率</li>
<li>γ 是动量系数（如 0.9）</li>
<li>v 是“速度向量”，表示历史梯度累积效果</li>
</ul>
<h6 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h6><ul>
<li>加快收敛速度</li>
<li>减少梯度震荡</li>
<li>更容易跳出局部最小值</li>
</ul>
<h6 id="直觉：-1"><a href="#直觉：-1" class="headerlink" title="直觉："></a>直觉：</h6><p>像滚动的小球有惯性，动量让优化器在梯度一致的方向上加速前进，在梯度变化剧烈的方向上稳定下来。</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><table>
<thead>
<tr>
<th>概念</th>
<th>定义</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>Batch</td>
<td>一次更新中使用的样本子集</td>
<td>提高效率、平滑梯度估计</td>
</tr>
<tr>
<td>Momentum</td>
<td>使用历史梯度加速更新</td>
<td>加速收敛、减少震荡、跳出局部最小值</td>
</tr>
</tbody></table>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>无论是在图像识别，音频处理等应用中，输入都可以看作是一个向量，输出是一个数值或类别。然而，若输入是<strong>一系列向量</strong>,同时长度会改变，例如把句子里的单词都描述为向量，那么模型的输入就是一个向量集合，并且每个向量的大小都不一样。<br>对于这个一系列向量，或者说是向量集合，我们通常会考虑上下文，因此我们会引入滑动窗口机制，每个向量查看窗口中相邻的其他向量的性质。为了量化地计算向量之间的相关性，我们引入了Self-attention技术。</p>
<h4 id="Basic-Concept-1"><a href="#Basic-Concept-1" class="headerlink" title="Basic Concept"></a>Basic Concept</h4><p>在注意力机制中，主要涉及三个向量：Query、Key 和 Value。</p>
<p>给定一个输入向量集合 ${a^1, a^2, \dots, a^n}$，我们对每个向量 $a^i$ 进行线性变换以获得对应的 Query、Key 和 Value 向量。</p>
<h4 id="向量计算"><a href="#向量计算" class="headerlink" title="向量计算"></a>向量计算</h4><ul>
<li>Query 向量：$q^i &#x3D; a^i W^q$</li>
<li>Key 向量：$k^i &#x3D; a^i W^k$</li>
<li>Value 向量：$v^i &#x3D; a^i W^v$</li>
</ul>
<p>其中，$W^q$、$W^k$、$W^v$ 是可学习的参数矩阵，维度通常为：</p>
<ul>
<li>$W^q \in \mathbb{R}^{d_{model} \times d_k}$</li>
<li>$W^k \in \mathbb{R}^{d_{model} \times d_k}$</li>
<li>$W^v \in \mathbb{R}^{d_{model} \times d_v}$</li>
</ul>
<h4 id="注意力打分机制"><a href="#注意力打分机制" class="headerlink" title="注意力打分机制"></a>注意力打分机制</h4><p>对于一个查询向量 $q^i$，其对应的注意力权重是通过它与所有 Key 向量的点积计算得到的：</p>
<p>$$<br>\text{score}_{ij} &#x3D; \frac{q^i \cdot (k^j)^T}{\sqrt{d_k}}<br>$$</p>
<p>然后使用 Softmax 函数对所有得分进行归一化，得到注意力分布：</p>
<p>$$<br>\alpha_{ij} &#x3D; \text{softmax}(\text{score}<em>{ij}) &#x3D; \frac{\exp(\text{score}</em>{ij})}{\sum_{j&#x3D;1}^{n} \exp(\text{score}_{ij})}<br>$$</p>
<h4 id="加权求和得到输出"><a href="#加权求和得到输出" class="headerlink" title="加权求和得到输出"></a>加权求和得到输出</h4><p>最后，使用这些注意力权重对对应的 Value 向量加权求和，得到最终的注意力输出向量：</p>
<p>$$<br>z^i &#x3D; \sum_{j&#x3D;1}^{n} \alpha_{ij} v^j<br>$$</p>
<p>这个过程可以表示为矩阵形式：</p>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V<br>$$</p>
<hr>
<h4 id="总结流程："><a href="#总结流程：" class="headerlink" title="总结流程："></a>总结流程：</h4><ol>
<li>输入向量集 $a^i$</li>
<li>计算 Query: $q^i &#x3D; a^i W^q$</li>
<li>计算 Key: $k^i &#x3D; a^i W^k$</li>
<li>计算 Value: $v^i &#x3D; a^i W^v$</li>
<li>计算注意力得分（点积）：$q^i \cdot (k^j)^T$</li>
<li>通过 Softmax 得到注意力权重</li>
<li>对 Value 加权求和得到输出 $z^i$</li>
</ol>
<hr>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li>$d_k$ 是 Key 向量的维度</li>
<li>$d_v$ 是 Value 向量的维度</li>
<li>注意力机制允许模型动态地聚焦于输入序列中最相关的部分</li>
</ul>
<h4 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h4><p>In <strong>local attention</strong>, each position only attends to a limited number of nearby positions (i.e., a fixed-size window). It mimics sliding-window machanims.<br><strong>Example (window size &#x3D; 2):</strong><br>$$<br>a_i attends to {a_{i-2}, a_{i-1}, a_i, a_{i+1}, a_{i+2}}<br>$$</p>
<h5 id="Pros"><a href="#Pros" class="headerlink" title="Pros:"></a>Pros:</h5><ul>
<li>Reduces computation to 0(nw), where w is the window size</li>
<li>Works well in spatial&#x2F;temporal data (e.g., images, audio)</li>
</ul>
<h4 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h4><p>In <strong>global attention</strong>, each query attends to <strong>all positions</strong> in the sequence.</p>
<h5 id="Pros-1"><a href="#Pros-1" class="headerlink" title="Pros:"></a>Pros:</h5><ul>
<li>Captures long-range dependencies</li>
<li>Powerful and general-purpose</li>
</ul>
<h5 id="Cons"><a href="#Cons" class="headerlink" title="Cons:"></a>Cons:</h5><ul>
<li>High computational cost</li>
</ul>
<h4 id="Clustering-based-Attention"><a href="#Clustering-based-Attention" class="headerlink" title="Clustering-based Attention"></a>Clustering-based Attention</h4><h3 id="RNN-Recurrent-Neural-Network"><a href="#RNN-Recurrent-Neural-Network" class="headerlink" title="RNN (Recurrent Neural Network)"></a>RNN (Recurrent Neural Network)</h3><h4 id="Long-Short-Term-Memory-LSTM-Cell-Explanation"><a href="#Long-Short-Term-Memory-LSTM-Cell-Explanation" class="headerlink" title="Long Short-Term Memory (LSTM) Cell Explanation"></a>Long Short-Term Memory (LSTM) Cell Explanation</h4><p>一个LSTM cell可以理解为一个cnn中的neuron<br>This project contains visual illustrations and numerical examples of how an LSTM (Long Short-Term Memory) cell works internally. The LSTM is a type of recurrent neural network (RNN) that is capable of learning long-term dependencies and solving the vanishing gradient problem.</p>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>LSTMs introduce <strong>memory cells</strong> and <strong>gating mechanisms</strong> to control the flow of information. The key gates in an LSTM cell are:</p>
<ul>
<li><p><strong>遗忘门（Forget Gate）</strong><br>控制保留多少过去的记忆。<br>计算方式：$f(z_f)$，使用 sigmoid 激活函数，输出范围在 $[0, 1]$，模拟“开”与“关”。</p>
</li>
<li><p><strong>输入门（Input Gate）</strong><br>决定当前输入中保留哪些信息。<br>表达式为：$f(z_i)$ 和 $g(z)$。</p>
</li>
<li><p><strong>记忆单元状态（Cell State）</strong><br>状态更新公式：<br>$$<br>C_{\text{new}} &#x3D; f(z_f) \cdot C_{\text{old}} + f(z_i) \cdot g(z)<br>$$</p>
</li>
<li><p><strong>输出门（Output Gate）</strong><br>决定当前时刻的隐藏状态输出 $a$：<br>$$<br>a &#x3D; f(z_o) \cdot h(C)<br>$$</p>
</li>
</ul>
<p>激活函数说明：</p>
<ul>
<li>$f(\cdot)$ 表示 sigmoid（$\sigma$ 函数），控制门的开关；</li>
<li>$g(\cdot)$ 表示 tanh，处理输入的候选记忆；</li>
<li>$h(\cdot)$ 通常也是 tanh，用于输出处理。</li>
</ul>
<hr>
<h4 id="LSTM-数值计算示例"><a href="#LSTM-数值计算示例" class="headerlink" title="LSTM 数值计算示例"></a>LSTM 数值计算示例</h4><img src="/images/LSTM_Formulation.png">

<p>该图展示了一个完整的 <strong>LSTM 单元的前向传播数值过程</strong>，主要特征包括：</p>
<ul>
<li>输入特征向量 $x_1$, $x_2$, $x_3$ 及偏置项；</li>
<li>不同门控的权重矩阵、偏置与线性组合；</li>
<li>显式显示了输入门、遗忘门、输出门的计算过程；</li>
<li>Cell 的状态值如何随着时间变化更新；</li>
<li>最终输出 $y$ 序列中在特定时间步激活（例如输出为 7）。</li>
</ul>
<p>通过可视化的矩阵计算，可以看到 LSTM 如何“记住”或“忘记”输入中的关键信息。</p>
<h5 id="关键计算公式"><a href="#关键计算公式" class="headerlink" title="关键计算公式"></a>关键计算公式</h5><p>$$<br>\begin{aligned}<br>f_t &amp;&#x3D; \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\<br>i_t &amp;&#x3D; \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\<br>g_t &amp;&#x3D; \tanh(W_g \cdot [h_{t-1}, x_t] + b_g) \\<br>C_t &amp;&#x3D; f_t \cdot C_{t-1} + i_t \cdot g_t \\<br>o_t &amp;&#x3D; \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\<br>h_t &amp;&#x3D; o_t \cdot \tanh(C_t)<br>\end{aligned}<br>$$<br>由此可知$W_f, W_i, W_g, W_o$的维度是(hidden_size, input_dim + hidder_size),因为输入是由$h_{t-1}$和$x_t$连接而成。<br>记号解释:</p>
<ul>
<li><p>隐藏状态向量<br>$h_{t-1} ∈ R^{64}$. </p>
<p>上一步的隐藏状态，维度为 64（对应 64 个 hidden units 或 memory cells）。</p>
</li>
<li><p>当前时间步输入向量<br> $x_t ∈ R^{d}$</p>
<p>当前时间步的输入，维度为输入特征数 d。</p>
</li>
<li><p>权重矩阵<br>$W_f, W_i, W_g, W_o ∈ R^{(64 + d) × 64}$<br>四个门（遗忘门、输入门、候选状态、输出门）的权重矩阵，输入是拼接的 $[h_{t-1}, x_t]$，输出是大小为 64 的门控向量。</p>
</li>
<li><p>偏置向量<br>$b_f, b_i, b_g, b_o ∈ R^{64}$. </p>
<p>四个门对应的偏置项，维度均为 64。</p>
</li>
</ul>
<h4 id="LSTM的模块说明以及应用场景"><a href="#LSTM的模块说明以及应用场景" class="headerlink" title="LSTM的模块说明以及应用场景"></a>LSTM的模块说明以及应用场景</h4><table>
<thead>
<tr>
<th>模块</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>遗忘门</td>
<td>学习去“忘记”无关记忆</td>
</tr>
<tr>
<td>输入门</td>
<td>学习将当前输入写入记忆</td>
</tr>
<tr>
<td>候选记忆块</td>
<td>生成当前输入的表示</td>
</tr>
<tr>
<td>Cell 状态</td>
<td>长期记忆存储通道</td>
</tr>
<tr>
<td>输出门</td>
<td>控制当前输出暴露多少内部记忆</td>
</tr>
</tbody></table>
<p>LSTM 网络适用于处理<strong>时间相关性较强</strong>的任务，例如：</p>
<ul>
<li>🧠 自然语言处理（NLP）：文本分类、情感分析、语言建模</li>
<li>📈 时间序列预测：股价、传感器数据、气象数据</li>
<li>🗣 语音识别、语音生成</li>
<li>🔄 序列到序列（Seq2Seq）：机器翻译、摘要生成等</li>
</ul>
<h4 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h4><p>Keras 的 LSTM 层接受的输入形状一般是 (batch_size, timesteps, input_dim)，<br>其中 timesteps（序列长度）可以是动态的，或者在模型构建时隐含定义。<br>如果你的 next_layer_in 是一个三维张量，Keras 会自动根据输入的 shape 识别序列长度。<br>只要你传入的数据形状符合要求，Keras 会根据实际输入动态处理序列长度，不必在代码里显式声明。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seq_in = Input(shape=(timesteps, input_dim))</span><br></pre></td></tr></table></figure>

<h4 id="LSTM参数计算公式"><a href="#LSTM参数计算公式" class="headerlink" title="LSTM参数计算公式"></a>LSTM参数计算公式</h4><p>对于一个LSTM层，参数数量是：<br>numParams &#x3D; 4 x [(units x input_dim + (units x units) + units]</p>
<ul>
<li>4 -&gt; 因为LSTM有4个门（输入门、遗忘门、候选单位、输出门），每个门都有一套权重</li>
<li>units x input_dim -&gt; 输入到隐藏层的权重</li>
<li>units x units -&gt; 隐藏到隐藏的循环权重</li>
<li>units -&gt; 每个门的偏置</li>
</ul>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>本示例结合一个LSTM模型的完整代码结构，介绍如何使用pandas准备数据、构建LSTM模型及计算参数量，方便初学者理解和应用。</p>
<h5 id="1-数据预处理（pandas基础）"><a href="#1-数据预处理（pandas基础）" class="headerlink" title="1. 数据预处理（pandas基础）"></a>1. 数据预处理（pandas基础）</h5><ul>
<li>读取CSV文件</li>
</ul>
<p>import pandas as pd<br>df &#x3D; pd.read_csv(“weatherHistory.csv”)</p>
<ul>
<li>查看数据前几行</li>
</ul>
<p>df.head()</p>
<ul>
<li>选取数值型列计算相关性</li>
</ul>
<p>num_df &#x3D; df.select_dtypes(include&#x3D;[‘number’])<br>corr_matrix &#x3D; num_df.corr()</p>
<ul>
<li>处理缺失值</li>
</ul>
<p>df.fillna(0, inplace&#x3D;True)</p>
<ul>
<li>标签编码</li>
</ul>
<p>from sklearn.preprocessing import LabelEncoder<br>labelencoder &#x3D; LabelEncoder()<br>df[‘Summary’] &#x3D; labelencoder.fit_transform(df[‘Summary’])</p>
<ul>
<li>时间序列转监督学习格式</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):</span><br><span class="line">    n_vars = 1 <span class="keyword">if</span> <span class="built_in">type</span>(data) is list <span class="keyword">else</span> data.shape[1]</span><br><span class="line">    <span class="built_in">df</span> = pd.DataFrame(data)</span><br><span class="line">    cols, names = [], []</span><br><span class="line">    <span class="comment"># 过去时间步 (t-n, ... t-1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, 0, -1):</span><br><span class="line">        cols.append(df.shift(i))</span><br><span class="line">        names += [f<span class="string">&#x27;var&#123;j+1&#125;(t-&#123;i&#125;)&#x27;</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="comment"># 未来时间步 (t, t+1, ... t+n)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_out):</span><br><span class="line">        cols.append(df.shift(-i))</span><br><span class="line">        <span class="keyword">if</span> i == 0:</span><br><span class="line">            names += [f<span class="string">&#x27;var&#123;j+1&#125;(t)&#x27;</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            names += [f<span class="string">&#x27;var&#123;j+1&#125;(t+&#123;i&#125;)&#x27;</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    agg = pd.concat(cols, axis=1)</span><br><span class="line">    agg.columns = names</span><br><span class="line">    <span class="keyword">if</span> dropnan:</span><br><span class="line">        agg.dropna(inplace=True)</span><br><span class="line">    <span class="built_in">return</span> agg</span><br></pre></td></tr></table></figure>
<hr>
<h5 id="2-数据分割与准备"><a href="#2-数据分割与准备" class="headerlink" title="2. 数据分割与准备"></a>2. 数据分割与准备</h5><p>假设有 n_hours 过去时间步，和 n_features 特征数：</p>
<p>n_obs &#x3D; n_hours * n_features<br>train_X, train_Y &#x3D; train[:, :n_obs], train[:, -6]   # 以倒数第6列为输出变量<br>test_X, test_Y &#x3D; test[:, :n_obs], test[:, -6]</p>
<hr>
<h5 id="3-模型结构示例（Keras）"><a href="#3-模型结构示例（Keras）" class="headerlink" title="3. 模型结构示例（Keras）"></a>3. 模型结构示例（Keras）</h5><p>Model: “sequential”</p>
<table>
<thead>
<tr>
<th>Layer (type)</th>
<th>Output Shape</th>
<th>Param #</th>
</tr>
</thead>
<tbody><tr>
<td>LSTM (lstm)</td>
<td>(None, 30)</td>
<td>4680</td>
</tr>
<tr>
<td>Dense (FC1)</td>
<td>(None, 256)</td>
<td>7936</td>
</tr>
<tr>
<td>Activation</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
<tr>
<td>Dropout</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
<tr>
<td>Dense (out_layer)</td>
<td>(None, 1)</td>
<td>257</td>
</tr>
<tr>
<td><strong>Total Params</strong></td>
<td></td>
<td><strong>12873</strong></td>
</tr>
</tbody></table>
<ul>
<li>LSTM层参数计算公式：</li>
</ul>
<p>Param &#x3D; 4 × [(input_dim + hidden_dim) × hidden_dim + hidden_dim]</p>
<p>其中，<br>input_dim 是输入特征维度，<br>hidden_dim 是LSTM隐藏单元数量（本例为30）。</p>
<hr>
<h5 id="4-LSTM细胞状态计算简述"><a href="#4-LSTM细胞状态计算简述" class="headerlink" title="4. LSTM细胞状态计算简述"></a>4. LSTM细胞状态计算简述</h5><p>细胞状态更新：</p>
<p>$C_new &#x3D; f(z_f) * C_old + f(z_i) * g(z)$</p>
<p>隐藏状态输出：</p>
<p>$a &#x3D; f(z_o) * h(C)$</p>
<p>其中：<br>f 是sigmoid激活函数，<br>g 是tanh激活函数，<br>维度均为 (hidden_dim,)。</p>
<hr>
<h5 id="5-预测测试集时的说明"><a href="#5-预测测试集时的说明" class="headerlink" title="5. 预测测试集时的说明"></a>5. 预测测试集时的说明</h5><p>训练完模型后，预测测试集时，需保证测试集的输入格式与训练集一致，包含足够的过去时间步作为输入。</p>
<hr>
<p>以上为LSTM时间序列预测的关键步骤与概念总结，结合pandas数据处理及模型搭建，适合初学者快速理解和实践。</p>
<h3 id="GNN-Graph-Neural-Network"><a href="#GNN-Graph-Neural-Network" class="headerlink" title="GNN(Graph Neural Network)"></a>GNN(Graph Neural Network)</h3><h4 id="NN4G-Contextual-Model"><a href="#NN4G-Contextual-Model" class="headerlink" title="NN4G (Contextual Model)"></a>NN4G (Contextual Model)</h4><h5 id="Hidden-Units-State-Variables"><a href="#Hidden-Units-State-Variables" class="headerlink" title="Hidden Units (State Variables)"></a>Hidden Units (State Variables)</h5><p>节点 v 的第 i  个隐藏单元（状态变量）$x_i(v)$ 通过以下公式计算：</p>
<p>$$<br>x_i(v) &#x3D; f\left( \sum_{j&#x3D;0}^L w_{ij} l_j(v) + \sum_{k&#x3D;1}^{i-1} \sum_{u \in \mathcal{N}(v)} w_{ijk}^{(u,v)} , x_k(u) \right)<br>$$</p>
<p>其中：</p>
<ul>
<li>$x_i(v)$ 表示节点 $v$ 第 $i$ 个隐藏单元状态。</li>
<li>$l_j(v)$ 是节点 $v$ 的第 $j$ 个输入特征。</li>
<li>$w_{ij}$ 是输入特征到隐藏单元的权重。</li>
<li>$\mathcal{N}(v)$ 是节点 $v$ 的邻居集合。</li>
<li>$x_k(u)$ 是邻居节点 $u$ 的第 $k$ 个隐藏单元状态。</li>
<li>$w_{ijk}^{(u,v)}$ 是邻居隐藏单元到当前隐藏单元的权重，带有邻居 $u$ 和节点 $v$ 的标记。</li>
<li>$f(\cdot)$ 是激活函数（如 sigmoid 或 tanh）。</li>
</ul>
<h3 id="Feature-Normalization（特征归一化）"><a href="#Feature-Normalization（特征归一化）" class="headerlink" title="Feature Normalization（特征归一化）"></a>Feature Normalization（特征归一化）</h3><p>在图神经网络（GNN）或一般的深度学习任务中，特征归一化（Normalization）在不同阶段有不同作用。它可以显著提升训练稳定性、收敛速度，并具备隐式正则化的效果。以下是几种常见的归一化方式：</p>
<h4 id="🔹-1-输入特征归一化（Input-Feature-Normalization）"><a href="#🔹-1-输入特征归一化（Input-Feature-Normalization）" class="headerlink" title="🔹 1. 输入特征归一化（Input Feature Normalization）"></a>🔹 1. 输入特征归一化（Input Feature Normalization）</h4><p>对输入节点特征 $x_v$ 做归一化是数据预处理的一部分，通常在模型训练前执行。常见方式包括：</p>
<ul>
<li><p><strong>Z-score 标准化</strong>：</p>
<p>$$<br>x_v^{\text{norm}} &#x3D; \frac{x_v - \mu}{\sigma}<br>$$</p>
</li>
<li><p><strong>Min-Max 缩放</strong>：</p>
<p>$$<br>x_v^{\text{norm}} &#x3D; \frac{x_v - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}<br>$$</p>
</li>
</ul>
<p><strong>作用</strong>：</p>
<ul>
<li>使所有维度特征处于相似数值范围</li>
<li>防止特征值差异过大导致模型训练不稳定</li>
<li>提高模型收敛速度</li>
</ul>
<hr>
<h4 id="🔹-2-中间输出归一化（对线性层输出-z-做-Normalization）"><a href="#🔹-2-中间输出归一化（对线性层输出-z-做-Normalization）" class="headerlink" title="🔹 2. 中间输出归一化（对线性层输出 $z$ 做 Normalization）"></a>🔹 2. 中间输出归一化（对线性层输出 $z$ 做 Normalization）</h4><p>在深度网络的每一层中，线性变换的输出（如 $z &#x3D; Wx$）可能存在均值偏移或方差爆炸，容易导致梯度消失或梯度爆炸。为了解决这个问题，通常会对 $z$ 进行归一化：</p>
<ul>
<li><p><strong>标准化公式</strong>：</p>
<p>$$<br>\tilde{z} &#x3D; \frac{z - \mu_z}{\sigma_z}<br>$$</p>
<p>在此基础上通常还会加入可学习的缩放与偏移参数：</p>
<p>$$<br>\tilde{z} &#x3D; \gamma \cdot \frac{z - \mu_z}{\sigma_z} + \beta<br>$$</p>
</li>
</ul>
<hr>
<h4 id="🔹-3-Batch-Normalization（BN）"><a href="#🔹-3-Batch-Normalization（BN）" class="headerlink" title="🔹 3. Batch Normalization（BN）"></a>🔹 3. Batch Normalization（BN）</h4><p><strong>BatchNorm</strong> 是最常用的中间层归一化技术，作用于小批量训练样本的每一层输出。</p>
<h5 id="📌-训练阶段："><a href="#📌-训练阶段：" class="headerlink" title="📌 训练阶段："></a>📌 训练阶段：</h5><p>对每个 feature 维度 $z$：</p>
<p>$$<br>\tilde{z}^{(i)} &#x3D; \frac{z^{(i)} - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}}<br>$$</p>
<p>然后进行仿射变换：</p>
<p>$$<br>y^{(i)} &#x3D; \gamma \tilde{z}^{(i)} + \beta<br>$$</p>
<p>其中：</p>
<ul>
<li>$\mu_{\text{batch}}, \sigma_{\text{batch}}$：当前 batch 内的均值与方差</li>
<li>$\gamma, \beta$：可学习参数</li>
<li>$\epsilon$：为避免除以 0 的一个小常数</li>
</ul>
<h5 id="🧮-统计更新（用于测试）："><a href="#🧮-统计更新（用于测试）：" class="headerlink" title="🧮 统计更新（用于测试）："></a>🧮 统计更新（用于测试）：</h5><p>在训练过程中同时维护滑动平均统计量：</p>
<p>$$<br>\bar{\mu} \leftarrow p \cdot \bar{\mu} + (1 - p) \cdot \mu_{\text{batch}}<br>$$</p>
<p>$$<br>\bar{\sigma}^2 \leftarrow p \cdot \bar{\sigma}^2 + (1 - p) \cdot \sigma_{\text{batch}}^2<br>$$</p>
<p>测试时使用：</p>
<p>$$<br>\tilde{z}_{\text{test}} &#x3D; \frac{z - \bar{\mu}}{\sqrt{\bar{\sigma}^2 + \epsilon}}<br>$$</p>
<h5 id="✅-优点："><a href="#✅-优点：" class="headerlink" title="✅ 优点："></a>✅ 优点：</h5><ul>
<li>减少内部协变量偏移（Internal Covariate Shift）</li>
<li>提升收敛速度</li>
<li>在训练中具备隐式正则化作用</li>
<li>对深层神经网络特别有效</li>
</ul>
<hr>
<h4 id="🔍-其他归一化方法（补充）"><a href="#🔍-其他归一化方法（补充）" class="headerlink" title="🔍 其他归一化方法（补充）"></a>🔍 其他归一化方法（补充）</h4><table>
<thead>
<tr>
<th>方法</th>
<th>场景</th>
<th>是否依赖 batch</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>LayerNorm</td>
<td>NLP、GNN、Transformer</td>
<td>否</td>
<td>对每个样本单独归一化</td>
</tr>
<tr>
<td>InstanceNorm</td>
<td>图像生成</td>
<td>否</td>
<td>对每个样本的每个通道归一化</td>
</tr>
<tr>
<td>GraphNorm</td>
<td>图神经网络</td>
<td>否</td>
<td>考虑图结构的特征归一化策略</td>
</tr>
</tbody></table>
<hr>
<h3 id="📌-小结"><a href="#📌-小结" class="headerlink" title="📌 小结"></a>📌 小结</h3><table>
<thead>
<tr>
<th>阶段</th>
<th>方法</th>
<th>目的</th>
</tr>
</thead>
<tbody><tr>
<td>输入阶段</td>
<td>Z-score、Min-Max</td>
<td>消除不同特征量纲差异</td>
</tr>
<tr>
<td>网络中间层</td>
<td>BatchNorm、LayerNorm</td>
<td>稳定训练、加速收敛</td>
</tr>
<tr>
<td>测试阶段</td>
<td>使用 EMA 平滑统计</td>
<td>保证预测一致性和稳定性</td>
</tr>
</tbody></table>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Input -&gt; Positional Encoding -&gt; Self-attention -&gt; 加residual -&gt; layer norm -&gt; FC的feed forward network -&gt; 加residual -&gt; layer norm<br>Encoder所需要做的事情：给一排向量输出另外一排向量，这个工作RNN、CNN也可以做。在Transformer里面，用的encoder是self-attention。<br>encoder分成了很多个block，每一个block都是输入一排向量、输出一排向量</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>不是每一个block是一个layer，因为一个block是好几个layer组合<br></p>
</blockquote>
<hr>
<h5 id="输入定义"><a href="#输入定义" class="headerlink" title="输入定义"></a>输入定义</h5><p>设输入序列为<br>$X \in \mathbb{R}^{T \times d_{\text{model}}}$</p>
<ul>
<li>$T$：序列长度  </li>
<li>$d_{\text{model}}$：每个 token 的特征维度</li>
</ul>
<p>加入位置编码：</p>
<p>$H^{(0)} &#x3D; X + PE$</p>
<p>其中 $PE \in \mathbb{R}^{T \times d_{\text{model}}}$ 是位置编码。</p>
<h5 id="第-l-层-Encoder-Block"><a href="#第-l-层-Encoder-Block" class="headerlink" title="第 l 层 Encoder Block"></a>第 l 层 Encoder Block</h5><p>每一层包含两个子层（Sub-layer）：</p>
<ol>
<li>多头自注意力（Multi-head Self-Attention）</li>
<li>前馈神经网络（Feed Forward Network, FFN）</li>
</ol>
<p>每一子层后面都接有残差连接和 Layer Normalization。</p>
<h6 id="Multi-head-Self-Attention"><a href="#Multi-head-Self-Attention" class="headerlink" title="Multi-head Self-Attention"></a>Multi-head Self-Attention</h6><p>首先计算Query, Key, Value:<br>$$<br>Q &#x3D; H^{(l-1)} W^Q,\quad K &#x3D; H^{(l-1)} W^K,\quad V &#x3D; H^{(l-1)} W^V<br>$$</p>
<p>其中：</p>
<ul>
<li>$W^Q, W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$<br>$d_model$通常等于输入或输出的特征维度 </li>
<li>通常 $d_k &#x3D; \frac{d_{\text{model}}}{h}$，$h$ 为注意力头数</li>
</ul>
<p>单个注意力头的输出：</p>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V<br>$$</p>
<p>多个头拼接：</p>
<p>$$<br>\text{MultiHead}(H) &#x3D; \text{Concat}(\text{head}_1, …, \text{head}_h) W^O<br>$$</p>
<p>加残差连接与 LayerNorm：</p>
<p>$$<br>\tilde{H}^{(l)} &#x3D; \text{LayerNorm}\left( H^{(l-1)} + \text{MultiHead}(H^{(l-1)}) \right)<br>$$</p>
<h6 id="前馈网络-Feed-Forward-Network-（MLP）"><a href="#前馈网络-Feed-Forward-Network-（MLP）" class="headerlink" title="前馈网络(Feed Forward Network)（MLP）"></a>前馈网络(Feed Forward Network)（MLP）</h6><p>位置前馈网络(Position-wise FFN)作用于每个位置的向量：<br>$$<br>\text{FFN}(x) &#x3D; \max(0, x W_1 + b_1) W_2 + b_2<br>$$<br>其中：</p>
<ul>
<li>$W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}$</li>
<li>$W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}$</li>
</ul>
<p>加残差与 LayerNorm：<br>$$<br>H^{(l)} &#x3D; \text{LayerNorm} \left( \tilde{H}^{(l)} + \text{FFN}(\tilde{H}^{(l)}) \right)<br>$$</p>
<h5 id="堆叠多个Encoder-Block"><a href="#堆叠多个Encoder-Block" class="headerlink" title="堆叠多个Encoder Block"></a>堆叠多个Encoder Block</h5><p>总共堆叠$L$层Encoder:<br>$$<br>H^{(L)} &#x3D; \text{Encoder}(X) &#x3D; \text{EncoderBlock}^{(L)} \circ \cdots \circ \text{EncoderBlock}^{(1)}(X + PE)<br>$$</p>
<p>每一层输入输出的维度保持为 $\mathbb{R}^{T \times d_{\text{model}}}$。</p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>decoder首先要读取encoder的输出，docoder先会输出一个distribution，选取得分最大的值（概率）输出</p>
<p>Transformer Decoder 接收两个输入：</p>
<ol>
<li>编码器输出 $H^{(L)} \in \mathbb{R}^{T_{\text{enc}} \times d_{\text{model}}}$</li>
<li>目标序列的前缀（在训练时为 ground truth，推理时为模型历史预测）(也就是已经生成的目标序列target sequence前缀的embedding向量 + 位置信息) e.g., 法语目标句: begin Je suis étudiant<br>由此可知在Transformer的训练过程中，target序列是明确参与训练的，这也叫<strong>teacher forcing</strong>的方法:</li>
</ol>
<ul>
<li>Transformer的Decoder输入是目标序列的前缀（即target的部分或全部已知部分），模型学习预测下一个token。</li>
<li>换句话说，模型“看到”正确的历史target作为上下文来训练，减少误差积累。</li>
</ul>
<p>Decoder 输出目标序列的每一个 token 表示（最终输入 softmax 得出概率分布）。</p>
<h5 id="第-l-层Decoder-Block构成"><a href="#第-l-层Decoder-Block构成" class="headerlink" title="第$l$层Decoder Block构成"></a>第$l$层Decoder Block构成</h5><p>每一层 decoder block 包含三个子层：</p>
<ol>
<li>Masked Multi-head Self-Attention（对目标序列自身）</li>
<li>Multi-head Encoder-Decoder Attention（使用 encoder 输出）</li>
<li>Feed Forward Network（FFN）</li>
</ol>
<p>每一子层后均使用残差连接和 LayerNorm。</p>
<h6 id="Masked-Multi-head-Self-Attention-对decoder输入做自注意"><a href="#Masked-Multi-head-Self-Attention-对decoder输入做自注意" class="headerlink" title="Masked Multi-head Self-Attention (对decoder输入做自注意)"></a>Masked Multi-head Self-Attention (对decoder输入做自注意)</h6><p>避免看到未来的消息，使用mask:<br>$$<br>Q^{\text{dec}} &#x3D; H_{\text{dec}}^{(l-1)} W^Q, \quad<br>K^{\text{dec}} &#x3D; H_{\text{dec}}^{(l-1)} W^K, \quad<br>V^{\text{dec}} &#x3D; H_{\text{dec}}^{(l-1)} W^V<br>$$</p>
<p>计算 masked attention：</p>
<p>$$<br>\text{MaskedAttention}(Q, K, V) &#x3D; \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} + M \right) V<br>$$</p>
<ul>
<li>$M$ 是 mask 矩阵，用于遮蔽未来的 token</li>
</ul>
<p>添加残差连接：</p>
<p>$$<br>\tilde{H}<em>{\text{dec}}^{(l,1)} &#x3D; \text{LayerNorm}\left( H</em>{\text{dec}}^{(l-1)} + \text{MaskedAttention}(\cdot) \right)<br>$$</p>
<h6 id="Encoder-Decoder-Attention-查询encoder输出"><a href="#Encoder-Decoder-Attention-查询encoder输出" class="headerlink" title="Encoder-Decoder Attention (查询encoder输出)"></a>Encoder-Decoder Attention (查询encoder输出)</h6><p>$$<br>Q^{\text{dec-enc}} &#x3D; \tilde{H}<em>{\text{dec}}^{(l,1)} W^Q, \quad<br>K^{\text{enc}} &#x3D; H</em>{\text{enc}}^{(L)} W^K, \quad<br>V^{\text{enc}} &#x3D; H_{\text{enc}}^{(L)} W^V<br>$$</p>
<p>$$<br>\text{CrossAttention}(Q, K, V) &#x3D; \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V<br>$$</p>
<p>残差连接：</p>
<p>$$<br>\tilde{H}<em>{\text{dec}}^{(l,2)} &#x3D; \text{LayerNorm}\left( \tilde{H}</em>{\text{dec}}^{(l,1)} + \text{CrossAttention}(\cdot) \right)<br>$$</p>
<h6 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed Forward Network"></a>Feed Forward Network</h6><p>$$<br>H_{\text{dec}}^{(l)} &#x3D; \text{LayerNorm}\left( \tilde{H}<em>{\text{dec}}^{(l,2)} + \text{FFN}(\tilde{H}</em>{\text{dec}}^{(l,2)}) \right)<br>$$</p>
<h6 id="最终输出"><a href="#最终输出" class="headerlink" title="最终输出"></a>最终输出</h6><p>输出：</p>
<p>$$<br>Y &#x3D; \text{Decoder}(H_{\text{enc}}^{(L)}, Y_{\text{in}}) \in \mathbb{R}^{T_{\text{dec}} \times d_{\text{model}}}<br>$$</p>
<p>最终通过一个线性变换 $W_o$ 和 softmax 映射为词表分布：</p>
<p>$$<br>P(y_t | y_{&lt;t}, X) &#x3D; \text{softmax}(W_o H_{\text{dec}, t}^{(L)} + b_o)<br>$$</p>
<h5 id="自回归-vs-非自回归"><a href="#自回归-vs-非自回归" class="headerlink" title="自回归 vs 非自回归"></a>自回归 vs 非自回归</h5><h6 id="Autoregressive-Transformer-AT"><a href="#Autoregressive-Transformer-AT" class="headerlink" title="Autoregressive Transformer (AT)"></a>Autoregressive Transformer (AT)</h6><ul>
<li>Decoder 每一步只接收过去的输出，逐步生成</li>
<li>序列生成是 <strong>逐步预测</strong> 的，无法并行</li>
<li>使用 Masked Self-Attention 限制信息流方向</li>
</ul>
<h6 id="Non-Autoregressive-Transformer-NAT"><a href="#Non-Autoregressive-Transformer-NAT" class="headerlink" title="Non-Autoregressive Transformer (NAT)"></a>Non-Autoregressive Transformer (NAT)</h6><ul>
<li>一次性生成全部 token，跳过逐步预测</li>
<li>模型需额外 <strong>预测输出长度</strong>，或引入特殊机制（如 mask、编辑路径、latent alignment）</li>
<li>编码器输出需携带更多信息</li>
<li>速度快，但精度通常低于 AT</li>
</ul>
<h3 id="Vision-Transformer-ViT"><a href="#Vision-Transformer-ViT" class="headerlink" title="Vision Transformer (ViT)"></a>Vision Transformer (ViT)</h3><p>ViT和一般的Transformer不同，不需要decoder，只需要encoder。<br>ViT将输入图片分为多个patch(16x16),再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。<br>对于图片分类，因此在输入序列中加入一个<strong>特殊的</strong>token，该token对应的输出即为最后的类别预测。</p>
<ul>
<li>patch embedding:<br>例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224&#x2F;16x16&#x3D;196个patch，也就是说sequence length &#x3D; 196，每个patch维度16x16x3 &#x3D; 768。因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是197x768。因此，通过patch embedding可以将vision问题转为一个seq2seq问题</li>
<li>positional encoding<br>和basic transformer一样,可以理解为为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列embedding的维度相同（768）</li>
<li>Layer Normalization&#x2F;multi-head attention&#x2F;Layer Normalization<br>LN输出维度依然是197x768。多头自注意力时，先将输入映射到q，k，v，如果只有一个头，qkv的维度都是197x768，如果有12个头（768&#x2F;12&#x3D;64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是197x768</li>
<li>MLP（Feed Forward Network FFN）<br>将维度放大再缩小回去，197x768放大为197x3072，再缩小变为197x768</li>
</ul>
<p>一个block之后维度依然和输入相同，都是197x768，因此可以堆叠多个block。最后会将特殊字符cls对应的输出<br>作为encoder的最终输出 ，代表最终的image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均）</p>
<h3 id="Generative-Adversarial-Networks-GAN"><a href="#Generative-Adversarial-Networks-GAN" class="headerlink" title="Generative Adversarial Networks (GAN)"></a>Generative Adversarial Networks (GAN)</h3><p>Generative Adversarial Networks (GANs) are a class of deep learning models in which <strong>two networks compete</strong>: a <strong>generator</strong> tries to produce realistic data, and a <strong>discriminator</strong> tries to distinguish between real and fake data. This adversarial setup drives both networks to improve.</p>
<h4 id="Key-Components"><a href="#Key-Components" class="headerlink" title="Key Components"></a>Key Components</h4><h5 id="1-Generator-G"><a href="#1-Generator-G" class="headerlink" title="1. Generator (G)"></a>1. Generator (G)</h5><ul>
<li><strong>Input</strong>: random noise vector <code>z</code> (e.g., 100-dimensional Gaussian)</li>
<li><strong>Output</strong>: fake data (e.g., an image)</li>
<li><strong>Goal</strong>: generate samples that look like real data and <strong>fool the discriminator</strong></li>
</ul>
<h5 id="2-Discriminator-D"><a href="#2-Discriminator-D" class="headerlink" title="2. Discriminator (D)"></a>2. Discriminator (D)</h5><ul>
<li><strong>Input</strong>: real or generated data</li>
<li><strong>Output</strong>: a probability (0 &#x3D; fake, 1 &#x3D; real)</li>
<li><strong>Goal</strong>: correctly distinguish real data from generated (fake) data</li>
</ul>
<h4 id="Overall-Objective"><a href="#Overall-Objective" class="headerlink" title="Overall Objective"></a>Overall Objective</h4><p>The two networks play a <strong>minimax game</strong><br>$$<br>\min_G \max_D V(D, G) &#x3D; \mathbb{E}<em>{x \sim p</em>{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]<br>$$</p>
<ul>
<li>Discriminator <code>D</code> tries to <strong>maximize</strong> the ability to classify real vs fake.</li>
<li>Generator <code>G</code> tries to <strong>minimize</strong> the ability of <code>D</code> to detect its fakes.</li>
</ul>
<h4 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h4><h5 id="Step-1-Fix-Generator-Train-Discriminator"><a href="#Step-1-Fix-Generator-Train-Discriminator" class="headerlink" title="Step 1: Fix Generator, Train Discriminator"></a>Step 1: Fix Generator, Train Discriminator</h5><p><strong>Goal</strong>: Teach <code>D</code> to distinguish real and fake data</p>
<ol>
<li>Sample a batch of real data <code>x_real</code> from the dataset.</li>
<li>Sample random noise <code>z</code>, and generate fake data <code>x_fake = G(z)</code>.</li>
<li>Compute discriminator outputs:<ul>
<li><code>D(x_real)</code> → should be close to 1</li>
<li><code>D(G(z))</code> → should be close to 0</li>
</ul>
</li>
<li>Compute discriminator loss:</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L_D = -[ \log D(x_&#123;\text&#123;real&#125;&#125;) + \log(1 - D(G(z))) ]</span><br></pre></td></tr></table></figure>
<p>用GAN原始论文中判别器的目标函数表达的是同一个意思，只是换了视角：<br>$$<br>\mathbb{E}<em>{x \sim P</em>{\text{data}}}[\log D(x)] + \mathbb{E}_{x’ \sim P_g}[\log(1 - D(x’))]<br>$$</p>
<ul>
<li>D(x) is the discriminator’s estimated probability that x is real</li>
<li>G(z) maps a noise vector z to a generated sample x’ &#x3D; G(z)</li>
<li>$P_{data}$是real image集，也是随机变量，$P_g$是经过generator后产生的数据集。</li>
</ul>
<ol start="5">
<li>Update only <strong>D’s parameters</strong> (freeze G).</li>
</ol>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>Assuming an optimal discriminator $D^<em>(x) &#x3D; \frac{P_{\text{data}}(x)}{P_{\text{data}}(x) + P_g(x)}$, plugging this into the value function yields:<br><br>$V(D^</em>, G) &#x3D; -\log(4) + 2 \cdot D_{JS}(P_{\text{data}} | P_g)$<br><br>This means <strong>minimizing $V(D^*, G)$ is equivalent to minimizing JS divergence</strong>. Hence, improving the generator means reducing<br> divergence between the two distributions.<br></p>
</blockquote>
<hr>
<h6 id="Step-2-Fix-Discriminator-Train-Generator"><a href="#Step-2-Fix-Discriminator-Train-Generator" class="headerlink" title="Step 2: Fix Discriminator, Train Generator"></a>Step 2: Fix Discriminator, Train Generator</h6><p><strong>Goal</strong>: Improve <code>G</code> so that <code>D</code> can’t tell its output is fake</p>
<ol>
<li>Sample new random noise <code>z</code></li>
<li>Generate fake data <code>x_fake = G(z)</code></li>
<li>Compute discriminator prediction <code>D(G(z))</code></li>
<li>Generator loss (trying to <strong>maximize</strong> D’s confidence that fakes are real):</li>
</ol>
<p>$$<br>L_G &#x3D; -\log D(G(z))<br>$$</p>
<ol start="5">
<li>Update only <strong>G’s parameters</strong> (freeze D)</li>
</ol>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>可以把Generator和Discriminator合成一个large network来看待<br><br>例如Genrator有5层神经网络，Discriminator有5层神经网络，那么我们一共要训练的就是10层神经网络<br></p>
</blockquote>
<hr>
<h3 id="Self-Supervised"><a href="#Self-Supervised" class="headerlink" title="Self-Supervised"></a>Self-Supervised</h3><p>Self-supervised learning（自监督学习）是一种无需人工标注标签的训练方法。它从<strong>数据本身构造训练信号</strong>，本质上是监督学习，但标签是自动生成的。</p>
<h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT（Bidirectional Encoder Representations from Transformers）是 Google 提出的语言模型，它基于 Transformer 编码器结构，利用双向上下文信息对文本进行深度理解。BERT 在多个自然语言处理任务上实现了 SOTA（state-of-the-art）性能。<br>BERT 的训练不需要人工标注数据，而是通过两种任务<strong>自动生成标签</strong>：</p>
<h5 id="Masked-Language-Modeling-MLM"><a href="#Masked-Language-Modeling-MLM" class="headerlink" title="Masked Language Modeling (MLM)"></a>Masked Language Modeling (MLM)</h5><ul>
<li>原始句子：<code>I love spicy food</code></li>
<li>遮盖构造：<code>I love [MASK] food</code></li>
<li>模型目标：预测 <code>[MASK]</code> 是 <code>spicy</code></li>
</ul>
<p><strong>标签 spicy 是从原始句子自动提取的，而不是人工写入。</strong></p>
<p>这就是自监督的关键点：<strong>标签自动构造。</strong></p>
<h5 id="Next-Sentence-Prediction-NSP"><a href="#Next-Sentence-Prediction-NSP" class="headerlink" title="Next Sentence Prediction (NSP)"></a>Next Sentence Prediction (NSP)</h5><ul>
<li>给定句子 A 和句子 B：<ul>
<li>有 50% 情况：B 是 A 的下一句（真实）</li>
<li>有 50% 情况：B 是随机句子（非真实）</li>
</ul>
</li>
<li>模型目标：预测 B 是否是 A 的下一句</li>
</ul>
<p><strong>这同样不需要人工标注，BERT 从语料库中自动采样句子构造标签。</strong></p>
<h5 id="BERT的训练流程"><a href="#BERT的训练流程" class="headerlink" title="BERT的训练流程"></a>BERT的训练流程</h5><ol>
<li><strong>输入构造</strong>：<ul>
<li>输入句子对 + Token Embeddings + Segment Embeddings + Positional Embeddings</li>
</ul>
</li>
<li><strong>通过多层 Transformer Encoder 处理</strong></li>
<li><strong>MLM 预测遮盖的词，NSP 判断句子顺序</strong></li>
</ol>
<h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><ul>
<li>BERT 使用了自监督学习方法训练，无需人工标签</li>
<li>虽然它是一个监督学习问题（有输入和目标），但这些目标是从数据本身自动生成的。</li>
<li>自监督学习让模型能利用大规模未标注语料，提升预训练效果。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/blog/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/blog/">1</a><span class="page-number current">2</span><a class="page-number" href="/blog/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/5/">5</a><a class="extend next" rel="next" href="/blog/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
