<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hechenyi.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Basic ConceptLook for a function for e.g., image recognition, speech recognition.    RegressionThe function outputs a scalar ClassificationGiven options (classes), the function outputs the correct one">
<meta property="og:type" content="article">
<meta property="og:title" content="MachineLearning">
<meta property="og:url" content="https://hechenyi.github.io/2025/06/16/MachineLearning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Basic ConceptLook for a function for e.g., image recognition, speech recognition.    RegressionThe function outputs a scalar ClassificationGiven options (classes), the function outputs the correct one">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hechenyi.github.io/images/LSTM_Formulation.png">
<meta property="article:published_time" content="2025-06-16T15:42:25.000Z">
<meta property="article:modified_time" content="2025-07-21T16:19:37.256Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hechenyi.github.io/images/LSTM_Formulation.png">

<link rel="canonical" href="https://hechenyi.github.io/2025/06/16/MachineLearning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>MachineLearning | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://hechenyi.github.io/2025/06/16/MachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MachineLearning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-06-16 23:42:25" itemprop="dateCreated datePublished" datetime="2025-06-16T23:42:25+08:00">2025-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-07-22 00:19:37" itemprop="dateModified" datetime="2025-07-22T00:19:37+08:00">2025-07-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h2><p>Look for a function for e.g., image recognition, speech recognition.  </p>
<ul>
<li>Regression<br>The function outputs a scalar</li>
<li>Classification<br>Given options (classes), the function outputs the correct one.</li>
<li>Structured Learning</li>
</ul>
<h3 id="Linear-Model"><a href="#Linear-Model" class="headerlink" title="Linear Model"></a>Linear Model</h3><h4 id="Define-function-with-unknown-parameters"><a href="#Define-function-with-unknown-parameters" class="headerlink" title="Define function with unknown parameters"></a>Define function with unknown parameters</h4><p>Model: $y &#x3D; b + w_1x_1$ based on domain knowledge<br>w (weight) and b (bias) are unknown parameters (learn from data)<br>如果是一组包含有多个feature（例如同时考虑前七天的数据来进行预测）<br>那么$y &#x3D; b + \sum_j w_jx_j$</p>
<h4 id="Define-Loss-from-Training-Data"><a href="#Define-Loss-from-Training-Data" class="headerlink" title="Define Loss from Training Data"></a>Define Loss from Training Data</h4><p>Loss is a function of the unknown parameters: L(b,w)<br>e.g., L(0.5k, 1) corresponding to $\hat{y} &#x3D; 0.5k + 1x_1$.<br>$\hat{y}$ is predicted value, y is ground truth (target)<br>我们定义一个loss function为:<br>$e_n &#x3D; |y_n - \hat{y_n}|$  </p>
<p>Loss: $L(b, w) &#x3D; \frac{1}{N} \sum_{n&#x3D;1}^{N} \left| y_n - \hat{y}_n \right|$</p>
<p>其中y为真实值，$\hat{y}$为预测值,这里我们定义的Loss是mean absolute error （MAE），如果是mean square error（MSE），则是差值的的平方。</p>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>我们通过最小化损失函数 <em>L(b, w)</em> 来学习未知参数 b 和 w。优化目标为：</p>
<p>$b^<em>, w^</em> &#x3D; \arg\min_{b, w} L(b, w)$</p>
<p>我们可以使用梯度下降法进行优化，参数更新如下：</p>
<p>$w \leftarrow w - \eta \frac{\partial L}{\partial w}, \quad<br>b \leftarrow b - \eta \frac{\partial L}{\partial b}$</p>
<p>其中 $\eta$ 为学习率。对于 MAE 损失，由于其不可导点较多，可以使用次梯度法；对于 MSE 损失，则可以直接使用标准梯度下降算法。</p>
<h3 id="Sophisticated-Models"><a href="#Sophisticated-Models" class="headerlink" title="Sophisticated Models"></a>Sophisticated Models</h3><p>Beacuse linear models have severe limitation (model bias), so we include more sophisticated models.</p>
<h4 id="Sigmoid-function（logistic-regression）"><a href="#Sigmoid-function（logistic-regression）" class="headerlink" title="Sigmoid function（logistic regression）"></a>Sigmoid function（logistic regression）</h4><p>$y &#x3D; b + wx_1 &#x3D;&gt; y &#x3D; b + \sum_i c_i \cdot \sigma(b_i + w_ix_1)$<br>注：不同的w，b，c可以让sigmoid曲线可以拟合不同的曲线。</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>输入层: $x_j (j &#x3D; 1…d)$<br><br>   ↓ <br><br>隐藏层: $h_i &#x3D; \sigma(b_i + \sum w_ij x_j) (i &#x3D; 1…N)$<br><br>   ↓ <br><br>输出层: $y &#x3D; b + \sum c_i h_i$ (标量输出)<br></p>
</blockquote>
<hr>
<p>因此这里设计到deep learning中的<strong>hiden node</strong>的概念(hiden node的数量可以理解为用来拟合的ReLU曲线数量)，隐藏点越多，能够拟合出来的function越准确。<br>注意这里是多个sigmoid的和，因为实际情况可能是由多个sigmoid拟合而成。<br>相对应的，对于$y &#x3D; b + \sum_j w_jx_j$, 则有$y &#x3D; b + \sum_i c_i \cdot \sigma(b_i + \sum_j w_{ij} x_j)$  </p>
<p>示例：i &#x3D; 1,2,3; j &#x3D; 1,2,3 时的显式形式<br>$$<br>r_1 &#x3D; b_1 + w_{11}x_1 + w_{12}x_2 + w_{13}x_3<br>$$</p>
<p>$$<br>r_2 &#x3D; b_2 + w_{21}x_1 + w_{22}x_2 + w_{23}x_3<br>$$</p>
<p>$$<br>r_3 &#x3D; b_3 + w_{31}x_1 + w_{32}x_2 + w_{33}x_3<br>$$</p>
<p>向量化（矩阵）形式<br>$$<br>\mathbf{r} &#x3D; W \mathbf{x} + \mathbf{b}<br>$$</p>
<p>然后对生成的r做sigmoid函数，矩阵表达形式如下</p>
<p>$$<br>\mathbf{a} &#x3D; \sigma(\mathbf{r})<br>$$</p>
<p>最后的输出y矩阵表达形式如下：<br>$$<br>y &#x3D; b + \mathbf{c}^T \mathbf{a}<br>$$</p>
<p>Optimization部分，先将所有unknown parameters放到$\mathbf{\theta}$向量中<br>$$<br>\mathbf{\theta} &#x3D; \left[ b,\ c_1,\ \dots,\ c_i,\ b_1,\ \dots,\ b_i,\ w_{11},\ \dots,\ w_{ij} \right]<br>$$</p>
<p>定义Loss function $L(\theta)$,最后想要取得的 $\mathbf{\theta^{*}} &#x3D; \arg\min_{\theta} L$  </p>
<p>因此我们同样的使用梯度下降更新去得到最后的$\mathbf{\theta^{*}}$<br>$$<br>\theta \leftarrow \theta - \eta \cdot \nabla_\theta L(\theta)<br>$$  </p>
<p><strong>Batch Size</strong>, <strong>Epoch</strong>, <strong>N</strong><br>1,000 examples (N &#x3D; 1,000), Batch size is 100, so 100 update in 1 epoch (10 Batch).</p>
<h3 id="Use-Quadratic-and-Higher-Order-Terms"><a href="#Use-Quadratic-and-Higher-Order-Terms" class="headerlink" title="Use Quadratic and Higher-Order Terms"></a>Use Quadratic and Higher-Order Terms</h3><p>To increase model expressiveness, you can replace the linear input with polynomial terms.</p>
<p>For example, a quadratic input:<br>$$<br>y &#x3D; b + w_1 \cdot x_1 + w_2 \cdot x_1^2 + w_3 \cdot x_1^3 + … + w_n \cdot x_1^n<br>$$</p>
<h4 id="Overfitting-Risk"><a href="#Overfitting-Risk" class="headerlink" title="Overfitting Risk"></a>Overfitting Risk</h4><ul>
<li>Adding higher-degree polynomials increases the parameter space.</li>
<li>The model can fit training data too well, including noise.</li>
<li>This leads to poor generalization on unseen data.</li>
</ul>
<h4 id="Rugularization-to-Prevent-Overfitting"><a href="#Rugularization-to-Prevent-Overfitting" class="headerlink" title="Rugularization to Prevent Overfitting"></a>Rugularization to Prevent Overfitting</h4><p>Add a penalty term to the loss function:</p>
<p>为了防止过拟合，通常在损失函数中加入正则化项，对权重进行惩罚。<br>常见的有 <strong>L2 正则化（权重衰减）</strong>：</p>
<p>$\text{Loss} &#x3D; \text{MSE} + \lambda \sum_{i} w_i^2$</p>
<ul>
<li>其中，(\lambda) 是正则化系数（也叫 <strong>weight decay</strong>）。</li>
<li>通过限制权重 (w_i) 的大小，减少模型对高阶项的过度依赖，从而降低过拟合风险。</li>
</ul>
<h4 id="训练时如何加上正则化相"><a href="#训练时如何加上正则化相" class="headerlink" title="训练时如何加上正则化相"></a>训练时如何加上正则化相</h4><p>训练模型时，需要在参数更新步骤中加上正则化项。<br>以 <strong>L2 正则化</strong> 为例，每次参数更新公式：</p>
<p>$w :&#x3D; w - \eta (\nabla L + \lambda w)$</p>
<ul>
<li>$\eta$：学习率（learning rate）</li>
<li>$\nabla L$：损失函数对参数的梯度</li>
<li>$\lambda w$：正则化项对参数的梯度（L2 正则化）</li>
</ul>
<p>✅ 直观理解</p>
<p>每次更新时，把权重往 0 再拉一点，<br>防止它们变得太大，避免模型拟合噪声，提升泛化能力。</p>
<h3 id="PyTorch-CNN-二分类示例"><a href="#PyTorch-CNN-二分类示例" class="headerlink" title="PyTorch CNN 二分类示例"></a>PyTorch CNN 二分类示例</h3><h4 id="预处理transform"><a href="#预处理transform" class="headerlink" title="预处理transform"></a>预处理transform</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="自定义Dataset"><a href="#自定义Dataset" class="headerlink" title="自定义Dataset"></a>自定义Dataset</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class MyData(Dataset):</span><br><span class="line">    def __init__(self, root_dir, classes, transform=None):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.classes = classes</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">        self.img_paths = []</span><br><span class="line">        self.labels = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx, cls <span class="keyword">in</span> enumerate(classes):</span><br><span class="line">            cls_path = os.path.join(root_dir, cls)</span><br><span class="line">            <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(cls_path):</span><br><span class="line">                self.img_paths.append(os.path.join(cls_path, img_name))</span><br><span class="line">                self.labels.append(idx)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        img_path = self.img_paths[idx]</span><br><span class="line">        img = Image.open(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        label = self.labels[idx]</span><br><span class="line">        <span class="built_in">return</span> img, label</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        <span class="built_in">return</span> len(self.img_paths)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="简单CNN模型"><a href="#简单CNN模型" class="headerlink" title="简单CNN模型"></a>简单CNN模型</h4><h5 id="输入数据格式"><a href="#输入数据格式" class="headerlink" title="输入数据格式"></a>输入数据格式</h5><ul>
<li>输入图像为彩色图像（RGB），尺寸统一为 256×256 像素。</li>
<li>输入张量形状为 <code>[batch_size, 3, 256, 256]</code>，其中 <code>3</code> 是通道数（RGB），<code>batch_size</code> 是批量大小。</li>
</ul>
<h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><h5 id="卷积层和池化层流程"><a href="#卷积层和池化层流程" class="headerlink" title="卷积层和池化层流程"></a>卷积层和池化层流程</h5><ul>
<li><p><strong>第一层卷积</strong>: <code>nn.Conv2d(3, 16, kernel_size=3, padding=1)</code></p>
<ul>
<li>输入通道数：3</li>
<li>输出通道数：16</li>
<li>卷积核大小：3×3</li>
<li>填充大小：1，步长默认1</li>
<li>输出尺寸计算公式：<br>[<br>\text{Output Size} &#x3D; \left\lfloor \frac{\text{Input Size} + 2 \times \text{padding} - \text{kernel size}}{\text{stride}} \right\rfloor + 1<br>]<br>代入参数：<br>[<br>\frac{256 + 2 \times 1 - 3}{1} + 1 &#x3D; 256<br>]</li>
<li>输出张量形状：<code>[batch_size, 16, 256, 256]</code></li>
</ul>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>第一次池化</strong>: <code>nn.MaxPool2d(2)</code></p>
<ul>
<li>池化窗口大小：2×2，步长2</li>
<li>尺寸减半，输出形状为：<code>[batch_size, 16, 128, 128]</code></li>
</ul>
</li>
<li><p><strong>第二层卷积</strong>: <code>nn.Conv2d(16, 32, kernel_size=3, padding=1)</code></p>
<ul>
<li>输入通道数：16</li>
<li>输出通道数：32</li>
<li>卷积核大小：3×3，填充1，步长1</li>
<li>输出尺寸同理计算为：<code>128</code></li>
<li>输出张量形状：<code>[batch_size, 32, 128, 128]</code></li>
</ul>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>第二次池化</strong>: <code>nn.MaxPool2d(2)</code></p>
<ul>
<li>尺寸减半，输出形状为：<code>[batch_size, 32, 64, 64]</code></li>
</ul>
</li>
</ul>
<h5 id="全连接层流程"><a href="#全连接层流程" class="headerlink" title="全连接层流程"></a>全连接层流程</h5><ul>
<li><p><strong>展平层</strong>: 将卷积层输出展平成一维向量<br>输入维度为：<br>[<br>32 \times 64 \times 64 &#x3D; 131072<br>]</p>
</li>
<li><p><strong>全连接层1</strong>: <code>nn.Linear(131072, 64)</code>  </p>
</li>
<li><p><strong>ReLU激活</strong></p>
</li>
<li><p><strong>全连接层2</strong>: <code>nn.Linear(64, 1)</code></p>
</li>
<li><p><strong>Sigmoid激活</strong>: 输出概率值，范围在[0,1]之间</p>
</li>
<li><p><strong>输出</strong>: 使用 <code>.squeeze(1)</code> 将输出维度从 <code>[batch_size, 1]</code> 转换为 <code>[batch_size]</code>，便于计算损失函数。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class SimpleCNN(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(3,16,3,padding = 1);</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">            nn.Conv2d(16,32,3,padding = 1);</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(2),</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(32 * 64 * 64, 64),  <span class="comment"># 256-&gt;128-&gt;64，宽高减半两次</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(64, 1),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        def forward(self, x):</span><br><span class="line">            x = self.conv(x)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line">            <span class="built_in">return</span> x.squeeze(1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="准备数据和模型"><a href="#准备数据和模型" class="headerlink" title="准备数据和模型"></a>准备数据和模型</h4><h5 id="train-validate-test数据集的区别"><a href="#train-validate-test数据集的区别" class="headerlink" title="train,validate,test数据集的区别"></a>train,validate,test数据集的区别</h5><table>
<thead>
<tr>
<th>集合</th>
<th>作用</th>
<th>使用阶段</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>训练集 (Train)</td>
<td>用于模型训练，更新模型参数</td>
<td>训练阶段</td>
<td>占比最大，数据多样性丰富</td>
</tr>
<tr>
<td>验证集 (Validation)</td>
<td>用于调参和模型选择，监控过拟合情况</td>
<td>训练过程中的评估阶段</td>
<td>不参与训练，用于调节超参数和模型结构</td>
</tr>
<tr>
<td>测试集 (Test)</td>
<td>用于评估最终模型性能</td>
<td>训练完成后的性能评估</td>
<td>完全独立，不参与训练和调参</td>
</tr>
</tbody></table>
<h5 id="train-validate-test数据集的划分"><a href="#train-validate-test数据集的划分" class="headerlink" title="train,validate,test数据集的划分"></a>train,validate,test数据集的划分</h5><ul>
<li><p>创建完整数据集<br>dataset &#x3D; MyData(root_dir, classes, transform&#x3D;transform)</p>
</li>
<li><p>按比例划分训练、验证、测试集 70%, 15%, 15%<br>total_size &#x3D; len(dataset)<br>train_size &#x3D; int(0.7 * total_size)<br>val_size &#x3D; int(0.15 * total_size)<br>test_size &#x3D; total_size - train_size - val_size</p>
</li>
</ul>
<p>train_dataset, val_dataset, test_dataset &#x3D; random_split(dataset, [train_size, val_size, test_size])</p>
<ul>
<li>创建DataLoader<br>train_loader &#x3D; DataLoader(train_dataset, batch_size&#x3D;64, shuffle&#x3D;True)<br>val_loader &#x3D; DataLoader(val_dataset, batch_size&#x3D;64, shuffle&#x3D;False)<br>test_loader &#x3D; DataLoader(test_dataset, batch_size&#x3D;64, shuffle&#x3D;False)</li>
</ul>
<h4 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = 10</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    running_loss = 0.0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">        x, y = x.to(device), y.float().to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        pred = model(x)</span><br><span class="line">        loss = criterion(pred, y)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item() * x.size(0)</span><br><span class="line"></span><br><span class="line">    epoch_loss = running_loss / len(dataset)</span><br><span class="line">    <span class="built_in">print</span>(f<span class="string">&quot;Epoch &#123;epoch+1&#125;/&#123;n_epochs&#125; - Loss: &#123;epoch_loss:.4f&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练完成！&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>在分类中，需要讲线性输出映射到概率</p>
<h4 id="Sigmoid（二分类）"><a href="#Sigmoid（二分类）" class="headerlink" title="Sigmoid（二分类）"></a>Sigmoid（二分类）</h4><p>模型：<br>$p &#x3D; \sigma(z) &#x3D; \frac{1}{1 + e^{-z}},\quad z &#x3D; b + \sum_j w_j x_j$.<br>预测类别：<br>$\hat{y} &#x3D;\begin{cases} 1 &amp; \text{if } p &gt; 0.5 \ 0 &amp; \text{otherwise} \end{cases}$.<br>Binary Cross-Entropy Loss (BCL):<br>$L(b, w) &#x3D; -\frac{1}{N} \sum_{n&#x3D;1}^{N} [, y_n \log p_n + (1 - y_n) \log (1 - p_n)$</p>
<h4 id="Softmax（多分类）"><a href="#Softmax（多分类）" class="headerlink" title="Softmax（多分类）"></a>Softmax（多分类）</h4><p>对于$K$个类别<br>$p_k &#x3D; \frac{e^{z_k}}{\sum_{j&#x3D;1}^{K} e^{z_j}},\quad z_k &#x3D; b_k + \sum_j w_{kj} x_j \quad k&#x3D;1 \dots,K$.<br>Multiclass Cross-Entropy Loss:<br>$L(b, w) &#x3D; -\frac{1}{N} \sum_{n&#x3D;1}^{N} \sum_{k&#x3D;1}^{K} y_{nk} \log p_{nk}$。<br>其中：<br>$y_{nk} &#x3D; 1$ if sample $n$ is of class $k$, otherwise $0$.<br>因此这里用了one-hot编码，因为只考虑了只有真实类别对应的$y_{nk} &#x3D; 1$,其余为0，$p_{nk}$是Softmax后预测属于类别k的概率</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><h4 id="Linear-Regression-vs-Sigmoid-Regression"><a href="#Linear-Regression-vs-Sigmoid-Regression" class="headerlink" title="Linear Regression vs Sigmoid Regression"></a>Linear Regression vs Sigmoid Regression</h4><table>
<thead>
<tr>
<th>Task</th>
<th>Output Function</th>
<th>Activation</th>
<th>Loss Function</th>
</tr>
</thead>
<tbody><tr>
<td>Regression</td>
<td>$y$</td>
<td>Linear</td>
<td>MAE &#x2F; MSE</td>
</tr>
<tr>
<td>Binary Classification</td>
<td>$p$</td>
<td>Sigmoid</td>
<td>Binary Cross-Entropy (BCE)</td>
</tr>
<tr>
<td>Multiclass Classification</td>
<td>$p_k$</td>
<td>Softmax</td>
<td>Multiclass Cross-Entropy</td>
</tr>
</tbody></table>
<h4 id="MSE-vs-Binary-Cross-Entropy"><a href="#MSE-vs-Binary-Cross-Entropy" class="headerlink" title="MSE vs Binary Cross-Entropy"></a>MSE vs Binary Cross-Entropy</h4><table>
<thead>
<tr>
<th></th>
<th>Linear Regression</th>
<th>Logistic Regression</th>
</tr>
</thead>
<tbody><tr>
<td>🎯 任务</td>
<td>预测连续值（回归）</td>
<td>预测二分类（分类）</td>
</tr>
<tr>
<td>📈 模型</td>
<td>$y_hat &#x3D; b + Σ w_j * x_j$</td>
<td>$p &#x3D; σ(z) &#x3D; 1&#x2F;(1+exp(-z)), z &#x3D; b + Σ w_j * x_j$</td>
</tr>
<tr>
<td>📊 输出</td>
<td>实数 (-∞, +∞)</td>
<td>概率 [0, 1]</td>
</tr>
<tr>
<td>📚 分布</td>
<td>假设输出服从高斯分布</td>
<td>假设输出服从伯努利分布</td>
</tr>
<tr>
<td>🔗 损失</td>
<td>均方误差 (MSE: Mean Squared Error)</td>
<td>二元交叉熵 (BCE: Binary Cross-Entropy)</td>
</tr>
<tr>
<td>⚡ 目标</td>
<td>最大化高斯似然</td>
<td>最大化伯努利似然</td>
</tr>
<tr>
<td>⚙️ 梯度</td>
<td>梯度线性，更新相对平滑</td>
<td>梯度在非饱和区间较大，下降快，但接近 0&#x2F;1 时可能饱和</td>
</tr>
<tr>
<td>总体来说，Cross-Entropy效果更好一点</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p><strong>Support Vector Machine(SVM)</strong> 是一种常见的监督学习方法，常用于二分类任务，核心思想是通过寻找一个最优的分割超平面，使得不同类别之间的间隔（margin）最大化。</p>
<h4 id="Linear-Model-1"><a href="#Linear-Model-1" class="headerlink" title="Linear Model"></a>Linear Model</h4><p>对于线性可分数据，SVM定义一个线性决策函数：<br>$$<br>f(x) &#x3D; w^T x + b<br>$$</p>
<p>其中：</p>
<ul>
<li>$w$ 是权重向量，</li>
<li>$b$ 是偏置项。</li>
</ul>
<p>分类规则：</p>
<p>$$<br>\hat{y} &#x3D; \text{sign}(f(x)) &#x3D;<br>\begin{cases}<br>+1 &amp; \text{if } f(x) \ge 0 \\<br>-1 &amp; \text{if } f(x) &lt; 0<br>\end{cases}<br>$$</p>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>SVM 常用 <strong>合页损失（Hinge Loss）</strong>：</p>
<p>$$<br>L_n &#x3D; \max(0,, 1 - y_n f(x_n))<br>$$</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>如果$y_nf(x_n)&gt;0$则说明分类正确<br><br>但是,$y_nf(x_n)&gt;&#x3D;1$才完美，因为SVM不仅要求分类正确，还要最大化间隔，也就是安全间隔，所以在合页损失中有1<br></p>
</blockquote>
<hr>
<p>整体目标函数：</p>
<h2 id="L-w-b-frac-1-2-w-2-C-sum-n-1-N-max-0-1-y-n-w-T-x-n-b"><a href="#L-w-b-frac-1-2-w-2-C-sum-n-1-N-max-0-1-y-n-w-T-x-n-b" class="headerlink" title="$$L(w,b) &#x3D; \frac{1}{2} |w|^2 + C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b)).$$"></a>$$<br>L(w,b) &#x3D; \frac{1}{2} |w|^2 + C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b)).<br>$$</h2><blockquote>
<p><strong>Notice</strong><br><br>$\frac{1}{2}|w|^2$是正则化项，用于控制模型的复杂度，防止过拟合，权重不要太大。<br><br>$C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b))$是所有样本的合页损失的加权和。<br></p>
</blockquote>
<hr>
<h4 id="Optimization-Gradient-Descent"><a href="#Optimization-Gradient-Descent" class="headerlink" title="Optimization (Gradient Descent)"></a>Optimization (Gradient Descent)</h4><p>合页损失可以用梯度下降或随机梯度下降（SGD）优化：</p>
<p>$$<br>w \leftarrow w - \eta (w - C \sum_{n \in M} y_n x_n),<br>\quad<br>M &#x3D; { n ,|, 1 - y_n f(x_n) &gt; 0 }<br>$$</p>
<p>其中：</p>
<ul>
<li>$\eta$ 是学习率，</li>
<li>$M$ 是当前违反 margin 条件的样本集合。</li>
</ul>
<h3 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h3><p>利用贝叶斯公式根据某特征的先验概率计算出其后验概率，然后选择具有最大后验概率的类作为该特征所属的类</p>
<h4 id="条件概率。"><a href="#条件概率。" class="headerlink" title="条件概率。"></a>条件概率。</h4><p>就是在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示<br>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$</p>
<h4 id="全概率"><a href="#全概率" class="headerlink" title="全概率"></a>全概率</h4><p>如果事件A1,A2,…,An构成一个完备事件都有正概率，那么对于任意一个事件B则有：<br>$P(B)&#x3D;\sum_{i&#x3D;1}^{n}P(A_i)P(B|A_i)$</p>
<h4 id="贝叶斯推断"><a href="#贝叶斯推断" class="headerlink" title="贝叶斯推断"></a>贝叶斯推断</h4><p>$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$ </p>
<p>$P(A_i|B) &#x3D; P(A_i)\frac{P(B|A_i)}{\sum_{i&#x3D;1}^{n}P(A_i)P(B|A_i)}$<br>其中P(A)为先验概率，$P(A|B)$为后验概率，$\frac{P(B|A)}{P(B)}$为可能性函数（Likely hood）<br>转换成分类任务的表达式：<br>$P(类别|特征) &#x3D; P(类别)\frac{P(特征|类别)}{P(特征)}$。</p>
<h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>$P(帅 性格不好 不上进) &#x3D; P(嫁)P(帅|嫁)P(性格不好|嫁)P(不上进｜嫁) +  P(不嫁)P(帅|不嫁)P(性格不好|不嫁)P(不上进｜不嫁) &#x3D; 2&#x2F;125 + 3&#x2F;125$  </p>
<p>最终计算结果：<br>P(嫁|帅 性格不好 不上进) &#x3D; P(嫁)P(帅|嫁)P(性格不好|嫁)P(不上进｜嫁) &#x2F; P(帅 性格不好 不上进) &#x3D; (2&#x2F;125) &#x2F; (2&#x2F;125 + 3&#x2F;125) &#x3D; 0.4.<br>P(嫁|帅 性格不好 不上进) &#x3D; (3&#x2F;125) &#x2F; (2&#x2F;125 + 3&#x2F;125) &#x3D; 0.6. </p>
<h4 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h4><h4 id="最大似然估计（MLE）及-GaussianNB-示例"><a href="#最大似然估计（MLE）及-GaussianNB-示例" class="headerlink" title="最大似然估计（MLE）及 GaussianNB 示例"></a>最大似然估计（MLE）及 GaussianNB 示例</h4><hr>
<p>观测数据：<br>$$<br>X &#x3D; {x_1, x_2, \ldots, x_n}<br>$$</p>
<p>参数：<br>$$<br>\theta<br>$$</p>
<p>那么数据的似然函数是：<br>$$<br>L(\theta; X) &#x3D; P(X \mid \theta)<br>$$</p>
<p>最大似然估计就是：<br>$$<br>\hat{\theta}<em>{MLE} &#x3D; \arg\max</em>{\theta} L(\theta; X)<br>$$</p>
<hr>
<p>🔬 <strong>用在 GaussianNB 的例子</strong>  </p>
<p>在高斯朴素贝叶斯中，假设每个特征<br>$$<br>X_j<br>$$<br>在类别<br>$$<br>C_k<br>$$<br>下服从正态分布。</p>
<p>那么对应参数的估计为：</p>
<ul>
<li><p>均值：<br>$$<br>\mu_{jk} &#x3D; \frac{1}{n_k} \sum_{i&#x3D;1}^{n_k} x_j^{(i)}<br>$$</p>
</li>
<li><p>方差：<br>$$<br>\sigma_{jk}^2 &#x3D; \frac{1}{n_k} \sum_{i&#x3D;1}^{n_k} \left( x_j^{(i)} - \mu_{jk} \right)^2<br>$$</p>
</li>
</ul>
<p>其中，  </p>
<ul>
<li>$n_k$ 是类别 $C_k$ 的样本数，  </li>
<li>$x_j^{(i)}$ 表示类别 $C_k$ 中第 i 个样本的第 j 个特征值。</li>
</ul>
<hr>
<h4 id="朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）"><a href="#朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）" class="headerlink" title="朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）"></a>朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）</h4><hr>
<p>给定类别集合 $A_1, A_2, \ldots, A_n$ 和观察到的特征向量 $B &#x3D; (x_1, x_2, \ldots, x_d)$，<br>后验概率计算公式为：</p>
<p>$$<br>P(A_i \mid B) &#x3D; \frac{P(A_i) , P(B \mid A_i)}{\sum_{j&#x3D;1}^n P(A_j) , P(B \mid A_j)}<br>$$</p>
<hr>
<p>在高斯朴素贝叶斯中，假设特征条件独立且服从高斯分布，<br>则类别 $A_i$ 下特征 $x_k$ 的条件概率为：</p>
<p>$$<br>P(x_k \mid A_i) &#x3D; \frac{1}{\sqrt{2\pi \sigma_{ik}^2}} \exp\left( -\frac{(x_k - \mu_{ik})^2}{2\sigma_{ik}^2} \right)<br>$$</p>
<p>其中：  </p>
<ul>
<li>$\mu_{ik}$ 和 $\sigma_{ik}^2$ 是类别 $A_i$ 下第 k 个特征的均值和方差。</li>
</ul>
<hr>
<p>根据特征独立性假设，整体似然为：</p>
<p>$$<br>P(B \mid A_i) &#x3D; \prod_{k&#x3D;1}^d P(x_k \mid A_i)<br>$$</p>
<hr>
<p>综上，后验概率计算步骤为：</p>
<ol>
<li>计算先验概率 $P(A_i)$，一般为类别样本比例；</li>
<li>利用高斯分布计算每个特征的条件概率 $P(x_k \mid A_i)$；</li>
<li>计算似然 $P(B \mid A_i) &#x3D; \prod_{k&#x3D;1}^d P(x_k \mid A_i)$；</li>
<li>代入贝叶斯公式求得后验概率：</li>
</ol>
<p>$$<br>P(A_i \mid B) &#x3D; \frac{P(A_i) \prod_{k&#x3D;1}^d P(x_k \mid A_i)}{\sum_{j&#x3D;1}^n P(A_j) \prod_{k&#x3D;1}^d P(x_k \mid A_j)}<br>$$</p>
<hr>
<blockquote>
<p><strong>Notice</strong><br><br>在 高斯朴素贝叶斯（Gaussian Naive Bayes） 中，每个类别（target）都会单独拟合一个高斯分布，而且是针对每个特征分别计算。<br><br>在同一个类别（target）下，多个特征是条件独立的（<strong>朴素贝叶斯的前提</strong>,便于求得<strong>联合概率</strong>）<br></p>
</blockquote>
<hr>
<h3 id="Discriminative-Model-vs-Generative-Model"><a href="#Discriminative-Model-vs-Generative-Model" class="headerlink" title="Discriminative Model vs Generative Model"></a>Discriminative Model vs Generative Model</h3><h4 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h4><table>
<thead>
<tr>
<th></th>
<th><strong>Discriminative Model (判别式模型)</strong></th>
<th><strong>Generative Model (生成式模型)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心思想</strong></td>
<td>直接学习 <strong>后验分布</strong> $p(y|x)$</td>
<td>先学习 <strong>联合分布</strong> $p(x,y)$，或条件分布 $p(x|y)$ 和先验 $p(y)$</td>
</tr>
<tr>
<td><strong>关键任务</strong></td>
<td>找到最好的决策边界，分清类别</td>
<td>理解数据是如何生成的，可用于生成新样本</td>
</tr>
<tr>
<td><strong>举个例子</strong></td>
<td>Logistic Regression, SVM, Neural Network</td>
<td>Naive Bayes, GMM, HMM, GAN, VAE</td>
</tr>
</tbody></table>
<h4 id="公式对比"><a href="#公式对比" class="headerlink" title="公式对比"></a>公式对比</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心建模</strong></td>
<td>$p(y|x)$</td>
<td>$p(x,y) &#x3D; p(x|y) p(y)$</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>最大化条件似然：$\max_\theta p(y|x)$</td>
<td>最大化联合似然：$\max_\theta p(x,y)$</td>
</tr>
</tbody></table>
<hr>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>优点</strong></td>
<td>- 不需要建模输入分布，简单高效  <br> - 一般分类准确率更高</td>
<td>- 可生成新数据  <br> - 可处理缺失值  <br> - 对小样本问题有优势</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>- 无法生成数据  <br> - 对缺失值敏感</td>
<td>- 对输入分布需要假设  <br> - 特征条件独立假设可能太强</td>
</tr>
</tbody></table>
<hr>
<h4 id="常见示例"><a href="#常见示例" class="headerlink" title="常见示例"></a>常见示例</h4><table>
<thead>
<tr>
<th></th>
<th><strong>判别式</strong></th>
<th><strong>生成式</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>线性模型</strong></td>
<td>Logistic Regression</td>
<td>Naive Bayes (Gaussian&#x2F;Multinomial)</td>
</tr>
<tr>
<td><strong>非线性</strong></td>
<td>Neural Network, SVM</td>
<td>GMM, HMM, GAN, VAE</td>
</tr>
</tbody></table>
<h3 id="Optimization-1"><a href="#Optimization-1" class="headerlink" title="Optimization"></a>Optimization</h3><h4 id="使用Hessian分析Saddle-Point和Local-Minimal"><a href="#使用Hessian分析Saddle-Point和Local-Minimal" class="headerlink" title="使用Hessian分析Saddle Point和Local Minimal"></a>使用Hessian分析Saddle Point和Local Minimal</h4><p>At critical point: $L(\theta) &#x3D; L(\theta_0) + \frac{1}{2}(\theta - \theta_0)^TH(\theta - \theta_0)$.<br>我们把u(an eigen vector of H)当作$\theta - \theta_0$,$\lambda$为eigen value. </p>
<p>那么$u^T H u &#x3D; u^T(\lambda u) &#x3D; \lambda|u|^2 $.<br>因此$\lambda$正负会直接L的increase or decrease<br>根据这个公式可知解决saddle point问题步骤如下：  </p>
<ul>
<li>找到H</li>
<li>找到H的某个负特征值</li>
<li>对应的特征向量u就是想找的下降方向</li>
<li>$\theta &#x3D; \theta_0 + u$</li>
</ul>
<h4 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h4><h5 id="Root-Mean-Square"><a href="#Root-Mean-Square" class="headerlink" title="Root Mean Square"></a>Root Mean Square</h5><p><strong>定义：</strong><br>Root Mean Square（RMS）是指一组数平方后求平均再开方，反映平均“能量”大小。</p>
<p>公式如下：</p>
<p>$\sigma_i^t &#x3D; \sqrt{\frac{1}{t+1}\sum_{i&#x3D;0}^t (g_i^t)^2}$.  </p>
<p>$\theta_i^{t+1} &#x3D; \theta_i^t - \frac{\eta}{\sigma_i^t}g_i^t$</p>
<p>在优化中，用于衡量梯度大小，作为学习率调整的依据。</p>
<h5 id="RMSProp-Root-Mean-Square-Propagation"><a href="#RMSProp-Root-Mean-Square-Propagation" class="headerlink" title="RMSProp (Root Mean Square Propagation)"></a>RMSProp (Root Mean Square Propagation)</h5><ul>
<li>梯度平方滑动平均：</li>
</ul>
<p>$\sigma_i^t &#x3D; \sqrt{\alpha (\sigma_i^{t-1})^2 + (1-\alpha)(g_i^t)^2}$</p>
<ul>
<li>参数更新：</li>
</ul>
<p>$θ_i^{t+1} &#x3D; θ_i^t - \frac{\eta}{\sigma_i^t}g_i^t$</p>
<p>其中：</p>
<ul>
<li>ρ 是衰减率（常用 0.9）</li>
<li>η 是基础学习率</li>
<li>ε 是防止除以 0 的小常数（如 1e-8）</li>
</ul>
<h5 id="直觉理解："><a href="#直觉理解：" class="headerlink" title="直觉理解："></a>直觉理解：</h5><ul>
<li>梯度大的参数方向：步长缩小，避免震荡</li>
<li>梯度小的参数方向：步长放大，避免停滞</li>
<li>RMSProp 根据每个参数的历史梯度自动调整学习率，实现更稳定的收敛过程</li>
</ul>
<h4 id="总结对比"><a href="#总结对比" class="headerlink" title="总结对比"></a>总结对比</h4><table>
<thead>
<tr>
<th>概念</th>
<th>含义 &#x2F; 作用</th>
<th>与 RMSProp 的关系</th>
</tr>
</thead>
<tbody><tr>
<td>Adaptive Learning Rate</td>
<td>每个参数独立学习率，动态调整</td>
<td>RMSProp 是其代表性实现方法</td>
</tr>
<tr>
<td>Root Mean Square</td>
<td>用于估计梯度平均能量，控制步长</td>
<td>被 RMSProp 用于缩放学习率</td>
</tr>
<tr>
<td>RMSProp</td>
<td>自适应优化算法，防止学习率过早衰减</td>
<td>实现了 adaptive learning rate</td>
</tr>
</tbody></table>
<h5 id="Learning-Rate-Scheduling"><a href="#Learning-Rate-Scheduling" class="headerlink" title="Learning Rate Scheduling"></a>Learning Rate Scheduling</h5><h4 id="Batch-和-Momentum简要说明"><a href="#Batch-和-Momentum简要说明" class="headerlink" title="Batch 和 Momentum简要说明"></a>Batch 和 Momentum简要说明</h4><h5 id="Batch（小批量）"><a href="#Batch（小批量）" class="headerlink" title="Batch（小批量）"></a>Batch（小批量）</h5><p>在训练神经网络时，我们通常不会每次使用整个训练集来更新参数，而是将数据划分为多个“小批量”，每个 batch 包含若干样本。</p>
<p>例如：</p>
<ul>
<li>数据总量为 10,000，batch size &#x3D; 32</li>
<li>则每个 epoch 有约 312 次更新</li>
</ul>
<h6 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h6><ul>
<li>提高训练效率（相比使用整个数据集）</li>
<li>提供更稳定的梯度估计（相比单个样本）</li>
<li>有利于 GPU 并行计算</li>
</ul>
<h6 id="直觉："><a href="#直觉：" class="headerlink" title="直觉："></a>直觉：</h6><ul>
<li>小 batch：训练快但不稳定，梯度噪声大</li>
<li>大 batch：训练稳定但计算开销大，收敛可能慢</li>
</ul>
<h5 id="Momentum（动量）"><a href="#Momentum（动量）" class="headerlink" title="Momentum（动量）"></a>Momentum（动量）</h5><p>Momentum 是优化器的一种加速技巧，通过引入“动量”，在梯度方向上积累速度。</p>
<p>常规梯度下降公式：</p>
<pre><code>$θ_&#123;t+1&#125; = θ_t - η ∇L(θ_t)$
</code></pre>
<p>加入动量后的更新规则：</p>
<pre><code>$v_&#123;t+1&#125; = γ v_t - η ∇L(θ_t)$
$θ_&#123;t+1&#125; = θ_t + v_&#123;t+1&#125;$
</code></pre>
<p>其中：</p>
<ul>
<li>η 是学习率</li>
<li>γ 是动量系数（如 0.9）</li>
<li>v 是“速度向量”，表示历史梯度累积效果</li>
</ul>
<h6 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h6><ul>
<li>加快收敛速度</li>
<li>减少梯度震荡</li>
<li>更容易跳出局部最小值</li>
</ul>
<h6 id="直觉：-1"><a href="#直觉：-1" class="headerlink" title="直觉："></a>直觉：</h6><p>像滚动的小球有惯性，动量让优化器在梯度一致的方向上加速前进，在梯度变化剧烈的方向上稳定下来。</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><table>
<thead>
<tr>
<th>概念</th>
<th>定义</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>Batch</td>
<td>一次更新中使用的样本子集</td>
<td>提高效率、平滑梯度估计</td>
</tr>
<tr>
<td>Momentum</td>
<td>使用历史梯度加速更新</td>
<td>加速收敛、减少震荡、跳出局部最小值</td>
</tr>
</tbody></table>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>无论是在图像识别，音频处理等应用中，输入都可以看作是一个向量，输出是一个数值或类别。然而，若输入是<strong>一系列向量</strong>,同时长度会改变，例如把句子里的单词都描述为向量，那么模型的输入就是一个向量集合，并且每个向量的大小都不一样。<br>对于这个一系列向量，或者说是向量集合，我们通常会考虑上下文，因此我们会引入滑动窗口机制，每个向量查看窗口中相邻的其他向量的性质。为了量化地计算向量之间的相关性，我们引入了Self-attention技术。</p>
<h4 id="Basic-Concept-1"><a href="#Basic-Concept-1" class="headerlink" title="Basic Concept"></a>Basic Concept</h4><p>在注意力机制中，主要涉及三个向量：Query、Key 和 Value。</p>
<p>给定一个输入向量集合 ${a^1, a^2, \dots, a^n}$，我们对每个向量 $a^i$ 进行线性变换以获得对应的 Query、Key 和 Value 向量。</p>
<h4 id="向量计算"><a href="#向量计算" class="headerlink" title="向量计算"></a>向量计算</h4><ul>
<li>Query 向量：$q^i &#x3D; a^i W^q$</li>
<li>Key 向量：$k^i &#x3D; a^i W^k$</li>
<li>Value 向量：$v^i &#x3D; a^i W^v$</li>
</ul>
<p>其中，$W^q$、$W^k$、$W^v$ 是可学习的参数矩阵，维度通常为：</p>
<ul>
<li>$W^q \in \mathbb{R}^{d_{model} \times d_k}$</li>
<li>$W^k \in \mathbb{R}^{d_{model} \times d_k}$</li>
<li>$W^v \in \mathbb{R}^{d_{model} \times d_v}$</li>
</ul>
<h4 id="注意力打分机制"><a href="#注意力打分机制" class="headerlink" title="注意力打分机制"></a>注意力打分机制</h4><p>对于一个查询向量 $q^i$，其对应的注意力权重是通过它与所有 Key 向量的点积计算得到的：</p>
<p>$$<br>\text{score}_{ij} &#x3D; \frac{q^i \cdot (k^j)^T}{\sqrt{d_k}}<br>$$</p>
<p>然后使用 Softmax 函数对所有得分进行归一化，得到注意力分布：</p>
<p>$$<br>\alpha_{ij} &#x3D; \text{softmax}(\text{score}<em>{ij}) &#x3D; \frac{\exp(\text{score}</em>{ij})}{\sum_{j&#x3D;1}^{n} \exp(\text{score}_{ij})}<br>$$</p>
<h4 id="加权求和得到输出"><a href="#加权求和得到输出" class="headerlink" title="加权求和得到输出"></a>加权求和得到输出</h4><p>最后，使用这些注意力权重对对应的 Value 向量加权求和，得到最终的注意力输出向量：</p>
<p>$$<br>z^i &#x3D; \sum_{j&#x3D;1}^{n} \alpha_{ij} v^j<br>$$</p>
<p>这个过程可以表示为矩阵形式：</p>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V<br>$$</p>
<hr>
<h4 id="总结流程："><a href="#总结流程：" class="headerlink" title="总结流程："></a>总结流程：</h4><ol>
<li>输入向量集 $a^i$</li>
<li>计算 Query: $q^i &#x3D; a^i W^q$</li>
<li>计算 Key: $k^i &#x3D; a^i W^k$</li>
<li>计算 Value: $v^i &#x3D; a^i W^v$</li>
<li>计算注意力得分（点积）：$q^i \cdot (k^j)^T$</li>
<li>通过 Softmax 得到注意力权重</li>
<li>对 Value 加权求和得到输出 $z^i$</li>
</ol>
<hr>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li>$d_k$ 是 Key 向量的维度</li>
<li>$d_v$ 是 Value 向量的维度</li>
<li>注意力机制允许模型动态地聚焦于输入序列中最相关的部分</li>
</ul>
<h3 id="RNN-Recurrent-Neural-Network"><a href="#RNN-Recurrent-Neural-Network" class="headerlink" title="RNN (Recurrent Neural Network)"></a>RNN (Recurrent Neural Network)</h3><h4 id="Long-Short-Term-Memory-LTSM-Cell-Explanation"><a href="#Long-Short-Term-Memory-LTSM-Cell-Explanation" class="headerlink" title="Long Short-Term Memory (LTSM) Cell Explanation"></a>Long Short-Term Memory (LTSM) Cell Explanation</h4><p>This project contains visual illustrations and numerical examples of how an LSTM (Long Short-Term Memory) cell works internally. The LSTM is a type of recurrent neural network (RNN) that is capable of learning long-term dependencies and solving the vanishing gradient problem.</p>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>LSTMs introduce <strong>memory cells</strong> and <strong>gating mechanisms</strong> to control the flow of information. The key gates in an LSTM cell are:</p>
<ul>
<li><p><strong>遗忘门（Forget Gate）</strong><br>控制保留多少过去的记忆。<br>计算方式：$f(z_f)$，使用 sigmoid 激活函数，输出范围在 $[0, 1]$，模拟“开”与“关”。</p>
</li>
<li><p><strong>输入门（Input Gate）</strong><br>决定当前输入中保留哪些信息。<br>表达式为：$f(z_i)$ 和 $g(z)$。</p>
</li>
<li><p><strong>记忆单元状态（Cell State）</strong><br>状态更新公式：<br>$$<br>C_{\text{new}} &#x3D; f(z_f) \cdot C_{\text{old}} + f(z_i) \cdot g(z)<br>$$</p>
</li>
<li><p><strong>输出门（Output Gate）</strong><br>决定当前时刻的隐藏状态输出 $a$：<br>$$<br>a &#x3D; f(z_o) \cdot h(C)<br>$$</p>
</li>
</ul>
<p>激活函数说明：</p>
<ul>
<li>$f(\cdot)$ 表示 sigmoid（$\sigma$ 函数），控制门的开关；</li>
<li>$g(\cdot)$ 表示 tanh，处理输入的候选记忆；</li>
<li>$h(\cdot)$ 通常也是 tanh，用于输出处理。</li>
</ul>
<hr>
<h4 id="LSTM-数值计算示例"><a href="#LSTM-数值计算示例" class="headerlink" title="LSTM 数值计算示例"></a>LSTM 数值计算示例</h4><img src="/images/LSTM_Formulation.png">

<p>该图展示了一个完整的 <strong>LSTM 单元的前向传播数值过程</strong>，主要特征包括：</p>
<ul>
<li>输入特征向量 $x_1$, $x_2$, $x_3$ 及偏置项；</li>
<li>不同门控的权重矩阵、偏置与线性组合；</li>
<li>显式显示了输入门、遗忘门、输出门的计算过程；</li>
<li>Cell 的状态值如何随着时间变化更新；</li>
<li>最终输出 $y$ 序列中在特定时间步激活（例如输出为 7）。</li>
</ul>
<p>通过可视化的矩阵计算，可以看到 LSTM 如何“记住”或“忘记”输入中的关键信息。</p>
<h5 id="关键计算公式"><a href="#关键计算公式" class="headerlink" title="关键计算公式"></a>关键计算公式</h5><p>$$<br>\begin{aligned}<br>f_t &amp;&#x3D; \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\<br>i_t &amp;&#x3D; \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\<br>g_t &amp;&#x3D; \tanh(W_g \cdot [h_{t-1}, x_t] + b_g) \\<br>C_t &amp;&#x3D; f_t \cdot C_{t-1} + i_t \cdot g_t \\<br>o_t &amp;&#x3D; \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\<br>h_t &amp;&#x3D; o_t \cdot \tanh(C_t)<br>\end{aligned}<br>$$</p>
<h4 id="LSTM的模块说明以及应用场景"><a href="#LSTM的模块说明以及应用场景" class="headerlink" title="LSTM的模块说明以及应用场景"></a>LSTM的模块说明以及应用场景</h4><table>
<thead>
<tr>
<th>模块</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>遗忘门</td>
<td>学习去“忘记”无关记忆</td>
</tr>
<tr>
<td>输入门</td>
<td>学习将当前输入写入记忆</td>
</tr>
<tr>
<td>候选记忆块</td>
<td>生成当前输入的表示</td>
</tr>
<tr>
<td>Cell 状态</td>
<td>长期记忆存储通道</td>
</tr>
<tr>
<td>输出门</td>
<td>控制当前输出暴露多少内部记忆</td>
</tr>
</tbody></table>
<p>LSTM 网络适用于处理<strong>时间相关性较强</strong>的任务，例如：</p>
<ul>
<li>🧠 自然语言处理（NLP）：文本分类、情感分析、语言建模</li>
<li>📈 时间序列预测：股价、传感器数据、气象数据</li>
<li>🗣 语音识别、语音生成</li>
<li>🔄 序列到序列（Seq2Seq）：机器翻译、摘要生成等</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>本示例结合一个LSTM模型的完整代码结构，介绍如何使用pandas准备数据、构建LSTM模型及计算参数量，方便初学者理解和应用。</p>
<h5 id="1-数据预处理（pandas基础）"><a href="#1-数据预处理（pandas基础）" class="headerlink" title="1. 数据预处理（pandas基础）"></a>1. 数据预处理（pandas基础）</h5><ul>
<li>读取CSV文件</li>
</ul>
<p>import pandas as pd<br>df &#x3D; pd.read_csv(“weatherHistory.csv”)</p>
<ul>
<li>查看数据前几行</li>
</ul>
<p>df.head()</p>
<ul>
<li>选取数值型列计算相关性</li>
</ul>
<p>num_df &#x3D; df.select_dtypes(include&#x3D;[‘number’])<br>corr_matrix &#x3D; num_df.corr()</p>
<ul>
<li>处理缺失值</li>
</ul>
<p>df.fillna(0, inplace&#x3D;True)</p>
<ul>
<li>标签编码</li>
</ul>
<p>from sklearn.preprocessing import LabelEncoder<br>labelencoder &#x3D; LabelEncoder()<br>df[‘Summary’] &#x3D; labelencoder.fit_transform(df[‘Summary’])</p>
<ul>
<li>时间序列转监督学习格式</li>
</ul>
<p>def series_to_supervised(data, n_in&#x3D;1, n_out&#x3D;1, dropnan&#x3D;True):<br>    n_vars &#x3D; 1 if type(data) is list else data.shape[1]<br>    df &#x3D; pd.DataFrame(data)<br>    cols, names &#x3D; [], []<br>    # 过去时间步 (t-n, … t-1)<br>    for i in range(n_in, 0, -1):<br>        cols.append(df.shift(i))<br>        names +&#x3D; [f’var{j+1}(t-{i})’ for j in range(n_vars)]<br>    # 未来时间步 (t, t+1, … t+n)<br>    for i in range(n_out):<br>        cols.append(df.shift(-i))<br>        if i &#x3D;&#x3D; 0:<br>            names +&#x3D; [f’var{j+1}(t)’ for j in range(n_vars)]<br>        else:<br>            names +&#x3D; [f’var{j+1}(t+{i})’ for j in range(n_vars)]<br>    agg &#x3D; pd.concat(cols, axis&#x3D;1)<br>    agg.columns &#x3D; names<br>    if dropnan:<br>        agg.dropna(inplace&#x3D;True)<br>    return agg</p>
<hr>
<h5 id="2-数据分割与准备"><a href="#2-数据分割与准备" class="headerlink" title="2. 数据分割与准备"></a>2. 数据分割与准备</h5><p>假设有 n_hours 过去时间步，和 n_features 特征数：</p>
<p>n_obs &#x3D; n_hours * n_features<br>train_X, train_Y &#x3D; train[:, :n_obs], train[:, -6]   # 以倒数第6列为输出变量<br>test_X, test_Y &#x3D; test[:, :n_obs], test[:, -6]</p>
<hr>
<h5 id="3-模型结构示例（Keras）"><a href="#3-模型结构示例（Keras）" class="headerlink" title="3. 模型结构示例（Keras）"></a>3. 模型结构示例（Keras）</h5><p>Model: “sequential”</p>
<table>
<thead>
<tr>
<th>Layer (type)</th>
<th>Output Shape</th>
<th>Param #</th>
</tr>
</thead>
<tbody><tr>
<td>LSTM (lstm)</td>
<td>(None, 30)</td>
<td>4680</td>
</tr>
<tr>
<td>Dense (FC1)</td>
<td>(None, 256)</td>
<td>7936</td>
</tr>
<tr>
<td>Activation</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
<tr>
<td>Dropout</td>
<td>(None, 256)</td>
<td>0</td>
</tr>
<tr>
<td>Dense (out_layer)</td>
<td>(None, 1)</td>
<td>257</td>
</tr>
<tr>
<td><strong>Total Params</strong></td>
<td></td>
<td><strong>12873</strong></td>
</tr>
</tbody></table>
<ul>
<li>LSTM层参数计算公式：</li>
</ul>
<p>Param &#x3D; 4 × [(input_dim + hidden_dim) × hidden_dim + hidden_dim]</p>
<p>其中，<br>input_dim 是输入特征维度，<br>hidden_dim 是LSTM隐藏单元数量（本例为30）。</p>
<hr>
<h5 id="4-LSTM细胞状态计算简述"><a href="#4-LSTM细胞状态计算简述" class="headerlink" title="4. LSTM细胞状态计算简述"></a>4. LSTM细胞状态计算简述</h5><p>细胞状态更新：</p>
<p>$C_new &#x3D; f(z_f) * C_old + f(z_i) * g(z)$</p>
<p>隐藏状态输出：</p>
<p>$a &#x3D; f(z_o) * h(C)$</p>
<p>其中：<br>f 是sigmoid激活函数，<br>g 是tanh激活函数，<br>维度均为 (hidden_dim,)。</p>
<hr>
<h5 id="5-预测测试集时的说明"><a href="#5-预测测试集时的说明" class="headerlink" title="5. 预测测试集时的说明"></a>5. 预测测试集时的说明</h5><p>训练完模型后，预测测试集时，需保证测试集的输入格式与训练集一致，包含足够的过去时间步作为输入。</p>
<hr>
<p>以上为LSTM时间序列预测的关键步骤与概念总结，结合pandas数据处理及模型搭建，适合初学者快速理解和实践。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/06/15/C++/" rel="prev" title="C++">
      <i class="fa fa-chevron-left"></i> C++
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/06/17/PyTorch/" rel="next" title="PyTorch">
      PyTorch <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Concept"><span class="nav-number">1.</span> <span class="nav-text">Basic Concept</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Model"><span class="nav-number">1.1.</span> <span class="nav-text">Linear Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Define-function-with-unknown-parameters"><span class="nav-number">1.1.1.</span> <span class="nav-text">Define function with unknown parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Define-Loss-from-Training-Data"><span class="nav-number">1.1.2.</span> <span class="nav-text">Define Loss from Training Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimization"><span class="nav-number">1.1.3.</span> <span class="nav-text">Optimization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sophisticated-Models"><span class="nav-number">1.2.</span> <span class="nav-text">Sophisticated Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid-function%EF%BC%88logistic-regression%EF%BC%89"><span class="nav-number">1.2.1.</span> <span class="nav-text">Sigmoid function（logistic regression）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Use-Quadratic-and-Higher-Order-Terms"><span class="nav-number">1.3.</span> <span class="nav-text">Use Quadratic and Higher-Order Terms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-Risk"><span class="nav-number">1.3.1.</span> <span class="nav-text">Overfitting Risk</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rugularization-to-Prevent-Overfitting"><span class="nav-number">1.3.2.</span> <span class="nav-text">Rugularization to Prevent Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%97%B6%E5%A6%82%E4%BD%95%E5%8A%A0%E4%B8%8A%E6%AD%A3%E5%88%99%E5%8C%96%E7%9B%B8"><span class="nav-number">1.3.3.</span> <span class="nav-text">训练时如何加上正则化相</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PyTorch-CNN-%E4%BA%8C%E5%88%86%E7%B1%BB%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.4.</span> <span class="nav-text">PyTorch CNN 二分类示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86transform"><span class="nav-number">1.4.1.</span> <span class="nav-text">预处理transform</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89Dataset"><span class="nav-number">1.4.2.</span> <span class="nav-text">自定义Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95CNN%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.3.</span> <span class="nav-text">简单CNN模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">输入数据格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%92%8C%E6%B1%A0%E5%8C%96%E5%B1%82%E6%B5%81%E7%A8%8B"><span class="nav-number">1.4.3.3.</span> <span class="nav-text">卷积层和池化层流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E6%B5%81%E7%A8%8B"><span class="nav-number">1.4.3.4.</span> <span class="nav-text">全连接层流程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.4.</span> <span class="nav-text">准备数据和模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#train-validate-test%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">train,validate,test数据集的区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#train-validate-test%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">train,validate,test数据集的划分</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="nav-number">1.4.5.</span> <span class="nav-text">训练循环</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification"><span class="nav-number">1.5.</span> <span class="nav-text">Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid%EF%BC%88%E4%BA%8C%E5%88%86%E7%B1%BB%EF%BC%89"><span class="nav-number">1.5.1.</span> <span class="nav-text">Sigmoid（二分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax%EF%BC%88%E5%A4%9A%E5%88%86%E7%B1%BB%EF%BC%89"><span class="nav-number">1.5.2.</span> <span class="nav-text">Softmax（多分类）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">1.6.</span> <span class="nav-text">Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Regression-vs-Sigmoid-Regression"><span class="nav-number">1.6.1.</span> <span class="nav-text">Linear Regression vs Sigmoid Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MSE-vs-Binary-Cross-Entropy"><span class="nav-number">1.6.2.</span> <span class="nav-text">MSE vs Binary Cross-Entropy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Support-Vector-Machine"><span class="nav-number">1.7.</span> <span class="nav-text">Support Vector Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Model-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">Linear Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-Function"><span class="nav-number">1.7.2.</span> <span class="nav-text">Loss Function</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-w-b-frac-1-2-w-2-C-sum-n-1-N-max-0-1-y-n-w-T-x-n-b"><span class="nav-number">2.</span> <span class="nav-text">$$L(w,b) &#x3D; \frac{1}{2} |w|^2 + C \sum_{n&#x3D;1}^N \max(0,, 1 - y_n (w^T x_n + b)).$$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimization-Gradient-Descent"><span class="nav-number">2.0.1.</span> <span class="nav-text">Optimization (Gradient Descent)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive-Bayes%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">朴素贝叶斯（Naive Bayes）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E3%80%82"><span class="nav-number">2.1.1.</span> <span class="nav-text">条件概率。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E6%A6%82%E7%8E%87"><span class="nav-number">2.1.2.</span> <span class="nav-text">全概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E6%96%AD"><span class="nav-number">2.1.3.</span> <span class="nav-text">贝叶斯推断</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.4.</span> <span class="nav-text">举例说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">2.1.5.</span> <span class="nav-text">最大似然估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89%E5%8F%8A-GaussianNB-%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.1.6.</span> <span class="nav-text">最大似然估计（MLE）及 GaussianNB 示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%AD%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%AE%A1%E7%AE%97%EF%BC%88%E4%BB%A5%E9%AB%98%E6%96%AF%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="nav-number">2.1.7.</span> <span class="nav-text">朴素贝叶斯中后验概率计算（以高斯朴素贝叶斯为例）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminative-Model-vs-Generative-Model"><span class="nav-number">2.2.</span> <span class="nav-text">Discriminative Model vs Generative Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%AE%9A%E4%B9%89"><span class="nav-number">2.2.1.</span> <span class="nav-text">基本定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="nav-number">2.2.2.</span> <span class="nav-text">公式对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">2.2.3.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.2.4.</span> <span class="nav-text">常见示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization-1"><span class="nav-number">2.3.</span> <span class="nav-text">Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Hessian%E5%88%86%E6%9E%90Saddle-Point%E5%92%8CLocal-Minimal"><span class="nav-number">2.3.1.</span> <span class="nav-text">使用Hessian分析Saddle Point和Local Minimal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaptive-Learning-Rate"><span class="nav-number">2.3.2.</span> <span class="nav-text">Adaptive Learning Rate</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Root-Mean-Square"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">Root Mean Square</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RMSProp-Root-Mean-Square-Propagation"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">RMSProp (Root Mean Square Propagation)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B4%E8%A7%89%E7%90%86%E8%A7%A3%EF%BC%9A"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">直觉理解：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94"><span class="nav-number">2.3.3.</span> <span class="nav-text">总结对比</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-Rate-Scheduling"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">Learning Rate Scheduling</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Batch-%E5%92%8C-Momentum%E7%AE%80%E8%A6%81%E8%AF%B4%E6%98%8E"><span class="nav-number">2.3.4.</span> <span class="nav-text">Batch 和 Momentum简要说明</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Batch%EF%BC%88%E5%B0%8F%E6%89%B9%E9%87%8F%EF%BC%89"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">Batch（小批量）</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8"><span class="nav-number">2.3.4.1.1.</span> <span class="nav-text">作用</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9B%B4%E8%A7%89%EF%BC%9A"><span class="nav-number">2.3.4.1.2.</span> <span class="nav-text">直觉：</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Momentum%EF%BC%88%E5%8A%A8%E9%87%8F%EF%BC%89"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">Momentum（动量）</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8%EF%BC%9A"><span class="nav-number">2.3.4.2.1.</span> <span class="nav-text">作用：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%9B%B4%E8%A7%89%EF%BC%9A-1"><span class="nav-number">2.3.4.2.2.</span> <span class="nav-text">直觉：</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.3.4.3.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-attention"><span class="nav-number">2.4.</span> <span class="nav-text">Self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-Concept-1"><span class="nav-number">2.4.1.</span> <span class="nav-text">Basic Concept</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-number">2.4.2.</span> <span class="nav-text">向量计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%89%93%E5%88%86%E6%9C%BA%E5%88%B6"><span class="nav-number">2.4.3.</span> <span class="nav-text">注意力打分机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%BE%97%E5%88%B0%E8%BE%93%E5%87%BA"><span class="nav-number">2.4.4.</span> <span class="nav-text">加权求和得到输出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="nav-number">2.4.5.</span> <span class="nav-text">总结流程：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%B4%E6%98%8E"><span class="nav-number">2.4.6.</span> <span class="nav-text">说明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-Recurrent-Neural-Network"><span class="nav-number">2.5.</span> <span class="nav-text">RNN (Recurrent Neural Network)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Long-Short-Term-Memory-LTSM-Cell-Explanation"><span class="nav-number">2.5.1.</span> <span class="nav-text">Long Short-Term Memory (LTSM) Cell Explanation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overview"><span class="nav-number">2.5.2.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM-%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.5.3.</span> <span class="nav-text">LSTM 数值计算示例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-number">2.5.3.1.</span> <span class="nav-text">关键计算公式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM%E7%9A%84%E6%A8%A1%E5%9D%97%E8%AF%B4%E6%98%8E%E4%BB%A5%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">2.5.4.</span> <span class="nav-text">LSTM的模块说明以及应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.5.5.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%88pandas%E5%9F%BA%E7%A1%80%EF%BC%89"><span class="nav-number">2.5.5.1.</span> <span class="nav-text">1. 数据预处理（pandas基础）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%E4%B8%8E%E5%87%86%E5%A4%87"><span class="nav-number">2.5.5.2.</span> <span class="nav-text">2. 数据分割与准备</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E7%A4%BA%E4%BE%8B%EF%BC%88Keras%EF%BC%89"><span class="nav-number">2.5.5.3.</span> <span class="nav-text">3. 模型结构示例（Keras）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-LSTM%E7%BB%86%E8%83%9E%E7%8A%B6%E6%80%81%E8%AE%A1%E7%AE%97%E7%AE%80%E8%BF%B0"><span class="nav-number">2.5.5.4.</span> <span class="nav-text">4. LSTM细胞状态计算简述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-%E9%A2%84%E6%B5%8B%E6%B5%8B%E8%AF%95%E9%9B%86%E6%97%B6%E7%9A%84%E8%AF%B4%E6%98%8E"><span class="nav-number">2.5.5.5.</span> <span class="nav-text">5. 预测测试集时的说明</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
